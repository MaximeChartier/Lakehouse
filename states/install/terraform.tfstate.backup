{
  "version": 4,
  "terraform_version": "1.1.0",
  "serial": 281,
  "lineage": "a7b42746-a975-4ef5-c812-6876791bc367",
  "outputs": {},
  "resources": [
    {
      "module": "module.argo_events_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "argo-events",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "argo-events",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "argoevents",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v1.5.0",
                "chart": "argo-events",
                "name": "argoevents",
                "namespace": "argo-events",
                "revision": 1,
                "values": "null",
                "version": "1.8.0"
              }
            ],
            "name": "argoevents",
            "namespace": "argo-events",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://argoproj.github.io/argo-helm",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1000,
            "values": [
              ""
            ],
            "verify": false,
            "version": "1.8.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.mlflow",
            "module.kube.kubernetes_namespace.spark",
            "module.istio_install.helm_release.istio_ingress",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.minio",
            "module.istio_install.helm_release.istio_base"
          ]
        }
      ]
    },
    {
      "module": "module.istio_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "istio_base",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "base",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "istio-base",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.12.0",
                "chart": "base",
                "name": "istio-base",
                "namespace": "istio-system",
                "revision": 1,
                "values": "null",
                "version": "1.12.0"
              }
            ],
            "name": "istio-base",
            "namespace": "istio-system",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://istio-release.storage.googleapis.com/charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 120,
            "values": null,
            "verify": false,
            "version": "1.12.0",
            "wait": true,
            "wait_for_jobs": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.minio",
            "module.kube.kubernetes_namespace.spark",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mlflow"
          ]
        }
      ]
    },
    {
      "module": "module.istio_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "istio_ingress",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "gateway",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "istio-gateway",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.12.0",
                "chart": "gateway",
                "name": "istio-gateway",
                "namespace": "istio-system",
                "revision": 1,
                "values": "{\"labels\":{\"app\":\"istio-ingressgateway\",\"istio\":\"ingressgateway\"},\"name\":\"istio-ingressgateway\"}",
                "version": "1.12.0"
              }
            ],
            "name": "istio-gateway",
            "namespace": "istio-system",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://istio-release.storage.googleapis.com/charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "labels.app",
                "type": "",
                "value": "istio-ingressgateway"
              },
              {
                "name": "labels.istio",
                "type": "",
                "value": "ingressgateway"
              },
              {
                "name": "name",
                "type": "",
                "value": "istio-ingressgateway"
              }
            ],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 120,
            "values": null,
            "verify": false,
            "version": "1.12.0",
            "wait": true,
            "wait_for_jobs": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.minio",
            "module.kube.kubernetes_namespace.mlflow",
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.spark",
            "module.istio_install.helm_release.istio_base",
            "module.istio_install.helm_release.istio_istiod"
          ]
        }
      ]
    },
    {
      "module": "module.istio_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "istio_istiod",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "istiod",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "istio-istiod",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.12.0",
                "chart": "istiod",
                "name": "istio-istiod",
                "namespace": "istio-system",
                "revision": 1,
                "values": "{\"pilot\":{\"resources\":{\"requests\":{\"memory\":\"1024Mi\"}}}}",
                "version": "1.12.0"
              }
            ],
            "name": "istio-istiod",
            "namespace": "istio-system",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://istio-release.storage.googleapis.com/charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "pilot.resources.requests.memory",
                "type": "",
                "value": "1024Mi"
              }
            ],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 120,
            "values": null,
            "verify": false,
            "version": "1.12.0",
            "wait": true,
            "wait_for_jobs": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.kube.kubernetes_namespace.mlflow",
            "module.kube.kubernetes_namespace.spark",
            "module.istio_install.helm_release.istio_base",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.minio",
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.ldap"
          ]
        }
      ]
    },
    {
      "module": "module.jupyterhub_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "jupyterhub",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "jupyterhub",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "jupyterhub",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.5.0",
                "chart": "jupyterhub",
                "name": "jupyterhub",
                "namespace": "jupyterhub",
                "revision": 1,
                "values": "{\"auxiliaryImage\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/bitnami-shell\",\"tag\":\"10-debian-10-r285\"},\"commonAnnotations\":{},\"commonLabels\":{},\"externalDatabase\":{\"database\":\"jupyterhub\",\"existingSecret\":\"\",\"host\":\"\",\"password\":\"\",\"port\":5432,\"user\":\"postgres\"},\"extraDeploy\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"storageClass\":\"\"},\"hub\":{\"adminUser\":\"user\",\"affinity\":{},\"args\":[],\"baseUrl\":\"/\",\"command\":[],\"configuration\":\"Chart:\\n  Name: {{ .Chart.Name }}\\n  Version: {{ .Chart.Version }}\\nRelease:\\n  Name: {{ .Release.Name }}\\n  Namespace: {{ .Release.Namespace }}\\n  Service: {{ .Release.Service }}\\nhub:\\n  config:\\n    JupyterHub:\\n      admin_access: true\\n      authenticator_class: dummy\\n      cookie_secret_file: /tmp/jupyterhub_cookie_secret\\n      DummyAuthenticator:\\n      {{- if .Values.hub.password }}\\n        password: {{ .Values.hub.password | quote }}\\n      {{- else }}\\n        password: {{ randAlphaNum 10 | quote }}\\n      {{- end }}\\n      Authenticator:\\n        admin_users:\\n          - {{ .Values.hub.adminUser }}\\n  cookieSecret:\\n  concurrentSpawnLimit: 64\\n  consecutiveFailureLimit: 5\\n  activeServerLimit:\\n  db:\\n    type: postgres\\n    url: postgresql://{{ ternary .Values.postgresql.postgresqlUsername .Values.externalDatabase.user .Values.postgresql.enabled }}@{{ ternary (include \\\"jupyterhub.postgresql.fullname\\\" .) .Values.externalDatabase.host .Values.postgresql.enabled }}:{{ ternary \\\"5432\\\" .Values.externalDatabase.port .Values.postgresql.enabled }}/{{ ternary .Values.postgresql.postgresqlDatabase .Values.externalDatabase.database .Values.postgresql.enabled }}\\n  services: {}\\n  allowNamedServers: false\\n  namedServerLimitPerUser:\\n  {{- if .Values.hub.metrics.serviceMonitor.enabled }}\\n  authenticatePrometheus: {{ .Values.hub.metrics.authenticatePrometheus }}\\n  {{- end }}\\n  redirectToServer:\\n  shutdownOnLogout:\\nsingleuser:\\n  podNameTemplate: {{ include \\\"common.names.fullname\\\" . }}-jupyter-{username}\\n  {{- if .Values.singleuser.tolerations }}\\n  extraTolerations: {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.tolerations \\\"context\\\" $) | nindent 4 }}\\n  {{- end }}\\n  {{- if .Values.singleuser.nodeSelector }}\\n  nodeSelector: {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.nodeSelector \\\"context\\\" $) | nindent 4 }}\\n  {{- end }}\\n  networkTools:\\n    image:\\n      name: {{ include \\\"jupyterhub.hubconfiguration.imageEntry\\\" ( dict \\\"imageRoot\\\" .Values.auxiliaryImage \\\"global\\\" $) }}\\n      tag: {{ .Values.auxiliaryImage.tag }}\\n      pullPolicy: {{ .Values.auxiliaryImage.pullPolicy }}\\n      pullSecrets: {{- include \\\"jupyterhub.imagePullSecrets.list\\\" . | nindent 8 }}\\n  cloudMetadata:\\n    blockWithIptables: false\\n  events: true\\n  extraAnnotations:\\n    {{- if .Values.commonAnnotations }}\\n    {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.commonAnnotations \\\"context\\\" $ ) | nindent 4 }}\\n    {{- end }}\\n    {{- if .Values.singleuser.podAnnotations }}\\n    {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.podAnnotations \\\"context\\\" $ ) | nindent 4 }}\\n    {{- end }}\\n  extraLabels:\\n    hub.jupyter.org/network-access-hub: \\\"true\\\"\\n    app.kubernetes.io/component: singleuser\\n    {{- include \\\"common.labels.standard\\\" . | nindent 4 }}\\n    {{- if .Values.commonLabels }}\\n    {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.commonLabels \\\"context\\\" $ ) | nindent 4 }}\\n    {{- end }}\\n    {{- if .Values.singleuser.podLabels }}\\n    {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.commonLabels \\\"context\\\" $ ) | nindent 4 }}\\n    {{- end }}\\n  {{- if .Values.singleuser.extraEnvVars }}\\n  extraEnv: {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.extraEnvVars \\\"context\\\" $ ) | nindent 4 }}\\n  {{- end }}\\n  {{- if .Values.singleuser.lifecycleHooks }}\\n  lifecycleHooks: {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.lifecycleHooks \\\"context\\\" $ ) | nindent 4 }}\\n  {{- end }}\\n  {{- if .Values.singleuser.initContainers }}\\n  initContainers: {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.initContainers \\\"context\\\" $ ) | nindent 4 }}\\n  {{- end }}\\n  {{- if .Values.singleuser.sidecars }}\\n  extraContainers: {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.sidecars \\\"context\\\" $ ) | nindent 4 }}\\n  {{- end }}\\n  {{- if .Values.singleuser.containerSecurityContext.enabled }}\\n  uid: {{ .Values.singleuser.containerSecurityContext.runAsUser }}\\n  {{- end }}\\n  {{- if .Values.singleuser.podSecurityContext.enabled }}\\n  fsGid: {{ .Values.singleuser.podSecurityContext.fsGroup }}\\n  {{- end }}\\n  serviceAccountName: {{ template \\\"jupyterhub.singleuserServiceAccountName\\\" . }}\\n  storage:\\n    {{- if .Values.singleuser.persistence.enabled }}\\n    type: dynamic\\n    {{- else }}\\n    type: none\\n    {{- end }}\\n    extraLabels:\\n      app.kubernetes.io/component: singleuser\\n      {{- include \\\"common.labels.standard\\\" . | nindent 6 }}\\n    {{- if .Values.singleuser.extraVolumes }}\\n    extraVolumes: {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.extraVolumes \\\"context\\\" $ ) | nindent 4 }}\\n    {{- end }}\\n    {{- if .Values.singleuser.extraVolumeMounts }}\\n    extraVolumeMounts: {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.extraVolumeMounts \\\"context\\\" $ ) | nindent 4 }}\\n    {{- end }}\\n    capacity: {{ .Values.singleuser.persistence.size }}\\n    homeMountPath: {{ .Values.singleuser.notebookDir }}\\n    dynamic:\\n      {{ include \\\"jupyterhub.storage.class\\\" (dict \\\"persistence\\\" .Values.singleuser.persistence \\\"global\\\" .Values.global) }}\\n      pvcNameTemplate: {{ include \\\"common.names.fullname\\\" . }}-claim-{username}{servername}\\n      volumeNameTemplate: {{ include \\\"common.names.fullname\\\" . }}-volume-{username}{servername}\\n      storageAccessModes: {{- include \\\"common.tplvalues.render\\\" ( dict \\\"value\\\" .Values.singleuser.persistence.accessModes \\\"context\\\" $ ) | nindent 8 }}\\n  image:\\n    name: {{ include \\\"jupyterhub.hubconfiguration.imageEntry\\\" ( dict \\\"imageRoot\\\" .Values.singleuser.image \\\"global\\\" $) }}\\n    tag: {{ .Values.singleuser.image.tag }}\\n    pullPolicy: {{ .Values.singleuser.image.pullPolicy }}\\n    pullSecrets: {{- include \\\"jupyterhub.imagePullSecrets.list\\\" . | nindent 8 }}\\n  startTimeout: 300\\n  {{- /* We need to replace the Kubernetes memory/cpu terminology (e.g. 10Gi, 10Mi) with one compatible with Python (10G, 10M) */}}\\n  cpu:\\n    limit: {{ regexReplaceAll \\\"([A-Za-z])i\\\" (default \\\"\\\" .Values.singleuser.resources.limits.cpu)  \\\"${1}\\\"}}\\n    guarantee: {{ regexReplaceAll \\\"([A-Za-z])i\\\" (default \\\"\\\" .Values.singleuser.resources.requests.cpu) \\\"${1}\\\" }}\\n  memory:\\n    limit: {{ regexReplaceAll \\\"([A-Za-z])i\\\" (default \\\"\\\" .Values.singleuser.resources.limits.memory) \\\"${1}\\\" }}\\n    guarantee: {{ regexReplaceAll \\\"([A-Za-z])i\\\" (default \\\"\\\" .Values.singleuser.resources.requests.memory) \\\"${1}\\\" }}\\n  {{- if .Values.singleuser.command }}\\n  cmd: {{- include \\\"common.tplvalues.render\\\" (dict \\\"value\\\" .Values.singleuser.command \\\"context\\\" $) | nindent 12 }}\\n  {{- else }}\\n  cmd: jupyterhub-singleuser\\n  {{- end }}\\n  defaultUrl:\\ncull:\\n  enabled: true\\n  users: false\\n  removeNamedServers: false\\n  timeout: 3600\\n  every: 600\\n  concurrency: 10\\n  maxAge: 0\\n\",\"containerPort\":8081,\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1000},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"existingConfigmap\":\"\",\"existingSecret\":\"\",\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/jupyterhub\",\"tag\":\"1.5.0-debian-10-r39\"},\"initContainers\":[],\"lifecycleHooks\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":30,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":3},\"metrics\":{\"authenticatePrometheus\":false,\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"namespace\":\"\",\"path\":\"/hub/metrics\",\"relabellings\":[],\"scrapeTimeout\":\"\"}},\"networkPolicy\":{\"allowInterspaceAccess\":true,\"enabled\":true,\"extraEgress\":\"## Hub --\\u003e Any IP:PORT\\n##\\n- to:\\n\",\"extraIngress\":\"\"},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"password\":\"\",\"pdb\":{\"create\":false,\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"priorityClassName\":\"\",\"rbac\":{\"create\":true},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":30,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":3},\"resources\":{\"limits\":{},\"requests\":{}},\"service\":{\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"http\":\"\"},\"port\":8081,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"create\":true,\"name\":\"\"},\"sidecars\":[],\"startupProbe\":{\"enabled\":true,\"failureThreshold\":30,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":3},\"tolerations\":[],\"updateStrategy\":{\"type\":\"RollingUpdate\"}},\"imagePuller\":{\"affinity\":{},\"args\":[],\"command\":[],\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1000},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"enabled\":false,\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"initContainers\":[],\"lifecycleHooks\":{},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1000},\"priorityClassName\":\"\",\"resources\":{\"limits\":{},\"requests\":{}},\"sidecars\":[],\"tolerations\":[],\"updateStrategy\":{\"type\":\"RollingUpdate\"}},\"kubeVersion\":\"\",\"nameOverride\":\"\",\"postgresql\":{\"enabled\":true,\"existingSecret\":\"\",\"nameOverride\":\"\",\"persistence\":{\"accessMode\":\"ReadWriteOnce\",\"enabled\":true,\"existingClaim\":\"\",\"size\":\"8Gi\",\"storageClass\":\"\"},\"postgresqlDatabase\":\"bitnami_jupyterhub\",\"postgresqlPassword\":\"\",\"postgresqlUsername\":\"bn_jupyterhub\",\"service\":{\"port\":5432}},\"proxy\":{\"affinity\":{},\"args\":[],\"command\":[],\"containerPort\":{\"api\":8001,\"http\":8000,\"metrics\":8002},\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1000},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"image\":{\"debug\":false,\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/configurable-http-proxy\",\"tag\":\"4.5.0-debian-10-r150\"},\"ingress\":{\"annotations\":{},\"apiVersion\":\"\",\"enabled\":false,\"extraHosts\":[],\"extraPaths\":[],\"extraTls\":[],\"hostname\":\"jupyterhub.local\",\"ingressClassName\":\"\",\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"secrets\":[],\"selfSigned\":false,\"tls\":false},\"initContainers\":[],\"lifecycleHooks\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":30,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":3},\"metrics\":{\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"namespace\":\"\",\"path\":\"/metrics\",\"relabellings\":[],\"scrapeTimeout\":\"\"}},\"networkPolicy\":{\"allowInterspaceAccess\":true,\"enabled\":true,\"extraEgress\":\"\",\"extraIngress\":\"## Any IP --\\u003e Proxy\\n##\\n- ports:\\n    - port: {{ .Values.proxy.containerPort.http }}\\n\"},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"pdb\":{\"create\":false,\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1000},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":30,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":3},\"resources\":{\"limits\":{},\"requests\":{}},\"secretToken\":\"\",\"service\":{\"api\":{\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"http\":\"\"},\"port\":8001,\"type\":\"ClusterIP\"},\"metrics\":{\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"http\":\"\"},\"port\":8002,\"type\":\"ClusterIP\"},\"public\":{\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"http\":\"\"},\"port\":80,\"type\":\"ClusterIP\"}},\"sidecars\":[],\"startupProbe\":{\"enabled\":true,\"failureThreshold\":30,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":3},\"tolerations\":[],\"updateStrategy\":{\"type\":\"RollingUpdate\"}},\"singleuser\":{\"command\":[\"jupyter-labhub\"],\"containerPort\":8888,\"containerSecurityContext\":{\"enabled\":true,\"runAsUser\":1000},\"extraEnvVars\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"pullPolicy\":\"Always\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"maximechartier/lakehouse-notebook\",\"tag\":\"spark-3.2.0\"},\"initContainers\":[],\"lifecycleHooks\":{},\"networkPolicy\":{\"allowCloudMetadataAccess\":false,\"allowInterspaceAccess\":true,\"enabled\":false,\"extraEgress\":\"\",\"extraIngress\":\"\"},\"nodeSelector\":{},\"notebookDir\":\"/opt/bitnami/jupyterhub-singleuser\",\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"enabled\":true,\"size\":\"10Gi\",\"storageClass\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1000},\"priorityClassName\":\"\",\"resources\":{\"limits\":{},\"requests\":{}},\"serviceAccount\":{\"create\":true,\"name\":\"\"},\"sidecars\":[],\"tolerations\":[]}}",
                "version": "0.3.6"
              }
            ],
            "name": "jupyterhub",
            "namespace": "jupyterhub",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.bitnami.com/bitnami",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1000,
            "values": [
              "## @section Global parameters\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass\n\n## @param global.imageRegistry Global Docker image registry\n## @param global.imagePullSecrets Global Docker registry secret names as an array\n## @param global.storageClass Global StorageClass for Persistent Volume(s)\n##\nglobal:\n  imageRegistry: \"\"\n  ## E.g.\n  ## imagePullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  imagePullSecrets: []\n  storageClass: \"\"\n\n## @section Common parameters\n\n## @param kubeVersion Override Kubernetes version\n##\nkubeVersion: \"\"\n## @param nameOverride String to partially override common.names.fullname (will maintain the release name)\n##\nnameOverride: \"\"\n## @param fullnameOverride String to fully override common.names.fullname\n##\nfullnameOverride: \"\"\n## @param commonLabels  Labels to add to all deployed objects\n##\ncommonLabels: {}\n## @param commonAnnotations  Annotations to add to all deployed objects\n##\ncommonAnnotations: {}\n## @param extraDeploy Array of extra objects to deploy with the release\n##\nextraDeploy: []\n\n## @section Hub deployment parameters\n\n## Hub deployment parameters\n##\nhub:\n  image:\n    ## @param hub.image.registry Hub image registry\n    ##\n    registry: docker.io\n    ## @param hub.image.repository Hub image repository\n    ##\n    repository: bitnami/jupyterhub\n    ## @param hub.image.tag Hub image tag (immutabe tags are recommended)\n    ##\n    tag: 1.5.0-debian-10-r39\n    ## Specify a imagePullPolicy\n    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n    ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n    ##\n    ## @param hub.image.pullPolicy Hub image pull policy\n    ##\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets.\n    ## Secrets must be manually created in the namespace.\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## e.g:\n    ## pullSecrets:\n    ##  - myRegistryKeySecretName\n    ## @param hub.image.pullSecrets Hub image pull secrets\n    ##\n    pullSecrets: []\n  ## Configure extra options for startup probes\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-startup-readiness-probes/#configure-probes\n  ## @param hub.startupProbe.enabled Enable startupProbe\n  ## @param hub.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n  ## @param hub.startupProbe.periodSeconds Period seconds for startupProbe\n  ## @param hub.startupProbe.timeoutSeconds Timeout seconds for startupProbe\n  ## @param hub.startupProbe.failureThreshold Failure threshold for startupProbe\n  ## @param hub.startupProbe.successThreshold Success threshold for startupProbe\n  ##\n  startupProbe:\n    enabled: true\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    failureThreshold: 30\n    timeoutSeconds: 3\n    successThreshold: 1\n  ## Configure extra options for liveness and readiness probes\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param hub.livenessProbe.enabled Enable livenessProbe\n  ## @param hub.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param hub.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param hub.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param hub.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param hub.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    failureThreshold: 30\n    timeoutSeconds: 3\n    successThreshold: 1\n  ## @param hub.readinessProbe.enabled Enable readinessProbe\n  ## @param hub.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param hub.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param hub.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param hub.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param hub.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    failureThreshold: 30\n    timeoutSeconds: 3\n    successThreshold: 1\n  ## @param hub.baseUrl Hub base URL\n  ##\n  baseUrl: /\n  ## @param hub.adminUser Hub Dummy authenticator admin user\n  ##\n  adminUser: user\n  ## @param hub.password Hub Dummy authenticator password\n  ##\n  password: \"\"\n  ## Configuration file passed to the hub. This will be used by the jupyterhub_config.py file\n  ## This configuration uses the values for the `singleuser` section. In the upstream chart the\n  ## values.yaml file is mounted in the hub container. This is chart, we tried to separate both\n  ## configuration so we could follow the Bitnami value standards\n  ## @param hub.configuration [string] Hub configuration file (to be used by jupyterhub_config.py)\n  ##\n  configuration: |\n    Chart:\n      Name: {{ .Chart.Name }}\n      Version: {{ .Chart.Version }}\n    Release:\n      Name: {{ .Release.Name }}\n      Namespace: {{ .Release.Namespace }}\n      Service: {{ .Release.Service }}\n    hub:\n      config:\n        JupyterHub:\n          admin_access: true\n          authenticator_class: dummy\n          cookie_secret_file: /tmp/jupyterhub_cookie_secret\n          DummyAuthenticator:\n          {{- if .Values.hub.password }}\n            password: {{ .Values.hub.password | quote }}\n          {{- else }}\n            password: {{ randAlphaNum 10 | quote }}\n          {{- end }}\n          Authenticator:\n            admin_users:\n              - {{ .Values.hub.adminUser }}\n      cookieSecret:\n      concurrentSpawnLimit: 64\n      consecutiveFailureLimit: 5\n      activeServerLimit:\n      db:\n        type: postgres\n        url: postgresql://{{ ternary .Values.postgresql.postgresqlUsername .Values.externalDatabase.user .Values.postgresql.enabled }}@{{ ternary (include \"jupyterhub.postgresql.fullname\" .) .Values.externalDatabase.host .Values.postgresql.enabled }}:{{ ternary \"5432\" .Values.externalDatabase.port .Values.postgresql.enabled }}/{{ ternary .Values.postgresql.postgresqlDatabase .Values.externalDatabase.database .Values.postgresql.enabled }}\n      services: {}\n      allowNamedServers: false\n      namedServerLimitPerUser:\n      {{- if .Values.hub.metrics.serviceMonitor.enabled }}\n      authenticatePrometheus: {{ .Values.hub.metrics.authenticatePrometheus }}\n      {{- end }}\n      redirectToServer:\n      shutdownOnLogout:\n    singleuser:\n      podNameTemplate: {{ include \"common.names.fullname\" . }}-jupyter-{username}\n      {{- if .Values.singleuser.tolerations }}\n      extraTolerations: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.tolerations \"context\" $) | nindent 4 }}\n      {{- end }}\n      {{- if .Values.singleuser.nodeSelector }}\n      nodeSelector: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.nodeSelector \"context\" $) | nindent 4 }}\n      {{- end }}\n      networkTools:\n        image:\n          name: {{ include \"jupyterhub.hubconfiguration.imageEntry\" ( dict \"imageRoot\" .Values.auxiliaryImage \"global\" $) }}\n          tag: {{ .Values.auxiliaryImage.tag }}\n          pullPolicy: {{ .Values.auxiliaryImage.pullPolicy }}\n          pullSecrets: {{- include \"jupyterhub.imagePullSecrets.list\" . | nindent 8 }}\n      cloudMetadata:\n        blockWithIptables: false\n      events: true\n      extraAnnotations:\n        {{- if .Values.commonAnnotations }}\n        {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.commonAnnotations \"context\" $ ) | nindent 4 }}\n        {{- end }}\n        {{- if .Values.singleuser.podAnnotations }}\n        {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.podAnnotations \"context\" $ ) | nindent 4 }}\n        {{- end }}\n      extraLabels:\n        hub.jupyter.org/network-access-hub: \"true\"\n        app.kubernetes.io/component: singleuser\n        {{- include \"common.labels.standard\" . | nindent 4 }}\n        {{- if .Values.commonLabels }}\n        {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.commonLabels \"context\" $ ) | nindent 4 }}\n        {{- end }}\n        {{- if .Values.singleuser.podLabels }}\n        {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.commonLabels \"context\" $ ) | nindent 4 }}\n        {{- end }}\n      {{- if .Values.singleuser.extraEnvVars }}\n      extraEnv: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.extraEnvVars \"context\" $ ) | nindent 4 }}\n      {{- end }}\n      {{- if .Values.singleuser.lifecycleHooks }}\n      lifecycleHooks: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.lifecycleHooks \"context\" $ ) | nindent 4 }}\n      {{- end }}\n      {{- if .Values.singleuser.initContainers }}\n      initContainers: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.initContainers \"context\" $ ) | nindent 4 }}\n      {{- end }}\n      {{- if .Values.singleuser.sidecars }}\n      extraContainers: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.sidecars \"context\" $ ) | nindent 4 }}\n      {{- end }}\n      {{- if .Values.singleuser.containerSecurityContext.enabled }}\n      uid: {{ .Values.singleuser.containerSecurityContext.runAsUser }}\n      {{- end }}\n      {{- if .Values.singleuser.podSecurityContext.enabled }}\n      fsGid: {{ .Values.singleuser.podSecurityContext.fsGroup }}\n      {{- end }}\n      serviceAccountName: {{ template \"jupyterhub.singleuserServiceAccountName\" . }}\n      storage:\n        {{- if .Values.singleuser.persistence.enabled }}\n        type: dynamic\n        {{- else }}\n        type: none\n        {{- end }}\n        extraLabels:\n          app.kubernetes.io/component: singleuser\n          {{- include \"common.labels.standard\" . | nindent 6 }}\n        {{- if .Values.singleuser.extraVolumes }}\n        extraVolumes: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.extraVolumes \"context\" $ ) | nindent 4 }}\n        {{- end }}\n        {{- if .Values.singleuser.extraVolumeMounts }}\n        extraVolumeMounts: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.extraVolumeMounts \"context\" $ ) | nindent 4 }}\n        {{- end }}\n        capacity: {{ .Values.singleuser.persistence.size }}\n        homeMountPath: {{ .Values.singleuser.notebookDir }}\n        dynamic:\n          {{ include \"jupyterhub.storage.class\" (dict \"persistence\" .Values.singleuser.persistence \"global\" .Values.global) }}\n          pvcNameTemplate: {{ include \"common.names.fullname\" . }}-claim-{username}{servername}\n          volumeNameTemplate: {{ include \"common.names.fullname\" . }}-volume-{username}{servername}\n          storageAccessModes: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.singleuser.persistence.accessModes \"context\" $ ) | nindent 8 }}\n      image:\n        name: {{ include \"jupyterhub.hubconfiguration.imageEntry\" ( dict \"imageRoot\" .Values.singleuser.image \"global\" $) }}\n        tag: {{ .Values.singleuser.image.tag }}\n        pullPolicy: {{ .Values.singleuser.image.pullPolicy }}\n        pullSecrets: {{- include \"jupyterhub.imagePullSecrets.list\" . | nindent 8 }}\n      startTimeout: 300\n      {{- /* We need to replace the Kubernetes memory/cpu terminology (e.g. 10Gi, 10Mi) with one compatible with Python (10G, 10M) */}}\n      cpu:\n        limit: {{ regexReplaceAll \"([A-Za-z])i\" (default \"\" .Values.singleuser.resources.limits.cpu)  \"${1}\"}}\n        guarantee: {{ regexReplaceAll \"([A-Za-z])i\" (default \"\" .Values.singleuser.resources.requests.cpu) \"${1}\" }}\n      memory:\n        limit: {{ regexReplaceAll \"([A-Za-z])i\" (default \"\" .Values.singleuser.resources.limits.memory) \"${1}\" }}\n        guarantee: {{ regexReplaceAll \"([A-Za-z])i\" (default \"\" .Values.singleuser.resources.requests.memory) \"${1}\" }}\n      {{- if .Values.singleuser.command }}\n      cmd: {{- include \"common.tplvalues.render\" (dict \"value\" .Values.singleuser.command \"context\" $) | nindent 12 }}\n      {{- else }}\n      cmd: jupyterhub-singleuser\n      {{- end }}\n      defaultUrl:\n    cull:\n      enabled: true\n      users: false\n      removeNamedServers: false\n      timeout: 3600\n      every: 600\n      concurrency: 10\n      maxAge: 0\n  ## @param hub.containerPort Hub container port\n  ##\n  containerPort: 8081\n  ## @param hub.existingConfigmap Configmap with Hub init scripts (replaces the scripts in templates/hub/configmap.yml)\n  ##\n  existingConfigmap: \"\"\n  ## @param hub.existingSecret Secret with hub configuration (replaces the hub.configuration value) and proxy token\n  ##\n  existingSecret: \"\"\n  ## @param hub.command Override Hub default command\n  ##\n  command: []\n  ## @param hub.args Override Hub default args\n  ##\n  args: []\n  pdb:\n    ## @param hub.pdb.create Deploy Hub PodDisruptionBudget\n    ##\n    create: false\n    ## @param hub.pdb.minAvailable Set minimum available hub instances\n    ##\n    minAvailable: \"\"\n    ## @param hub.pdb.maxUnavailable Set maximum available hub instances\n    ##\n    maxUnavailable: \"\"\n  ## @param hub.priorityClassName Hub pod priority class name\n  ##\n  priorityClassName: \"\"\n  ## @param hub.hostAliases Add deployment host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ##\n  hostAliases: []\n  ## hub resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param hub.resources.limits The resources limits for the container\n  ## @param hub.resources.requests The requested resources for the container\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##   cpu: 200m\n    ##   memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##   cpu: 200m\n    ##   memory: 10Mi\n    requests: {}\n  ## hub containers' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param hub.containerSecurityContext.enabled Enabled Hub containers' Security Context\n  ## @param hub.containerSecurityContext.runAsUser Set Hub container's Security Context runAsUser\n  ## @param hub.containerSecurityContext.runAsNonRoot Set Hub container's Security Context runAsNonRoot\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1000\n    runAsNonRoot: true\n  ## hub pods' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n  ## @param hub.podSecurityContext.enabled Enabled Hub pods' Security Context\n  ## @param hub.podSecurityContext.fsGroup Set Hub pod's Security Context fsGroup\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1001\n  ## Pod affinity preset\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ## Allowed values: soft, hard\n  ## @param hub.podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ##\n  podAffinityPreset: \"\"\n  ## Pod anti-affinity preset\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ## @param hub.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ##\n  podAntiAffinityPreset: soft\n  ## Node affinity preset\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n  ## @param hub.nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ## @param hub.nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set\n  ## @param hub.nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set\n  ##\n  nodeAffinityPreset:\n    type: \"\"\n    ## E.g.\n    ## key: \"kubernetes.io/e2e-az-name\"\n    ##\n    key: \"\"\n    ## E.g.\n    ## values:\n    ##   - e2e-az1\n    ##   - e2e-az2\n    ##\n    values: []\n  ## @param hub.affinity  Affinity for pod assignment.\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ##\n  affinity: {}\n  ## @param hub.nodeSelector Node labels for pod assignment.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param hub.tolerations Tolerations for pod assignment.\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## @param hub.podLabels Pod extra labels\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n  ##\n  podLabels: {}\n  ## @param hub.podAnnotations Annotations for server pods.\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n  ##\n  podAnnotations: {}\n  ## @param hub.lifecycleHooks LifecycleHooks for the hub container to automate configuration before or after startup\n  ##\n  lifecycleHooks: {}\n  ## @param hub.customStartupProbe Override default startup probe\n  ##\n  customStartupProbe: {}\n  ## @param hub.customLivenessProbe Override default liveness probe\n  ##\n  customLivenessProbe: {}\n  ## @param hub.customReadinessProbe Override default readiness probe\n  ##\n  customReadinessProbe: {}\n  ## @param hub.updateStrategy.type Update strategy - only really applicable for deployments with RWO PVs attached\n  ## If replicas = 1, an update can get \"stuck\", as the previous pod remains attached to the\n  ## PV, and the \"incoming\" pod can never start. Changing the strategy to \"Recreate\" will\n  ## terminate the single previous pod, so that the new, incoming pod can attach to the PV\n  ##\n  updateStrategy:\n    type: RollingUpdate\n  ## @param hub.extraEnvVars Add extra environment variables to the Hub container\n  ## Example:\n  ## extraEnvVars:\n  ##   - name: FOO\n  ##     value: \"bar\"\n  ##\n  extraEnvVars: []\n  ## @param hub.extraEnvVarsCM Name of existing ConfigMap containing extra env vars\n  ##\n  extraEnvVarsCM: \"\"\n  ## @param hub.extraEnvVarsSecret Name of existing Secret containing extra env vars\n  ##\n  extraEnvVarsSecret: \"\"\n  ## @param hub.extraVolumes Optionally specify extra list of additional volumes for Hub pods\n  ##\n  extraVolumes: []\n  ## @param hub.extraVolumeMounts Optionally specify extra list of additional volumeMounts for Hub container(s)\n  ##\n  extraVolumeMounts: []\n  ## @param hub.initContainers Add additional init containers to the Hub pods\n  ## Example:\n  ## initContainers:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  initContainers: []\n  ## @param hub.sidecars Add additional sidecar containers to the Hub pod\n  ## Example:\n  ## sidecars:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  sidecars: []\n\n  ## @section Hub RBAC parameters\n\n  ## ServiceAccount parameters\n  ##\n  serviceAccount:\n    ## @param hub.serviceAccount.create Specifies whether a ServiceAccount should be created\n    ##\n    create: true\n    ## @param hub.serviceAccount.name Override Hub service account name\n    ## If not set and create is true, a name is generated using the fullname template\n    ##\n    name: \"\"\n  ## RBAC resources\n  ##\n  rbac:\n    ## @param hub.rbac.create Specifies whether RBAC resources should be created\n    ##\n    create: true\n\n  ## @section Hub Traffic Exposure Parameters\n\n  ## Network policy\n  ##\n  networkPolicy:\n    ## @param hub.networkPolicy.enabled Deploy Hub network policies\n    ##\n    enabled: true\n    ## @param hub.networkPolicy.allowInterspaceAccess Allow communication between pods in different namespaces\n    ##\n    allowInterspaceAccess: true\n    ## @param hub.networkPolicy.extraIngress Add extra ingress rules to the NetworkPolicy\n    ##\n    extraIngress: \"\"\n    ## @param hub.networkPolicy.extraEgress [string] Add extra ingress rules to the NetworkPolicy\n    ##\n    extraEgress: |\n      ## Hub --\u003e Any IP:PORT\n      ##\n      - to:\n  service:\n    ## @param hub.service.type Hub service type\n    ##\n    type: ClusterIP\n    ## @param hub.service.port Hub service HTTP port\n    port: 8081\n    ## @param hub.service.loadBalancerIP Hub service LoadBalancer IP (optional, cloud specific)\n    ## ref: https://kubernetes.io/docs/user-guide/services/#type-loadbalancer\n    ##\n    loadBalancerIP: \"\"\n    ## @param hub.service.loadBalancerSourceRanges loadBalancerIP source ranges for the Service\n    ##\n    loadBalancerSourceRanges: []\n    ## @param hub.service.nodePorts.http NodePort for the HTTP endpoint\n    ## nodePorts:\n    ##   http: \u003cto set explicitly, choose port between 30000-32767\u003e\n    ##   https: \u003cto set explicitly, choose port between 30000-32767\u003e\n    ##\n    nodePorts:\n      http: \"\"\n    ## @param hub.service.externalTrafficPolicy External traffic policy for the service\n    ## Enable client source IP preservation\n    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n    ##\n    externalTrafficPolicy: Cluster\n\n  ## @section Hub Metrics parameters\n\n  metrics:\n    ## @param hub.metrics.authenticatePrometheus Use authentication for Prometheus\n    ## To allow public access without authentication for prometheus metrics set environment as follows.\n    ##\n    authenticatePrometheus: false\n    ## Prometheus Operator ServiceMonitor configuration\n    ##\n    serviceMonitor:\n      ## @param hub.metrics.serviceMonitor.enabled If the operator is installed in your cluster, set to true to create a Service Monitor Entry\n      ##\n      enabled: false\n      ## @param hub.metrics.serviceMonitor.namespace Namespace which Prometheus is running in\n      ##\n      namespace: \"\"\n      ## @param hub.metrics.serviceMonitor.path HTTP path to scrape for metrics\n      ##\n      path: /hub/metrics\n      ## @param hub.metrics.serviceMonitor.interval Interval at which metrics should be scraped\n      ##\n      interval: 30s\n      ## @param hub.metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended\n      ## e.g:\n      ## scrapeTimeout: 30s\n      scrapeTimeout: \"\"\n      ## @param hub.metrics.serviceMonitor.relabellings Specify Metric Relabellings to add to the scrape endpoint\n      ##\n      relabellings: []\n      ## @param hub.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint\n      ##\n      honorLabels: false\n      ## @param hub.metrics.serviceMonitor.additionalLabels Used to pass Labels that are required by the installed Prometheus Operator\n      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec\n      ##\n      additionalLabels: {}\n\n## @section Proxy deployment parameters\n\n## Proxy deployment parameters\n##\nproxy:\n  ## @param proxy.image.registry Proxy image registry\n  ## @param proxy.image.repository Proxy image repository\n  ## @param proxy.image.tag Proxy image tag (immutable tags are recommended)\n  ## @param proxy.image.pullPolicy Proxy image pull policy\n  ## @param proxy.image.pullSecrets Proxy image pull secrets\n  ## @param proxy.image.debug Activate verbose output\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/configurable-http-proxy\n    tag: 4.5.0-debian-10-r150\n    ## Specify a imagePullPolicy\n    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n    ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n    ##\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets.\n    ## Secrets must be manually created in the namespace.\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## e.g:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n    ## Enable debug mode\n    ##\n    debug: false\n  ## Configure extra options for startup probes\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-startup-readiness-probes/#configure-probes\n  ## @param proxy.startupProbe.enabled Enable startupProbe\n  ## @param proxy.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n  ## @param proxy.startupProbe.periodSeconds Period seconds for startupProbe\n  ## @param proxy.startupProbe.timeoutSeconds Timeout seconds for startupProbe\n  ## @param proxy.startupProbe.failureThreshold Failure threshold for startupProbe\n  ## @param proxy.startupProbe.successThreshold Success threshold for startupProbe\n  ##\n  startupProbe:\n    enabled: true\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    failureThreshold: 30\n    timeoutSeconds: 3\n    successThreshold: 1\n  ## Configure extra options for liveness and readiness probes\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param proxy.livenessProbe.enabled Enable livenessProbe\n  ## @param proxy.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param proxy.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param proxy.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param proxy.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param proxy.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    failureThreshold: 30\n    timeoutSeconds: 3\n    successThreshold: 1\n  ## @param proxy.readinessProbe.enabled Enable readinessProbe\n  ## @param proxy.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param proxy.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param proxy.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param proxy.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param proxy.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    failureThreshold: 30\n    timeoutSeconds: 3\n    successThreshold: 1\n  ## @param proxy.command Override Proxy default command\n  ##\n  command: []\n  ## @param proxy.args Override Proxy default args\n  ##\n  args: []\n  ## @param proxy.secretToken Proxy secret token (used for communication with the Hub)\n  ##\n  secretToken: \"\"\n  ## Deployment pod host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ## @param proxy.hostAliases Add deployment host aliases\n  ##\n  hostAliases: []\n  ## PodDisruptionBudget settings\n  ##\n  pdb:\n    ## @param proxy.pdb.create Deploy Proxy PodDisruptionBudget\n    ##\n    create: false\n    ## @param proxy.pdb.minAvailable Set minimum available proxy instances\n    ##\n    minAvailable: \"\"\n    ## @param proxy.pdb.maxUnavailable Set maximum available proxy instances\n    ##\n    maxUnavailable: \"\"\n  ## Container ports\n  ##\n  containerPort:\n    ## @param proxy.containerPort.api Proxy api container port\n    ##\n    api: 8001\n    ## @param proxy.containerPort.metrics Proxy metrics container port\n    ##\n    metrics: 8002\n    ## @param proxy.containerPort.http Proxy http container port\n    ##\n    http: 8000\n  ## @param proxy.priorityClassName Proxy pod priority class name\n  ##\n  priorityClassName: \"\"\n  ## proxy resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param proxy.resources.limits The resources limits for the container\n  ## @param proxy.resources.requests The requested resources for the container\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##   cpu: 200m\n    ##   memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##   cpu: 200m\n    ##   memory: 10Mi\n    requests: {}\n  ## proxy containers' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param proxy.containerSecurityContext.enabled Enabled Proxy containers' Security Context\n  ## @param proxy.containerSecurityContext.runAsUser Set Proxy container's Security Context runAsUser\n  ## @param proxy.containerSecurityContext.runAsNonRoot Set Proxy container's Security Context runAsNonRoot\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1000\n    runAsNonRoot: true\n  ## Proxy pods' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n  ## @param proxy.podSecurityContext.enabled Enabled Proxy pods' Security Context\n  ## @param proxy.podSecurityContext.fsGroup Set Proxy pod's Security Context fsGroup\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1000\n  ## @param proxy.podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAffinityPreset: \"\"\n  ## @param proxy.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAntiAffinityPreset: soft\n  ## Node affinity preset\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n  ## Allowed values: soft, hard\n  ## @param proxy.nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ## @param proxy.nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set\n  ## @param proxy.nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set\n  ##\n  nodeAffinityPreset:\n    type: \"\"\n    ## E.g.\n    ## key: \"kubernetes.io/e2e-az-name\"\n    ##\n    key: \"\"\n    ## E.g.\n    ## values:\n    ##   - e2e-az1\n    ##   - e2e-az2\n    ##\n    values: []\n  ## @param proxy.affinity Affinity for pod assignment. Evaluated as a template.\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ##\n  affinity: {}\n  ## @param proxy.nodeSelector Node labels for pod assignment. Evaluated as a template.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param proxy.tolerations Tolerations for pod assignment. Evaluated as a template.\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## @param proxy.podLabels Extra labels for Proxy pods\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n  ##\n  podLabels: {}\n  ## @param proxy.podAnnotations Annotations for Proxy pods\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n  ##\n  podAnnotations: {}\n  ## @param proxy.lifecycleHooks Add lifecycle hooks to the Proxy deployment\n  ##\n  lifecycleHooks: {}\n  ## @param proxy.customStartupProbe Override default startup probe\n  ##\n  customStartupProbe: {}\n  ## @param proxy.customLivenessProbe Override default liveness probe\n  ##\n  customLivenessProbe: {}\n  ## @param proxy.customReadinessProbe Override default readiness probe\n  ##\n  customReadinessProbe: {}\n  ## @param proxy.updateStrategy.type Update strategy - only really applicable for deployments with RWO PVs attached\n  ## If replicas = 1, an update can get \"stuck\", as the previous pod remains attached to the\n  ## PV, and the \"incoming\" pod can never start. Changing the strategy to \"Recreate\" will\n  ## terminate the single previous pod, so that the new, incoming pod can attach to the PV\n  ##\n  updateStrategy:\n    type: RollingUpdate\n  ## @param proxy.extraEnvVars Add extra environment variables to the Proxy container\n  ## Example:\n  ## extraEnvVars:\n  ##   - name: FOO\n  ##     value: \"bar\"\n  ##\n  extraEnvVars: []\n  ## @param proxy.extraEnvVarsCM Name of existing ConfigMap containing extra env vars\n  ##\n  extraEnvVarsCM: \"\"\n  ## @param proxy.extraEnvVarsSecret Name of existing Secret containing extra env vars\n  ##\n  extraEnvVarsSecret: \"\"\n  ## @param proxy.extraVolumes Optionally specify extra list of additional volumes for Proxy pods\n  ##\n  extraVolumes: []\n  ## @param proxy.extraVolumeMounts Optionally specify extra list of additional volumeMounts for Proxy container(s)\n  ##\n  extraVolumeMounts: []\n  ## @param proxy.initContainers Add additional init containers to the Proxy pods\n  ## Example:\n  ## initContainers:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  initContainers: []\n  ## @param proxy.sidecars Add additional sidecar containers to the Proxy pod\n  ## Example:\n  ## sidecars:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  sidecars: []\n\n  ## @section Proxy Traffic Exposure Parameters\n\n  ## Network policy\n  ##\n  networkPolicy:\n    ## @param proxy.networkPolicy.enabled Deploy Proxy network policies\n    ##\n    enabled: true\n    ## @param proxy.networkPolicy.allowInterspaceAccess Allow communication between pods in different namespaces\n    ##\n    allowInterspaceAccess: true\n    ## @param proxy.networkPolicy.extraIngress [string] Add extra ingress rules to the NetworkPolicy\n    ##\n    extraIngress: |\n      ## Any IP --\u003e Proxy\n      ##\n      - ports:\n          - port: {{ .Values.proxy.containerPort.http }}\n    ## @param proxy.networkPolicy.extraEgress Add extra egress rules to the NetworkPolicy\n    ##\n    extraEgress: \"\"\n  service:\n    api:\n      ## @param proxy.service.api.type API service type\n      ##\n      type: ClusterIP\n      ## @param proxy.service.api.port API service port\n      ##\n      port: 8001\n      ## @param proxy.service.api.loadBalancerIP API service LoadBalancer IP (optional, cloud specific)\n      ## ref: https://kubernetes.io/docs/user-guide/services/#type-loadbalancer\n      ##\n      loadBalancerIP: \"\"\n      ## @param proxy.service.api.loadBalancerSourceRanges loadBalancerIP source ranges for the Service\n      ##\n      loadBalancerSourceRanges: []\n      ## @param proxy.service.api.nodePorts.http NodePort for the HTTP endpoint\n      ## nodePorts:\n      ##   http: \u003cto set explicitly, choose port between 30000-32767\u003e\n      ##   https: \u003cto set explicitly, choose port between 30000-32767\u003e\n      ##\n      nodePorts:\n        http: \"\"\n      ## @param proxy.service.api.externalTrafficPolicy External traffic policy for the service\n      ## Enable client source IP preservation\n      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n      ##\n      externalTrafficPolicy: Cluster\n    metrics:\n      ## @param proxy.service.metrics.type Metrics service type\n      ##\n      type: ClusterIP\n      ## @param proxy.service.metrics.port Metrics service port\n      ##\n      port: 8002\n      ## @param proxy.service.metrics.loadBalancerIP Metrics service LoadBalancer IP (optional, cloud specific)\n      ## ref: https://kubernetes.io/docs/user-guide/services/#type-loadbalancer\n      ##\n      loadBalancerIP: \"\"\n      ## @param proxy.service.metrics.loadBalancerSourceRanges loadBalancerIP source ranges for the Service\n      ##\n      loadBalancerSourceRanges: []\n      ## @param proxy.service.metrics.nodePorts.http NodePort for the HTTP endpoint\n      ## nodePorts:\n      ##   http: \u003cto set explicitly, choose port between 30000-32767\u003e\n      ##   https: \u003cto set explicitly, choose port between 30000-32767\u003e\n      ##\n      nodePorts:\n        http: \"\"\n      ## @param proxy.service.metrics.externalTrafficPolicy External traffic policy for the service\n      ## Enable client source IP preservation\n      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n      ##\n      externalTrafficPolicy: Cluster\n    public:\n      ## @param proxy.service.public.type Public service type\n      ##\n      type: ClusterIP\n      # HTTP Port\n      ## @param proxy.service.public.port Public service port\n      ##\n      port: 80\n      ## @param proxy.service.public.loadBalancerIP Public service LoadBalancer IP (optional, cloud specific)\n      ## ref: https://kubernetes.io/docs/user-guide/services/#type-loadbalancer\n      ##\n      loadBalancerIP: \"\"\n      ## @param proxy.service.public.loadBalancerSourceRanges loadBalancerIP source ranges for the Service\n      ##\n      loadBalancerSourceRanges: []\n      ## @param proxy.service.public.nodePorts.http NodePort for the HTTP endpoint\n      ## nodePorts:\n      ##   http: \u003cto set explicitly, choose port between 30000-32767\u003e\n      ##   https: \u003cto set explicitly, choose port between 30000-32767\u003e\n      ##\n      nodePorts:\n        http: \"\"\n      ## @param proxy.service.public.externalTrafficPolicy External traffic policy for the service\n      ## Enable client source IP preservation\n      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n      ##\n      externalTrafficPolicy: Cluster\n  ## Configure the ingress resource that allows you to access to your JupyterHub instance\n  ##\n  ingress:\n    ## @param proxy.ingress.enabled Set to true to enable ingress record generation\n    ##\n    enabled: false\n    ## @param proxy.ingress.apiVersion Force Ingress API version (automatically detected if not set)\n    ##\n    apiVersion: \"\"\n    ## @param proxy.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)\n    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster.\n    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/\n    ##\n    ingressClassName: \"\"\n    ## @param proxy.ingress.path Path to the Proxy pod.\n    ##\n    path: /\n    ## @param proxy.ingress.pathType Ingress path type\n    ##\n    pathType: ImplementationSpecific\n    ## DEPRECATED: Use ingress.annotations instead of ingress.certManager\n    ## certManager: false\n    ##\n\n    ## When the ingress is enabled, a host pointing to this will be created\n    ## @param proxy.ingress.hostname Set ingress rule hostname\n    ##\n    hostname: jupyterhub.local\n    ## @param proxy.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.\n    ## For a full list of possible ingress annotations, please see\n    ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md\n    ## Use this parameter to set the required annotations for cert-manager, see\n    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations\n    ##\n    ## e.g:\n    ## annotations:\n    ##   kubernetes.io/ingress.class: nginx\n    ##   cert-manager.io/cluster-issuer: cluster-issuer-name\n    ##\n    annotations: {}\n    ## @param proxy.ingress.tls Enable ingress tls configuration for the hostname defined at proxy.ingress.hostname\n    ## TLS certificates will be retrieved from a TLS secret with name: {{- printf \"%s-tls\" .Values.ingress.hostname }}\n    ## You can use the ingress.secrets parameter to create this TLS secret, relay on cert-manager to create it, or\n    ## let the chart create self-signed certificates for you\n    ##\n    tls: false\n    ## @param proxy.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm\n    ##\n    selfSigned: false\n    ## @param proxy.ingress.extraHosts The list of additional hostnames to be covered with this ingress record.\n    ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array\n    ## extraHosts:\n    ## - name: aspnet-core.local\n    ##   path: /\n    ##\n    extraHosts: []\n    ## @param proxy.ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.\n    ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls\n    ## extraTls:\n    ## - hosts:\n    ##     - aspnet-core.local\n    ##   secretName: aspnet-core.local-tls\n    ##\n    extraTls: []\n    ## @param proxy.ingress.extraPaths Any additional arbitrary paths that may need to be added to the ingress under the main host.\n    ## For example: The ALB ingress controller requires a special rule for handling SSL redirection.\n    ##\n    extraPaths: []\n    ## @param proxy.ingress.secrets Add extra secrets for the tls configuration\n    ## If you're providing your own certificates, please use this to add the certificates as secrets\n    ## key and certificate should start with -----BEGIN CERTIFICATE----- or -----BEGIN RSA PRIVATE KEY-----\n    ## name should line up with a secretName set further up\n    ##\n    ## If it is not set and you're using cert-manager, this is unneeded, as it will create the secret for you\n    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created\n    ## It is also possible to create and manage the certificates outside of this helm chart\n    ##\n    ## Please see README.md for more information\n    ## - name: aspnet-core.local-tls\n    ##   key:\n    ##   certificate:\n    ##\n    secrets: []\n\n  ## @section Proxy Metrics parameters\n\n  metrics:\n    ## Prometheus Operator ServiceMonitor configuration\n    ##\n    serviceMonitor:\n      ## @param proxy.metrics.serviceMonitor.enabled If the operator is installed in your cluster, set to true to create a Service Monitor Entry\n      ##\n      enabled: false\n      ## @param proxy.metrics.serviceMonitor.namespace Namespace which Prometheus is running in\n      ##\n      namespace: \"\"\n      ## @param proxy.metrics.serviceMonitor.path HTTP path to scrape for metrics\n      ##\n      path: /metrics\n      ## @param proxy.metrics.serviceMonitor.interval Interval at which metrics should be scraped\n      ##\n      interval: 30s\n      ## @param proxy.metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended\n      ## e.g:\n      ## scrapeTimeout: 30s\n      scrapeTimeout: \"\"\n      ## @param proxy.metrics.serviceMonitor.relabellings Specify Metric Relabellings to add to the scrape endpoint\n      ##\n      relabellings: []\n      ## @param proxy.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint\n      ##\n      honorLabels: false\n      ## @param proxy.metrics.serviceMonitor.additionalLabels Used to pass Labels that are required by the installed Prometheus Operator\n      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec\n      ##\n      additionalLabels: {}\n\n## @section Image puller deployment parameters\n\n## Image Puller deployment parameters\n##\nimagePuller:\n  ## @param imagePuller.enabled Deploy ImagePuller daemonset\n  ##\n  enabled: false\n  ## @param imagePuller.command Override ImagePuller default command\n  ##\n  command: []\n  ## @param imagePuller.args Override ImagePuller default args\n  ##\n  args: []\n  ## @param imagePuller.hostAliases Add deployment host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ##\n  hostAliases: []\n  ## imagePuller resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param imagePuller.resources.limits The resources limits for the container\n  ## @param imagePuller.resources.requests The requested resources for the container\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##   cpu: 200m\n    ##   memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##   cpu: 200m\n    ##   memory: 10Mi\n    requests: {}\n  ## imagePuller containers' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param imagePuller.containerSecurityContext.enabled Enabled ImagePuller containers' Security Context\n  ## @param imagePuller.containerSecurityContext.runAsUser Set ImagePuller container's Security Context runAsUser\n  ## @param imagePuller.containerSecurityContext.runAsNonRoot Set ImagePuller container's Security Context runAsNonRoot\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1000\n    runAsNonRoot: true\n  ## imagePuller pods' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n  ## @param imagePuller.podSecurityContext.enabled Enabled ImagePuller pods' Security Context\n  ## @param imagePuller.podSecurityContext.fsGroup Set ImagePuller pod's Security Context fsGroup\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1000\n  ## @param imagePuller.podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAffinityPreset: \"\"\n  ## @param imagePuller.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAntiAffinityPreset: soft\n  ## Node affinity preset\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n  ## @param imagePuller.nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ## @param imagePuller.nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set\n  ## @param imagePuller.nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set\n  ##\n  nodeAffinityPreset:\n    type: \"\"\n    ## E.g.\n    ## key: \"kubernetes.io/e2e-az-name\"\n    ##\n    key: \"\"\n    ## E.g.\n    ## values:\n    ##   - e2e-az1\n    ##   - e2e-az2\n    ##\n    values: []\n  ## @param imagePuller.affinity Affinity for pod assignment. Evaluated as a template.\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ##\n  affinity: {}\n  ## @param imagePuller.nodeSelector Node labels for pod assignment. Evaluated as a template.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param imagePuller.tolerations Tolerations for pod assignment. Evaluated as a template.\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## @param imagePuller.podLabels Pod extra labels\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n  ##\n  podLabels: {}\n  ## @param imagePuller.podAnnotations Annotations for ImagePuller pods\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n  ##\n  podAnnotations: {}\n  ## @param imagePuller.priorityClassName ImagePuller pod priority class name\n  ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n  ##\n  priorityClassName: \"\"\n  ## @param imagePuller.lifecycleHooks Add lifecycle hooks to the ImagePuller deployment\n  ##\n  lifecycleHooks: {}\n  ## @param imagePuller.customStartupProbe  Override default startup probe\n  ##\n  customStartupProbe: {}\n  ## @param imagePuller.customLivenessProbe  Override default liveness probe\n  ##\n  customLivenessProbe: {}\n  ## @param imagePuller.customReadinessProbe  Override default readiness probe\n  ##\n  customReadinessProbe: {}\n  ## @param imagePuller.updateStrategy.type Update strategy - only really applicable for deployments with RWO PVs attached\n  ## If replicas = 1, an update can get \"stuck\", as the previous pod remains attached to the\n  ## PV, and the \"incoming\" pod can never start. Changing the strategy to \"Recreate\" will\n  ## terminate the single previous pod, so that the new, incoming pod can attach to the PV\n  ##\n  updateStrategy:\n    type: RollingUpdate\n  ## @param imagePuller.extraEnvVars Add extra environment variables to the ImagePuller container\n  ## Example:\n  ## extraEnvVars:\n  ##   - name: FOO\n  ##     value: \"bar\"\n  ##\n  extraEnvVars: []\n  ## @param imagePuller.extraEnvVarsCM Name of existing ConfigMap containing extra env vars\n  ##\n  extraEnvVarsCM: \"\"\n  ## @param imagePuller.extraEnvVarsSecret Name of existing Secret containing extra env vars\n  ##\n  extraEnvVarsSecret: \"\"\n  ## @param imagePuller.extraVolumes  Optionally specify extra list of additional volumes for ImagePuller pods\n  ##\n  extraVolumes: []\n  ## @param imagePuller.extraVolumeMounts  Optionally specify extra list of additional volumeMounts for ImagePuller container(s)\n  ##\n  extraVolumeMounts: []\n  ## @param imagePuller.initContainers Add additional init containers to the ImagePuller pods\n  ## Example:\n  ## initContainers:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  initContainers: []\n  ## @param imagePuller.sidecars Add additional sidecar containers to the ImagePuller pod\n  ## Example:\n  ## sidecars:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  sidecars: []\n\n## @section Singleuser deployment parameters\n\n## Singleuser deployment parameters\n## NOTE: The values in this section are used for generating the hub.configuration value. In case you provide\n## a custom hub.configuration or a configmap, these will be ignored.\n## @param singleuser.image.registry Single User image registry\n## @param singleuser.image.repository Single User image repository\n## @param singleuser.image.tag Single User image tag (immutabe tags are recommended)\n## @param singleuser.image.pullPolicy Single User image pull policy\n## @param singleuser.image.pullSecrets Single User image pull secrets\n##\nsingleuser:\n  image:\n    registry: docker.io\n    repository: maximechartier/lakehouse-notebook\n    tag: spark-3.2.0\n    ## Specify a imagePullPolicy\n    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n    ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n    ##\n    ##\n    pullPolicy: Always\n    ## Optionally specify an array of imagePullSecrets.\n    ## Secrets must be manually created in the namespace.\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## e.g:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## Command for running the container (set to default if not set). Use array form\n  ## @param singleuser.command Override Single User default command\n  ##\n  command: [\"jupyter-labhub\"]\n  ## @param singleuser.tolerations Tolerations for pod assignment. Evaluated as a template.\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## @param singleuser.containerPort Single User container port\n  ##\n  containerPort: 8888\n  ## @param singleuser.notebookDir Notebook directory (it will be the same as the PVC volume mount)\n  ##\n  notebookDir: /opt/bitnami/jupyterhub-singleuser\n  ## singleuser resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param singleuser.resources.limits The resources limits for the container\n  ## @param singleuser.resources.requests The requested resources for the container\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##   cpu: 200m\n    ##   memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##   cpu: 200m\n    ##   memory: 10Mi\n    requests: {}\n  ## singleuser containers' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param singleuser.containerSecurityContext.enabled Enabled Single User containers' Security Context\n  ## @param singleuser.containerSecurityContext.runAsUser Set Single User container's Security Context runAsUser\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1000\n  ## singleuser pods' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n  ## @param singleuser.podSecurityContext.enabled Enabled Single User pods' Security Context\n  ## @param singleuser.podSecurityContext.fsGroup Set Single User pod's Security Context fsGroup\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1000\n  ## @param singleuser.nodeSelector Node labels for pod assignment. Evaluated as a template.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param singleuser.podLabels Extra labels for Single User pods\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n  ##\n  podLabels: {}\n  ## @param singleuser.podAnnotations Annotations for Single User pods\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n  ##\n  podAnnotations: {}\n  ## @param singleuser.priorityClassName Single User pod priority class name\n  ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n  ##\n  priorityClassName: \"\"\n  ## @param singleuser.lifecycleHooks Add lifecycle hooks to the Single User deployment to automate configuration before or after startup\n  ##\n  lifecycleHooks: {}\n  ## @param singleuser.extraEnvVars Add extra environment variables to the Single User container\n  ## Example:\n  ## extraEnvVars:\n  ##   - name: FOO\n  ##     value: \"bar\"\n  ##\n  extraEnvVars: []\n  ## @param singleuser.extraVolumes Optionally specify extra list of additional volumes for Single User pods\n  ##\n  extraVolumes: []\n  ## @param singleuser.extraVolumeMounts Optionally specify extra list of additional volumeMounts for Single User container(s)\n  ##\n  extraVolumeMounts: []\n  ## @param singleuser.initContainers  Add additional init containers to the Single User pods\n  ## Example:\n  ## initContainers:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  initContainers: []\n  ## @param singleuser.sidecars  Add additional sidecar containers to the Single User pod\n  ## Example:\n  ## sidecars:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  sidecars: []\n\n  ## @section Single User RBAC parameters\n\n  ## Service account parameters\n  ##\n  serviceAccount:\n    ## @param singleuser.serviceAccount.create Specifies whether a ServiceAccount should be created\n    ##\n    create: true\n    ## @param singleuser.serviceAccount.name Override Single User service account name\n    ## If not set and create is true, a name is generated using the fullname template\n    ##\n    name: \"\"\n\n  ## @section Single User Persistence parameters\n\n  ## Enable persistence using Persistent Volume Claims\n  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/\n  ##\n  persistence:\n    ## @param singleuser.persistence.enabled Enable persistent volume creation on Single User instances\n    ## If true, use a Persistent Volume Claim, If false, use emptyDir\n    ##\n    enabled: true\n    ## @param singleuser.persistence.storageClass Persistent Volumes storage class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    storageClass: \"\"\n    ## @param singleuser.persistence.accessModes Persistent Volumes access modes\n    ##\n    accessModes:\n      - ReadWriteOnce\n    ## @param singleuser.persistence.size Persistent Volumes size\n    ##\n    size: 10Gi\n\n  ## @section Traffic exposure parameters\n\n  ## Network policy\n  ##\n  networkPolicy:\n    ## @param singleuser.networkPolicy.enabled Deploy Single User network policies\n    ##\n    enabled: false\n    ## @param singleuser.networkPolicy.allowInterspaceAccess Allow communication between pods in different namespaces\n    ##\n    allowInterspaceAccess: true\n    ## @param singleuser.networkPolicy.allowCloudMetadataAccess Allow Single User pods to access Cloud Metada endpoints\n    ##\n    allowCloudMetadataAccess: false\n    ## @param singleuser.networkPolicy.extraIngress Add extra ingress rules to the NetworkPolicy\n    ##\n    extraIngress: \"\"\n    ## @param singleuser.networkPolicy.extraEgress Add extra egress rules to the NetworkPolicy\n    ##\n    extraEgress: \"\"\n\n## @section Auxiliary image parameters\n\n## @param auxiliaryImage.registry Auxiliary image registry\n## @param auxiliaryImage.repository Auxiliary image repository\n## @param auxiliaryImage.tag Auxiliary image tag (immutabe tags are recommended)\n## @param auxiliaryImage.pullPolicy Auxiliary image pull policy\n## @param auxiliaryImage.pullSecrets Auxiliary image pull secrets\n##\nauxiliaryImage:\n  registry: docker.io\n  repository: bitnami/bitnami-shell\n  tag: 10-debian-10-r285\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## e.g:\n  ## pullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  pullSecrets: []\n\n## @section External Database settings\n\n## External Database Configuration\n## All of these values are only used when postgresql.enabled is set to false\n##\nexternalDatabase:\n  ## @param externalDatabase.host Host of an external PostgreSQL instance to connect (only if postgresql.enabled=false)\n  ##\n  host: \"\"\n  ## @param externalDatabase.user User of an external PostgreSQL instance to connect (only if postgresql.enabled=false)\n  ##\n  user: postgres\n  ## @param externalDatabase.password Password of an external PostgreSQL instance to connect (only if postgresql.enabled=false)\n  ##\n  password: \"\"\n  ## @param externalDatabase.existingSecret Secret containing the password of an external PostgreSQL instance to connect (only if postgresql.enabled=false)\n  ## Name of an existing secret resource containing the DB password in a 'postgresql-password' key\n  ##\n  existingSecret: \"\"\n  ## @param externalDatabase.database Database inside an external PostgreSQL to connect (only if postgresql.enabled=false)\n  ##\n  database: jupyterhub\n  ## @param externalDatabase.port Port of an external PostgreSQL to connect (only if postgresql.enabled=false)\n  ##\n  port: 5432\n\n## @section PostgreSQL subchart settings\n\n## PostgreSQL chart configuration\n## https://github.com/bitnami/charts/blob/master/bitnami/postgresql/values.yaml\n##\npostgresql:\n  ## @param postgresql.enabled Deploy PostgreSQL subchart\n  ##\n  enabled: true\n  ## @param postgresql.nameOverride Override name of the PostgreSQL chart\n  ##\n  nameOverride: \"\"\n  ## @param postgresql.existingSecret Existing secret containing the password of the PostgreSQL chart\n  ##\n  existingSecret: \"\"\n  ## @param postgresql.postgresqlPassword Password for the postgres user of the PostgreSQL chart (auto-generated if not set)\n  ## ref: https://hub.docker.com/_/postgres/\n  ##\n  postgresqlPassword: \"\"\n  ## @param postgresql.postgresqlUsername Username to create when deploying the PostgreSQL chart\n  ##\n  postgresqlUsername: bn_jupyterhub\n  ## @param postgresql.postgresqlDatabase Database to create when deploying the PostgreSQL chart\n  ##\n  postgresqlDatabase: bitnami_jupyterhub\n  ## PostgreSQL service\n  ##\n  service:\n    ## @param postgresql.service.port PostgreSQL service port\n    ##\n    port: 5432\n  ## Enable persistence using Persistent Volume Claims\n  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/\n  ##\n  persistence:\n    ## @param postgresql.persistence.enabled Use PVCs when deploying the PostgreSQL chart\n    ##\n    enabled: true\n    ## @param postgresql.persistence.existingClaim Use an existing PVC when deploying the PostgreSQL chart\n    ##\n    existingClaim: \"\"\n    ## @param postgresql.persistence.storageClass storageClass of the created PVCs\n    ## postgresql data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    storageClass: \"\"\n    ## @param postgresql.persistence.accessMode Access mode of the created PVCs\n    ##\n    accessMode: ReadWriteOnce\n    ## @param postgresql.persistence.size Size of the created PVCs\n    ##\n    size: 8Gi\n"
            ],
            "verify": false,
            "version": "0.3.6",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.istio_install.helm_release.istio_ingress",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.spark",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mlflow",
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.keycloak",
            "module.mariadb_install.helm_release.mariadb",
            "module.istio_install.helm_release.istio_base",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.minio"
          ]
        }
      ]
    },
    {
      "module": "module.kafka_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "kafka",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "kafka",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "kafka",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.8.1",
                "chart": "kafka",
                "name": "kafka",
                "namespace": "kafka",
                "revision": 1,
                "values": "{\"advertisedListeners\":[],\"affinity\":{},\"allowEveryoneIfNoAclFound\":true,\"allowPlaintextListener\":true,\"args\":[],\"auth\":{\"clientProtocol\":\"plaintext\",\"interBrokerProtocol\":\"plaintext\",\"jaas\":{\"clientPasswords\":[],\"clientUsers\":[\"user\"],\"existingSecret\":\"\",\"interBrokerPassword\":\"\",\"interBrokerUser\":\"admin\",\"zookeeperPassword\":\"\",\"zookeeperUser\":\"\"},\"jksKeystoreSAN\":\"\",\"jksPassword\":\"\",\"jksSecret\":\"\",\"jksTruststore\":\"\",\"jksTruststoreSecret\":\"\",\"sasl\":{\"interBrokerMechanism\":\"plain\",\"jaas\":{\"clientPasswords\":[],\"clientUsers\":[\"user\"],\"existingSecret\":\"\",\"interBrokerPassword\":\"\",\"interBrokerUser\":\"admin\",\"zookeeperPassword\":\"\",\"zookeeperUser\":\"\"},\"mechanisms\":\"plain,scram-sha-256,scram-sha-512\"},\"saslInterBrokerMechanism\":\"plain\",\"saslMechanisms\":\"plain,scram-sha-256,scram-sha-512\",\"tls\":{\"autoGenerated\":false,\"endpointIdentificationAlgorithm\":\"https\",\"existingSecret\":\"\",\"existingSecrets\":[],\"jksKeystoreSAN\":\"\",\"jksTruststore\":\"\",\"jksTruststoreSecret\":\"\",\"password\":\"\",\"type\":\"jks\"},\"tlsEndpointIdentificationAlgorithm\":\"https\"},\"authorizerClassName\":\"\",\"autoCreateTopicsEnable\":true,\"clusterDomain\":\"cluster.local\",\"command\":[\"/scripts/setup.sh\"],\"commonAnnotations\":{},\"commonLabels\":{},\"config\":\"\",\"containerSecurityContext\":{\"enabled\":false},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"defaultReplicationFactor\":1,\"deleteTopicEnable\":false,\"diagnosticMode\":{\"args\":[\"infinity\"],\"command\":[\"sleep\"],\"enabled\":false},\"existingConfigmap\":\"\",\"existingLog4jConfigMap\":\"\",\"externalAccess\":{\"autoDiscovery\":{\"enabled\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/kubectl\",\"tag\":\"1.23.0-debian-10-r5\"},\"resources\":{\"limits\":{},\"requests\":{}}},\"enabled\":false,\"service\":{\"annotations\":{},\"domain\":\"\",\"loadBalancerIPs\":[],\"loadBalancerSourceRanges\":[],\"nodePorts\":[],\"port\":9094,\"type\":\"LoadBalancer\",\"useHostIPs\":false,\"usePodIPs\":false}},\"externalZookeeper\":{\"servers\":[]},\"extraDeploy\":[],\"extraEnvVars\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"storageClass\":\"\"},\"heapOpts\":\"-Xmx1024m -Xms1024m\",\"hostAliases\":[],\"image\":{\"debug\":false,\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/kafka\",\"tag\":\"2.8.1-debian-10-r73\"},\"initContainers\":[],\"interBrokerListenerName\":\"INTERNAL\",\"listenerSecurityProtocolMap\":\"\",\"listeners\":[],\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"log4j\":\"\",\"logFlushIntervalMessages\":\"_10000\",\"logFlushIntervalMs\":1000,\"logPersistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":false,\"existingClaim\":\"\",\"existingLogClaim\":\"\",\"mountPath\":\"/opt/bitnami/kafka/logs\",\"selector\":{},\"size\":\"8Gi\"},\"logRetentionBytes\":\"_1073741824\",\"logRetentionCheckIntervalMs\":300000,\"logRetentionHours\":168,\"logSegmentBytes\":\"_1073741824\",\"logsDirs\":\"/bitnami/kafka/data\",\"maxMessageBytes\":\"_1000012\",\"metrics\":{\"jmx\":{\"config\":\"jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi\\nlowercaseOutputName: true\\nlowercaseOutputLabelNames: true\\nssl: false\\n{{- if .Values.metrics.jmx.whitelistObjectNames }}\\nwhitelistObjectNames: [\\\"{{ join \\\"\\\\\\\",\\\\\\\"\\\" .Values.metrics.jmx.whitelistObjectNames }}\\\"]\\n{{- end }}\",\"containerSecurityContext\":{\"enabled\":false},\"enabled\":false,\"existingConfigmap\":\"\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/jmx-exporter\",\"tag\":\"0.16.1-debian-10-r149\"},\"resources\":{\"limits\":{},\"requests\":{}},\"service\":{\"annotations\":{\"prometheus.io/path\":\"/\",\"prometheus.io/port\":\"{{ .Values.metrics.jmx.service.port }}\",\"prometheus.io/scrape\":\"true\"},\"clusterIP\":\"\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":\"\",\"port\":5556,\"type\":\"ClusterIP\"},\"whitelistObjectNames\":[\"kafka.controller:*\",\"kafka.server:*\",\"java.lang:*\",\"kafka.network:*\",\"kafka.log:*\"]},\"kafka\":{\"affinity\":{},\"certificatesSecret\":\"\",\"containerSecurityContext\":{\"enabled\":false},\"enabled\":false,\"extraFlags\":{},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/kafka-exporter\",\"tag\":\"1.4.2-debian-10-r87\"},\"initContainers\":[],\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"resources\":{\"limits\":{},\"requests\":{}},\"schedulerName\":\"\",\"service\":{\"annotations\":{\"prometheus.io/path\":\"/metrics\",\"prometheus.io/port\":\"{{ .Values.metrics.kafka.service.port }}\",\"prometheus.io/scrape\":\"true\"},\"clusterIP\":\"\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":\"\",\"port\":9308,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"tlsCaCert\":\"ca-file\",\"tlsCaSecret\":\"\",\"tlsCert\":\"cert-file\",\"tlsKey\":\"key-file\",\"tolerations\":[]},\"serviceMonitor\":{\"enabled\":false,\"interval\":\"\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scrapeTimeout\":\"\",\"selector\":{}}},\"minBrokerId\":0,\"nameOverride\":\"\",\"networkPolicy\":{\"allowExternal\":true,\"egressRules\":{\"customRules\":[]},\"enabled\":false,\"explicitNamespacesSelector\":{},\"externalAccess\":{\"from\":[]}},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"numIoThreads\":8,\"numNetworkThreads\":3,\"numPartitions\":1,\"numRecoveryThreadsPerDataDir\":1,\"offsetsTopicReplicationFactor\":1,\"pdb\":{\"create\":false,\"maxUnavailable\":1,\"minAvailable\":\"\"},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":true,\"existingClaim\":\"\",\"mountPath\":\"/bitnami/kafka\",\"selector\":{},\"size\":\"8Gi\",\"storageClass\":\"\"},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001,\"runAsUser\":1001},\"priorityClassName\":\"\",\"provisioning\":{\"args\":[],\"command\":[],\"enabled\":false,\"numPartitions\":1,\"podAnnotations\":{},\"replicationFactor\":1,\"resources\":{\"limits\":{},\"requests\":{}},\"schedulerName\":\"\",\"topics\":[]},\"rbac\":{\"create\":false},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"replicaCount\":1,\"resources\":{\"limits\":{},\"requests\":{}},\"rollingUpdatePartition\":\"\",\"schedulerName\":\"\",\"service\":{\"annotations\":{},\"externalPort\":9094,\"internalPort\":9093,\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"client\":\"\",\"external\":\"\"},\"port\":9092,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"sidecars\":[],\"socketReceiveBufferBytes\":102400,\"socketRequestMaxBytes\":\"_104857600\",\"socketSendBufferBytes\":102400,\"superUsers\":\"User:admin\",\"terminationGracePeriodSeconds\":\"\",\"tolerations\":[],\"topologySpreadConstraints\":{},\"transactionStateLogMinIsr\":1,\"transactionStateLogReplicationFactor\":1,\"updateStrategy\":\"RollingUpdate\",\"volumePermissions\":{\"enabled\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/bitnami-shell\",\"tag\":\"10-debian-10-r279\"},\"resources\":{\"limits\":{},\"requests\":{}},\"securityContext\":{\"runAsUser\":0}},\"zookeeper\":{\"auth\":{\"clientPassword\":\"\",\"clientUser\":\"\",\"enabled\":false,\"serverPasswords\":\"\",\"serverUsers\":\"\"},\"enabled\":true},\"zookeeperChrootPath\":\"\",\"zookeeperConnectionTimeoutMs\":6000}",
                "version": "14.8.1"
              }
            ],
            "name": "kafka",
            "namespace": "kafka",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.bitnami.com/bitnami",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1000,
            "values": [
              "## @section Global parameters\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass\n\n## @param global.imageRegistry Global Docker image registry\n## @param global.imagePullSecrets Global Docker registry secret names as an array\n## @param global.storageClass Global StorageClass for Persistent Volume(s)\n##\nglobal:\n  imageRegistry: \"\"\n  ## E.g.\n  ## imagePullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  imagePullSecrets: []\n  storageClass: \"\"\n\n## @section Common parameters\n\n## @param nameOverride String to partially override kafka.fullname\n##\nnameOverride: \"\"\n## @param fullnameOverride String to fully override kafka.fullname\n##\nfullnameOverride: \"\"\n## @param clusterDomain Default Kubernetes cluster domain\n##\nclusterDomain: cluster.local\n## @param commonLabels Labels to add to all deployed objects\n##\ncommonLabels: {}\n## @param commonAnnotations Annotations to add to all deployed objects\n##\ncommonAnnotations: {}\n## @param extraDeploy Array of extra objects to deploy with the release\n##\nextraDeploy: []\n\n## Enable diagnostic mode in the deployment\n##\ndiagnosticMode:\n  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)\n  ##\n  enabled: false\n  ## @param diagnosticMode.command Command to override all containers in the deployment\n  ##\n  command:\n    - sleep\n  ## @param diagnosticMode.args Args to override all containers in the deployment\n  ##\n  args:\n    - infinity\n\n## @section Kafka parameters\n\n## Bitnami Kafka image version\n## ref: https://hub.docker.com/r/bitnami/kafka/tags/\n## @param image.registry Kafka image registry\n## @param image.repository Kafka image repository\n## @param image.tag Kafka image tag (immutable tags are recommended)\n## @param image.pullPolicy Kafka image pull policy\n## @param image.pullSecrets Specify docker-registry secret names as an array\n## @param image.debug Set to true if you would like to see extra information on logs\n##\nimage:\n  registry: docker.io\n  repository: bitnami/kafka\n  tag: 2.8.1-debian-10-r73\n  ## Specify a imagePullPolicy\n  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n  ##\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## Example:\n  ## pullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  pullSecrets: []\n  ## Set to true if you would like to see extra information on logs\n  ##\n  debug: false\n## @param config Configuration file for Kafka. Auto-generated based on other parameters when not specified (see [below](\n## Specify content for server.properties\n## NOTE: This will override any KAFKA_CFG_ environment variables (including those set by the chart)\n## The server.properties is auto-generated based on other parameters when this parameter is not specified\n##\n## Example:\n## config: |-\n##   broker.id=-1\n##   listeners=PLAINTEXT://:9092\n##   advertised.listeners=PLAINTEXT://KAFKA_IP:9092\n##   num.network.threads=3\n##   num.io.threads=8\n##   socket.send.buffer.bytes=102400\n##   socket.receive.buffer.bytes=102400\n##   socket.request.max.bytes=104857600\n##   log.dirs=/bitnami/kafka/data\n##   num.partitions=1\n##   num.recovery.threads.per.data.dir=1\n##   offsets.topic.replication.factor=1\n##   transaction.state.log.replication.factor=1\n##   transaction.state.log.min.isr=1\n##   log.flush.interval.messages=10000\n##   log.flush.interval.ms=1000\n##   log.retention.hours=168\n##   log.retention.bytes=1073741824\n##   log.segment.bytes=1073741824\n##   log.retention.check.interval.ms=300000\n##   zookeeper.connect=ZOOKEEPER_SERVICE_NAME\n##   zookeeper.connection.timeout.ms=6000\n##   group.initial.rebalance.delay.ms=0\n##\nconfig: \"\"\n## @param existingConfigmap ConfigMap with Kafka Configuration\n## NOTE: This will override config AND any KAFKA_CFG_ environment variables.\n##\nexistingConfigmap: \"\"\n## @param log4j An optional log4j.properties file to overwrite the default of the Kafka brokers.\n## An optional log4j.properties file to overwrite the default of the Kafka brokers.\n## See an example log4j.properties at:\n## https://github.com/apache/kafka/blob/trunk/config/log4j.properties\n##\nlog4j: \"\"\n## @param existingLog4jConfigMap The name of an existing ConfigMap containing a log4j.properties file.\n## The name of an existing ConfigMap containing a log4j.properties file.\n## NOTE: this will override log4j.\n##\nexistingLog4jConfigMap: \"\"\n## @param heapOpts Kafka's Java Heap size\n##\nheapOpts: -Xmx1024m -Xms1024m\n## @param deleteTopicEnable Switch to enable topic deletion or not\n##\ndeleteTopicEnable: false\n## @param autoCreateTopicsEnable Switch to enable auto creation of topics. Enabling auto creation of topics not recommended for production or similar environments\n##\nautoCreateTopicsEnable: true\n## @param logFlushIntervalMessages The number of messages to accept before forcing a flush of data to disk\n##\nlogFlushIntervalMessages: _10000\n## @param logFlushIntervalMs The maximum amount of time a message can sit in a log before we force a flush\n##\nlogFlushIntervalMs: 1000\n## @param logRetentionBytes A size-based retention policy for logs\n##\nlogRetentionBytes: _1073741824\n## @param logRetentionCheckIntervalMs The interval at which log segments are checked to see if they can be deleted\n##\nlogRetentionCheckIntervalMs: 300000\n## @param logRetentionHours The minimum age of a log file to be eligible for deletion due to age\n##\nlogRetentionHours: 168\n## @param logSegmentBytes The maximum size of a log segment file. When this size is reached a new log segment will be created\n##\nlogSegmentBytes: _1073741824\n## @param logsDirs A comma separated list of directories under which to store log files\n##\nlogsDirs: /bitnami/kafka/data\n## @param maxMessageBytes The largest record batch size allowed by Kafka\n##\nmaxMessageBytes: _1000012\n## @param defaultReplicationFactor Default replication factors for automatically created topics\n##\ndefaultReplicationFactor: 1\n## @param offsetsTopicReplicationFactor The replication factor for the offsets topic\n##\noffsetsTopicReplicationFactor: 1\n## @param transactionStateLogReplicationFactor The replication factor for the transaction topic\n##\ntransactionStateLogReplicationFactor: 1\n## @param transactionStateLogMinIsr Overridden min.insync.replicas config for the transaction topic\n##\ntransactionStateLogMinIsr: 1\n## @param numIoThreads The number of threads doing disk I/O\n##\nnumIoThreads: 8\n## @param numNetworkThreads The number of threads handling network requests\n##\nnumNetworkThreads: 3\n## @param numPartitions The default number of log partitions per topic\n##\nnumPartitions: 1\n## @param numRecoveryThreadsPerDataDir The number of threads per data directory to be used for log recovery at startup and flushing at shutdown\n##\nnumRecoveryThreadsPerDataDir: 1\n## @param socketReceiveBufferBytes The receive buffer (SO_RCVBUF) used by the socket server\n##\nsocketReceiveBufferBytes: 102400\n## @param socketRequestMaxBytes The maximum size of a request that the socket server will accept (protection against OOM)\n##\nsocketRequestMaxBytes: _104857600\n## @param socketSendBufferBytes The send buffer (SO_SNDBUF) used by the socket server\n##\nsocketSendBufferBytes: 102400\n## @param zookeeperConnectionTimeoutMs Timeout in ms for connecting to Zookeeper\n##\nzookeeperConnectionTimeoutMs: 6000\n## @param zookeeperChrootPath Path which puts data under some path in the global ZooKeeper namespace\n## ref: https://kafka.apache.org/documentation/#brokerconfigs_zookeeper.connect\n##\nzookeeperChrootPath: \"\"\n## @param authorizerClassName The Authorizer is configured by setting authorizer.class.name=kafka.security.authorizer.AclAuthorizer in server.properties.\n##\nauthorizerClassName: \"\"\n## @param allowEveryoneIfNoAclFound By default, if a resource has no associated ACLs, then no one is allowed to access that resource except super users.\n##\nallowEveryoneIfNoAclFound: true\n## @param superUsers You can add super users in server.properties\n##\nsuperUsers: User:admin\n## @param command Override kafka container command\n##\ncommand:\n  - /scripts/setup.sh\n## @param args Override kafka container arguments\n##\nargs: []\n## @param extraEnvVars Extra environment variables to add to kafka pods (see [below]({KEY}\n## ref: https://github.com/bitnami/bitnami-docker-kafka#configuration\n## Example:\n## extraEnvVars:\n##   - name: KAFKA_CFG_BACKGROUND_THREADS\n##     value: \"10\"\n##\nextraEnvVars: []\n## @param extraVolumes Extra volume(s) to add to Kafka statefulset\n## Examples:\n## extraVolumes:\n##   - name: kafka-jaas\n##     secret:\n##       secretName: kafka-jaas\nextraVolumes: []\n## @param extraVolumeMounts Extra volumeMount(s) to add to Kafka containers\n## extraVolumeMounts:\n##   - name: kafka-jaas\n##     mountPath: /bitnami/kafka/config/kafka_jaas.conf\n##     subPath: kafka_jaas.conf\nextraVolumeMounts: []\n## Authentication parameteres\n## https://github.com/bitnami/bitnami-docker-kafka#security\n##\nauth:\n  ## Authentication protocol for client and inter-broker communications\n  ## This table shows the security provided on each protocol:\n  ## | Method    | Authentication                | Encryption via TLS |\n  ## | plaintext | None                          | No                 |\n  ## | tls       | None                          | Yes                |\n  ## | mtls      | Yes (two-way authentication)  | Yes                |\n  ## | sasl      | Yes (via SASL)                | No                 |\n  ## | sasl_tls  | Yes (via SASL)                | Yes                |\n  ## @param auth.clientProtocol Authentication protocol for communications with clients. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`\n  ## @param auth.interBrokerProtocol Authentication protocol for inter-broker communications. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`\n  ##\n  clientProtocol: plaintext\n  interBrokerProtocol: plaintext\n  ## SASL configuration\n  ##\n  sasl:\n    ## @param auth.sasl.mechanisms SASL mechanisms when either `auth.interBrokerProtocol` or `auth.clientProtocol` are `sasl`. Allowed types: `plain`, `scram-sha-256`, `scram-sha-512`\n    ##\n    mechanisms: plain,scram-sha-256,scram-sha-512\n    ## @param auth.sasl.interBrokerMechanism SASL mechanism for inter broker communication.\n    ##\n    interBrokerMechanism: plain\n    ## JAAS configuration for SASL authentication.\n    ##\n    jaas:\n      ## @param auth.sasl.jaas.clientUsers Kafka client user list\n      ##\n      ## clientUsers:\n      ##   - user1\n      ##   - user2\n      ##\n      clientUsers:\n        - user\n      ## @param auth.sasl.jaas.clientPasswords Kafka client passwords. This is mandatory if more than one user is specified in clientUsers\n      ##\n      ## clientPasswords:\n      ##   - password1\n      ##   - password2\"\n      ##\n      clientPasswords: []\n      ## @param auth.sasl.jaas.interBrokerUser Kafka inter broker communication user for SASL authentication\n      ##\n      interBrokerUser: admin\n      ## @param auth.sasl.jaas.interBrokerPassword Kafka inter broker communication password for SASL authentication\n      ##\n      interBrokerPassword: \"\"\n      ## @param auth.sasl.jaas.zookeeperUser Kafka Zookeeper user for SASL authentication\n      ##\n      zookeeperUser: \"\"\n      ## @param auth.sasl.jaas.zookeeperPassword Kafka Zookeeper password for SASL authentication\n      ##\n      zookeeperPassword: \"\"\n      ## @param auth.sasl.jaas.existingSecret Name of the existing secret containing credentials for clientUsers, interBrokerUser and zookeeperUser\n      ## Create this secret running the command below where SECRET_NAME is the name of the secret you want to create:\n      ##       kubectl create secret generic SECRET_NAME --from-literal=client-passwords=CLIENT_PASSWORD1,CLIENT_PASSWORD2 --from-literal=inter-broker-password=INTER_BROKER_PASSWORD --from-literal=zookeeper-password=ZOOKEEPER_PASSWORD\n      ##\n      existingSecret: \"\"\n  ## @param auth.saslMechanisms DEPRECATED: use `auth.sasl.mechanisms` instead.\n  ##\n  saslMechanisms: plain,scram-sha-256,scram-sha-512\n  ## @param auth.saslInterBrokerMechanism DEPRECATED: use `auth.sasl.interBrokerMechanism` instead.\n  ##\n  saslInterBrokerMechanism: plain\n  ## @param auth.jaas [object] DEPRECATED: use `auth.sasl.jaas` instead.\n  ## @skip auth.jaas.clientUsers\n  ##\n  jaas:\n    clientUsers:\n      - user\n    clientPasswords: []\n    interBrokerUser: admin\n    interBrokerPassword: \"\"\n    zookeeperUser: \"\"\n    zookeeperPassword: \"\"\n    existingSecret: \"\"\n  ## TLS configuration\n  ##\n  tls:\n    ## @param auth.tls.type Format to use for TLS certificates. Allowed types: `jks` and `pem`\n    ##\n    type: jks\n    ## @param auth.tls.existingSecrets Array existing secrets containing the TLS certificates for the Kafka brokers\n    ## When using 'jks' format for certificates, each secret should contain a truststore and a keystore.\n    ## Create these secrets following the steps below:\n    ## 1) Generate your truststore and keystore files. Helpful script: https://raw.githubusercontent.com/confluentinc/confluent-platform-security-tools/master/kafka-generate-ssl.sh\n    ## 2) Rename your truststore to `kafka.truststore.jks`.\n    ## 3) Rename your keystores to `kafka-X.keystore.jks` where X is the ID of each Kafka broker.\n    ## 4) Run the command below one time per broker to create its associated secret (SECRET_NAME_X is the name of the secret you want to create):\n    ##       kubectl create secret generic SECRET_NAME_0 --from-file=kafka.truststore.jks=./kafka.truststore.jks --from-file=kafka.keystore.jks=./kafka-0.keystore.jks\n    ##       kubectl create secret generic SECRET_NAME_1 --from-file=kafka.truststore.jks=./kafka.truststore.jks --from-file=kafka.keystore.jks=./kafka-1.keystore.jks\n    ##       ...\n    ##\n    ## When using 'pem' format for certificates, each secret should contain a public CA certificate, a public certificate and one private key.\n    ## Create these secrets following the steps below:\n    ## 1) Create a certificate key and signing request per Kafka broker, and sign the signing request with your CA\n    ## 2) Rename your CA file to `kafka.ca.crt`.\n    ## 3) Rename your certificates to `kafka-X.tls.crt` where X is the ID of each Kafka broker.\n    ## 3) Rename your keys to `kafka-X.tls.key` where X is the ID of each Kafka broker.\n    ## 4) Run the command below one time per broker to create its associated secret (SECRET_NAME_X is the name of the secret you want to create):\n    ##       kubectl create secret generic SECRET_NAME_0 --from-file=ca.crt=./kafka.ca.crt --from-file=tls.crt=./kafka-0.tls.crt --from-file=tls.key=./kafka-0.tls.key\n    ##       kubectl create secret generic SECRET_NAME_1 --from-file=ca.crt=./kafka.ca.crt --from-file=tls.crt=./kafka-1.tls.crt --from-file=tls.key=./kafka-1.tls.key\n    ##       ...\n    ##\n    existingSecrets: []\n    ## @param auth.tls.existingSecret DEPRECATED: use `auth.tls.existingSecrets` instead.\n    ##\n    existingSecret: \"\"\n    ## @param auth.tls.autoGenerated Generate automatically self-signed TLS certificates for Kafka brokers. Currently only supported if `auth.tls.type` is `pem`\n    ## Note: ignored when using 'jks' format or `auth.tls.existingSecrets` is not empty\n    ##\n    autoGenerated: false\n    ## @param auth.tls.password Password to access the JKS files or PEM key when they are password-protected.\n    ##\n    password: \"\"\n    ## @param auth.tls.jksTruststoreSecret Name of the existing secret containing your truststore if truststore not existing or different from the ones in the `auth.tls.existingSecrets`\n    ## Note: ignored when using 'pem' format for certificates .\n    ##\n    jksTruststoreSecret: \"\"\n    ## @param auth.tls.jksKeystoreSAN The secret key from the `auth.tls.existingSecret` containing the keystore with a SAN certificate\n    ## The SAN certificate in it should be issued with Subject Alternative Names for all headless services:\n    ##  - kafka-0.kafka-headless.kafka.svc.cluster.local\n    ##  - kafka-1.kafka-headless.kafka.svc.cluster.local\n    ##  - kafka-2.kafka-headless.kafka.svc.cluster.local\n    ## Note: ignored when using 'pem' format for certificates.\n    ##\n    jksKeystoreSAN: \"\"\n    ## @param auth.tls.jksTruststore The secret key from the `auth.tls.existingSecret` or `auth.tls.jksTruststoreSecret` containing the truststore\n    ## Note: ignored when using 'pem' format for certificates.\n    ##\n    jksTruststore: \"\"\n    ## @param auth.tls.endpointIdentificationAlgorithm The endpoint identification algorithm to validate server hostname using server certificate\n    ## Disable server host name verification by setting it to an empty string.\n    ## ref: https://docs.confluent.io/current/kafka/authentication_ssl.html#optional-settings\n    ##\n    endpointIdentificationAlgorithm: https\n  ## @param auth.jksSecret DEPRECATED: use `auth.tls.existingSecrets` instead.\n  ##\n  jksSecret: \"\"\n  ## @param auth.jksTruststoreSecret DEPRECATED: use `auth.tls.jksTruststoreSecret` instead.\n  ##\n  jksTruststoreSecret: \"\"\n  ## @param auth.jksKeystoreSAN DEPRECATED: use `auth.tls.jksKeystoreSAN` instead.\n  ##\n  jksKeystoreSAN: \"\"\n  ## @param auth.jksTruststore DEPRECATED: use `auth.tls.jksTruststore` instead.\n  ##\n  jksTruststore: \"\"\n  ## @param auth.jksPassword DEPRECATED: use `auth.tls.password` instead.\n  ##\n  jksPassword: \"\"\n  ## @param auth.tlsEndpointIdentificationAlgorithm DEPRECATED: use `auth.tls.endpointIdentificationAlgorithm` instead.\n  ##\n  tlsEndpointIdentificationAlgorithm: https\n## @param listeners The address(es) the socket server listens on. Auto-calculated it's set to an empty array\n## When it's set to an empty array, the listeners will be configured\n## based on the authentication protocols (auth.clientProtocol and auth.interBrokerProtocol parameters)\n##\nlisteners: []\n## @param advertisedListeners The address(es) (hostname:port) the broker will advertise to producers and consumers. Auto-calculated it's set to an empty array\n## When it's set to an empty array, the advertised listeners will be configured\n## based on the authentication protocols (auth.clientProtocol and auth.interBrokerProtocol parameters)\n##\nadvertisedListeners: []\n## @param listenerSecurityProtocolMap The protocol-\u003elistener mapping. Auto-calculated it's set to nil\n## When it's nil, the listeners will be configured based on the authentication protocols (auth.clientProtocol and auth.interBrokerProtocol parameters)\n##\nlistenerSecurityProtocolMap: \"\"\n## @param allowPlaintextListener Allow to use the PLAINTEXT listener\n##\nallowPlaintextListener: true\n## @param interBrokerListenerName The listener that the brokers should communicate on\n##\ninterBrokerListenerName: INTERNAL\n\n## @section Statefulset parameters\n\n## @param replicaCount Number of Kafka nodes\n##\nreplicaCount: 1\n## @param minBrokerId Minimal broker.id value, nodes increment their `broker.id` respectively\n## Brokers increment their ID starting at this minimal value.\n## E.g., with `minBrokerId=100` and 3 nodes, IDs will be 100, 101, 102 for brokers 0, 1, and 2, respectively.\n##\nminBrokerId: 0\n## @param updateStrategy Update strategy for the stateful set\n## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets\n##\nupdateStrategy: RollingUpdate\n## @param rollingUpdatePartition Partition update strategy\n## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions\n##\nrollingUpdatePartition: \"\"\n## @param hostAliases Add deployment host aliases\n## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n##\nhostAliases: []\n## @param podManagementPolicy StatefulSet controller supports relax its ordering guarantees while preserving its uniqueness and identity guarantees. There are two valid pod management policies: OrderedReady and Parallel\n## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy\n##\npodManagementPolicy: Parallel\n## @param schedulerName Name of the k8s scheduler (other than default)\n## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n##\nschedulerName: \"\"\n## @param podLabels Kafka pod labels\n## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n##\npodLabels: {}\n## @param podAnnotations Kafka Pod annotations\n## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n##\npodAnnotations: {}\n## @param priorityClassName Name of the existing priority class to be used by kafka pods\n## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n##\npriorityClassName: \"\"\n## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAffinityPreset: \"\"\n## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAntiAffinityPreset: soft\n## Node affinity preset\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n##\nnodeAffinityPreset:\n  ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ##\n  type: \"\"\n  ## @param nodeAffinityPreset.key Node label key to match Ignored if `affinity` is set.\n  ## E.g.\n  ## key: \"kubernetes.io/e2e-az-name\"\n  ##\n  key: \"\"\n  ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.\n  ## E.g.\n  ## values:\n  ##   - e2e-az1\n  ##   - e2e-az2\n  ##\n  values: []\n## @param affinity Affinity for pod assignment\n## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set\n##\naffinity: {}\n## @param nodeSelector Node labels for pod assignment\n## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n##\nnodeSelector: {}\n## @param tolerations Tolerations for pod assignment\n## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n##\ntolerations: []\n## @param topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template\n## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods\n##\ntopologySpreadConstraints: {}\n## @param terminationGracePeriodSeconds Seconds the pod needs to gracefully terminate\n## ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-handler-execution\n##\nterminationGracePeriodSeconds: \"\"\n## Kafka pods' Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n## @param podSecurityContext.enabled Enable security context for the pods\n## @param podSecurityContext.fsGroup Group ID for the filesystem used by the containers\n## @param podSecurityContext.runAsUser User ID for the service user running the pod\n##\npodSecurityContext:\n  enabled: true\n  fsGroup: 1001\n  runAsUser: 1001\n## Kafka containers' Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n## @param containerSecurityContext.enabled Enable Kafka containers' Security Context\n## Example:\n##   containerSecurityContext:\n##     enabled: true\n##     capabilities:\n##       drop: [\"NET_RAW\"]\n##     readOnlyRootFilesystem: true\n##\ncontainerSecurityContext:\n  enabled: false\n## Kafka containers' resource requests and limits\n## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n## We usually recommend not to specify default resources and to leave this as a conscious\n## choice for the user. This also increases chances charts run on environments with little\n## resources, such as Minikube. If you do want to specify resources, uncomment the following\n## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n## @param resources.limits The resources limits for Kafka containers\n## @param resources.requests The requested resources for Kafka containers\n##\nresources:\n  ## Example:\n  ## limits:\n  ##    cpu: 250m\n  ##    memory: 1Gi\n  limits: {}\n  ## Examples:\n  ## requests:\n  ##    cpu: 250m\n  ##    memory: 256Mi\n  requests: {}\n## Kafka containers' liveness probe. Evaluated as a template.\n## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes\n## @param livenessProbe.enabled Enable livenessProbe\n## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n## @param livenessProbe.periodSeconds Period seconds for livenessProbe\n## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n## @param livenessProbe.failureThreshold Failure threshold for livenessProbe\n## @param livenessProbe.successThreshold Success threshold for livenessProbe\n##\nlivenessProbe:\n  enabled: true\n  initialDelaySeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  periodSeconds: 10\n  successThreshold: 1\n## Kafka containers' readiness probe. Evaluated as a template.\n## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes\n## @param readinessProbe.enabled Enable readinessProbe\n## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n## @param readinessProbe.periodSeconds Period seconds for readinessProbe\n## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n## @param readinessProbe.failureThreshold Failure threshold for readinessProbe\n## @param readinessProbe.successThreshold Success threshold for readinessProbe\n##\nreadinessProbe:\n  enabled: true\n  initialDelaySeconds: 5\n  failureThreshold: 6\n  timeoutSeconds: 5\n  periodSeconds: 10\n  successThreshold: 1\n## @param customLivenessProbe Custom Liveness probe configuration for Kafka\n##\ncustomLivenessProbe: {}\n## @param customReadinessProbe Custom Readiness probe configuration for Kafka\n##\ncustomReadinessProbe: {}\n## Pod Disruption Budget configuration\n## The PDB will only be created if replicaCount is greater than 1\n## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions\n##\npdb:\n  ## @param pdb.create Enable/disable a Pod Disruption Budget creation\n  ##\n  create: false\n  ## @param pdb.minAvailable Minimum number/percentage of pods that should remain scheduled\n  ##\n  minAvailable: \"\"\n  ## @param pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable\n  ##\n  maxUnavailable: 1\n## @param sidecars Attach additional sidecar containers to the Kafka pod\n## Example:\n## sidecars:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\nsidecars: []\n## @param initContainers Add extra init containers\n##\ninitContainers: []\n\n## @section Exposure parameters\nnetworkPolicy:\n  ## @param networkPolicy.enabled Enable creation of NetworkPolicy resources.\n  ##\n  enabled: false\n  ## @param networkPolicy.allowExternal Don't require client label for connections\n  ## The Policy model to apply. When set to false, only pods with the correct\n  ## client label will have network access to Kafka port for client connections (service.port).\n  ## When true, kafka will accept connections from any source\n  ## (with the correct destination port).\n  ##\n  allowExternal: true\n  ## @param networkPolicy.explicitNamespacesSelector A Kubernetes LabelSelector to explicitly select namespaces from which traffic could be allowed\n  ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace\n  ## and that match other criteria, the ones that have the good label, can reach the kafka.\n  ## But sometimes, we want the kafka to be accessible to clients from other namespaces, in this case, we can use this\n  ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.\n  ##\n  ## Example:\n  ## explicitNamespacesSelector:\n  ##   matchLabels:\n  ##     role: frontend\n  ##   matchExpressions:\n  ##    - {key: role, operator: In, values: [frontend]}\n  ##\n  explicitNamespacesSelector: {}\n  ## @param networkPolicy.externalAccess.from customize the from section for External Access on tcp-external port\n  ## Example:\n  ## - ipBlock:\n  ##    cidr: 172.9.0.0/16\n  ##    except:\n  ##    - 172.9.1.0/24\n  ##\n  externalAccess:\n    from: []\n  ## @param networkPolicy.egressRules.customRules [object] Custom network policy rule\n  ##\n  egressRules:\n    ## Additional custom egress rules\n    ## e.g:\n    ## customRules:\n    ##   - to:\n    ##       - namespaceSelector:\n    ##           matchLabels:\n    ##             label: example\n    customRules: []\n## Service parameters\n##\nservice:\n  ## @param service.type Kubernetes Service type\n  ##\n  type: ClusterIP\n  ## @param service.port Kafka port for client connections\n  ##\n  port: 9092\n  ## @param service.internalPort Kafka port for inter-broker connections\n  ##\n  internalPort: 9093\n  ## @param service.externalPort Kafka port for external connections\n  ##\n  externalPort: 9094\n  ## @param service.nodePorts [object] Specify the nodePort value for the LoadBalancer and NodePort service types.\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n  ##\n  nodePorts:\n    client: \"\"\n    external: \"\"\n  ## @param service.loadBalancerIP loadBalancerIP for Kafka Service\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n  ##\n  loadBalancerIP: \"\"\n  ## @param service.loadBalancerSourceRanges Address(es) that are allowed when service is LoadBalancer\n  ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n  ## Example:\n  ## loadBalancerSourceRanges:\n  ## - 10.10.10.0/24\n  ##\n  loadBalancerSourceRanges: []\n  ## @param service.annotations Service annotations\n  ##\n  annotations: {}\n## External Access to Kafka brokers configuration\n##\nexternalAccess:\n  ## @param externalAccess.enabled Enable Kubernetes external cluster access to Kafka brokers\n  ##\n  enabled: false\n  ## External IPs auto-discovery configuration\n  ## An init container is used to auto-detect LB IPs or node ports by querying the K8s API\n  ## Note: RBAC might be required\n  ##\n  autoDiscovery:\n    ## @param externalAccess.autoDiscovery.enabled Enable using an init container to auto-detect external IPs/ports by querying the K8s API\n    ##\n    enabled: false\n    ## Bitnami Kubectl image\n    ## ref: https://hub.docker.com/r/bitnami/kubectl/tags/\n    ## @param externalAccess.autoDiscovery.image.registry Init container auto-discovery image registry\n    ## @param externalAccess.autoDiscovery.image.repository Init container auto-discovery image repository\n    ## @param externalAccess.autoDiscovery.image.tag Init container auto-discovery image tag (immutable tags are recommended)\n    ## @param externalAccess.autoDiscovery.image.pullPolicy Init container auto-discovery image pull policy\n    ## @param externalAccess.autoDiscovery.image.pullSecrets Init container auto-discovery image pull secrets\n    ##\n    image:\n      registry: docker.io\n      repository: bitnami/kubectl\n      tag: 1.23.0-debian-10-r5\n      ## Specify a imagePullPolicy\n      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n      ##\n      pullPolicy: IfNotPresent\n      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n      ## Example:\n      ## pullSecrets:\n      ##   - myRegistryKeySecretName\n      ##\n      pullSecrets: []\n    ## Init Container resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## We usually recommend not to specify default resources and to leave this as a conscious\n    ## choice for the user. This also increases chances charts run on environments with little\n    ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n    ## @param externalAccess.autoDiscovery.resources.limits Init container auto-discovery resource limits\n    ## @param externalAccess.autoDiscovery.resources.requests Init container auto-discovery resource requests\n    ##\n    resources:\n      ## Example:\n      ## limits:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      limits: {}\n      ## Examples:\n      ## requests:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      requests: {}\n  ## Parameters to configure K8s service(s) used to externally access Kafka brokers\n  ## A new service per broker will be created\n  ##\n  service:\n    ## @param externalAccess.service.type Kubernetes Service type for external access. It can be NodePort or LoadBalancer\n    ##\n    type: LoadBalancer\n    ## @param externalAccess.service.port Kafka port used for external access when service type is LoadBalancer\n    ##\n    port: 9094\n    ## @param externalAccess.service.loadBalancerIPs Array of load balancer IPs for each Kafka broker. Length must be the same as replicaCount\n    ## Example:\n    ## loadBalancerIPs:\n    ##   - X.X.X.X\n    ##   - Y.Y.Y.Y\n    ##\n    loadBalancerIPs: []\n    ## @param externalAccess.service.loadBalancerSourceRanges Address(es) that are allowed when service is LoadBalancer\n    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n    ## Example:\n    ## loadBalancerSourceRanges:\n    ## - 10.10.10.0/24\n    ##\n    loadBalancerSourceRanges: []\n    ## @param externalAccess.service.nodePorts Array of node ports used for each Kafka broker. Length must be the same as replicaCount\n    ## Example:\n    ## nodePorts:\n    ##   - 30001\n    ##   - 30002\n    ##\n    nodePorts: []\n    ## @param externalAccess.service.useHostIPs Use service host IPs to configure Kafka external listener when service type is NodePort\n    ##\n    useHostIPs: false\n    ## @param externalAccess.service.domain Domain or external ip used to configure Kafka external listener when service type is NodePort\n    ## If not specified, the container will try to get the kubernetes node external IP\n    ##\n    domain: \"\"\n    ## @param externalAccess.service.annotations Service annotations for external access\n    ##\n    annotations: {}\n    ## @param externalAccess.service.usePodIPs using the MY_POD_IP address for external access.\n    ##\n    usePodIPs: false\n\n## @section Persistence parameters\n\n## Persistence parameters\n##\npersistence:\n  ## @param persistence.enabled Enable Kafka data persistence using PVC, note that Zookeeper persistence is unaffected\n  ##\n  enabled: true\n  ## @param persistence.existingClaim Provide an existing `PersistentVolumeClaim`, the value is evaluated as a template\n  ## If defined, PVC must be created manually before volume will be bound\n  ## The value is evaluated as a template\n  ##\n  existingClaim: \"\"\n  ## @param persistence.storageClass PVC Storage Class for Kafka data volume\n  ## If defined, storageClassName: \u003cstorageClass\u003e\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ## set, choosing the default provisioner.\n  ##\n  storageClass: \"\"\n  ## @param persistence.accessModes PV Access Mode\n  ##\n  accessModes:\n    - ReadWriteOnce\n  ## @param persistence.size PVC Storage Request for Kafka data volume\n  ##\n  size: 8Gi\n  ## @param persistence.annotations Annotations for the PVC\n  ##\n  annotations: {}\n  ## @param persistence.selector Selector to match an existing Persistent Volume for Kafka's data PVC. If set, the PVC can't have a PV dynamically provisioned for it\n  ## selector:\n  ##   matchLabels:\n  ##     app: my-app\n  selector: {}\n  ## @param persistence.mountPath Mount path of the Kafka data volume\n  ##\n  mountPath: /bitnami/kafka\n## Log Persistence parameters\n##\nlogPersistence:\n  ## @param logPersistence.enabled Enable Kafka logs persistence using PVC, note that Zookeeper persistence is unaffected\n  ##\n  enabled: false\n  ## @param logPersistence.existingClaim A manually managed Persistent Volume and Claim\n  ## If defined, PVC must be created manually before volume will be bound\n  ## The value is evaluated as a template\n  ##\n  existingClaim: \"\"\n  ## @param logPersistence.existingLogClaim PV Storage Class\n  ## If defined, storageClassName: \u003cstorageClass\u003e\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ## set, choosing the default provisioner.\n  existingLogClaim: \"\"\n  ## @param logPersistence.accessModes PV Access Mode\n  ##\n  accessModes:\n    - ReadWriteOnce\n  ## @param logPersistence.size PVC Storage Request for Kafka logs volume\n  ##\n  size: 8Gi\n  ## @param logPersistence.annotations Annotations for the PVC\n  ##\n  annotations: {}\n  ## @param logPersistence.selector Selector to match an existing Persistent Volume for Kafka's log data PVC. If set, the PVC can't have a PV dynamically provisioned for it\n  ## selector:\n  ##   matchLabels:\n  ##     app: my-app\n  selector: {}\n  ## @param logPersistence.mountPath Mount path of the Kafka logs volume\n  ##\n  mountPath: /opt/bitnami/kafka/logs\n\n## @section RBAC parameters\n\n## Kafka pods ServiceAccount\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n##\nserviceAccount:\n  ## @param serviceAccount.create Enable creation of ServiceAccount for Kafka pods\n  ##\n  create: true\n  ## @param serviceAccount.name The name of the service account to use. If not set and `create` is `true`, a name is generated\n  ## If not set and create is true, a name is generated using the kafka.serviceAccountName template\n  ##\n  name: \"\"\n  ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created\n  ## Can be set to false if pods using this serviceAccount do not need to use K8s API\n  ##\n  automountServiceAccountToken: true\n## Role Based Access\n## ref: https://kubernetes.io/docs/admin/authorization/rbac/\n##\nrbac:\n  ## @param rbac.create Whether to create \u0026 use RBAC resources or not\n  ## binding Kafka ServiceAccount to a role\n  ## that allows Kafka pods querying the K8s API\n  ##\n  create: false\n\n## @section Volume Permissions parameters\n\n## Init Container parameters\n## Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each component\n## values from the securityContext section of the component\n##\nvolumePermissions:\n  ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to `runAsUser:fsGroup`\n  ##\n  enabled: false\n  ## The security context for the volumePermissions init container\n  ## @param volumePermissions.securityContext.runAsUser User ID for the container.\n  ## Can be set to \"auto\". \"auto\" is especially useful for OpenShift which has scc with dynamic user ids (and 0 is not allowed).\n  ##\n  securityContext:\n    runAsUser: 0\n  ## @param volumePermissions.image.registry Init container volume-permissions image registry\n  ## @param volumePermissions.image.repository Init container volume-permissions image name\n  ## @param volumePermissions.image.tag Init container volume-permissions image tag\n  ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy\n  ## @param volumePermissions.image.pullSecrets Specify docker-registry secret names as an array\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/bitnami-shell\n    tag: 10-debian-10-r279\n    ## Specify a imagePullPolicy\n    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n    ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n    ##\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## Example:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## Init Container resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param volumePermissions.resources.limits Init container volume-permissions resource  limits\n  ## @param volumePermissions.resources.requests Init container volume-permissions resource  requests\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 100m\n    ##    memory: 128Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##    cpu: 100m\n    ##    memory: 128Mi\n    requests: {}\n\n## @section Metrics parameters\n\n## Prometheus Exporters / Metrics\n##\nmetrics:\n  ## Prometheus Kafka Exporter: exposes complimentary metrics to JMX Exporter\n  ##\n  kafka:\n    ## @param metrics.kafka.enabled Whether or not to create a standalone Kafka exporter to expose Kafka metrics\n    ##\n    enabled: false\n    ## Bitnami Kafka exporter image\n    ## ref: https://hub.docker.com/r/bitnami/kafka-exporter/tags/\n    ## @param metrics.kafka.image.registry Kafka exporter image registry\n    ## @param metrics.kafka.image.repository Kafka exporter image repository\n    ## @param metrics.kafka.image.tag Kafka exporter image tag (immutable tags are recommended)\n    ## @param metrics.kafka.image.pullPolicy Kafka exporter image pull policy\n    ## @param metrics.kafka.image.pullSecrets Specify docker-registry secret names as an array\n    ##\n    image:\n      registry: docker.io\n      repository: bitnami/kafka-exporter\n      tag: 1.4.2-debian-10-r87\n      ## Specify a imagePullPolicy\n      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n      ##\n      pullPolicy: IfNotPresent\n      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n      ## Example:\n      ## pullSecrets:\n      ##   - myRegistryKeySecretName\n      ##\n      pullSecrets: []\n    ## Kafka exporter pods ServiceAccount\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    ##\n    serviceAccount:\n      ## @param metrics.kafka.serviceAccount.create Enable creation of ServiceAccount for Kafka exporter pods\n      ##\n      create: true\n      ## @param metrics.kafka.serviceAccount.name The name of the service account to use. If not set and `create` is `true`, a name is generated\n      ## If not set and create is true, a name is generated using the kafka.metrics.kafka.serviceAccountName template\n      ##\n      name: \"\"\n      ## @param metrics.kafka.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created\n      ## Can be set to false if pods using this serviceAccount do not need to use K8s API\n      ##\n      automountServiceAccountToken: true\n    ## @param metrics.kafka.schedulerName Name of the k8s scheduler (other than default) for Kafka Exporter\n    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n    ##\n    schedulerName: \"\"\n    ## @param metrics.kafka.extraFlags Extra flags to be passed to Kafka exporter\n    ## Example:\n    ## extraFlags:\n    ##   tls.insecure-skip-tls-verify: \"\"\n    ##   web.telemetry-path: \"/metrics\"\n    ##\n    extraFlags: {}\n    ## @param metrics.kafka.certificatesSecret Name of the existing secret containing the optional certificate and key files\n    ## for Kafka Exporter client authentication\n    ##\n    certificatesSecret: \"\"\n    ## @param metrics.kafka.tlsCert The secret key from the certificatesSecret if 'client-cert' key different from the default (cert-file)\n    ##\n    tlsCert: cert-file\n    ## @param metrics.kafka.tlsKey The secret key from the certificatesSecret if 'client-key' key different from the default (key-file)\n    ##\n    tlsKey: key-file\n    ## @param metrics.kafka.tlsCaSecret Name of the existing secret containing the optional ca certificate for Kafka Exporter client authentication\n    ##\n    tlsCaSecret: \"\"\n    ## @param metrics.kafka.tlsCaCert The secret key from the certificatesSecret or tlsCaSecret if 'ca-cert' key different from the default (ca-file)\n    ##\n    tlsCaCert: ca-file\n    ## @param metrics.kafka.podLabels Kafka exporter pod labels\n    ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n    ##\n    podLabels: {}\n    ## @param metrics.kafka.podAnnotations Kafka exporter pod annotations\n    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n    ##\n    podAnnotations: {}\n    ## Prometheus Kafka Exporter containers' Security Context\n    ## @param metrics.kafka.containerSecurityContext.enabled Enable Prometheus Kafka Exporter containers' Security Context\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n    ## Example:\n    ##   containerSecurityContext:\n    ##     enabled: true\n    ##     capabilities:\n    ##       drop: [\"NET_RAW\"]\n    ##     readOnlyRootFilesystem: true\n    ##\n    containerSecurityContext:\n      enabled: false\n    ## Prometheus Kafka Exporter' resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## We usually recommend not to specify default resources and to leave this as a conscious\n    ## choice for the user. This also increases chances charts run on environments with little\n    ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n    ## @param metrics.kafka.resources.limits Kafka Exporter container resource limits\n    ## @param metrics.kafka.resources.requests Kafka Exporter container resource requests\n    ##\n    resources:\n      ## Example:\n      ## limits:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      limits: {}\n      ## Examples:\n      ## requests:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      requests: {}\n    ## @param metrics.kafka.affinity Affinity for Kafka Exporter pod assignment\n    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n    ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set\n    ##\n    affinity: {}\n    ## @param metrics.kafka.nodeSelector Node labels for Kafka Exporter pod assignment\n    ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n    ## @param metrics.kafka.tolerations Tolerations for Kafka Exporter pod assignment\n    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    ## @param metrics.kafka.initContainers Add init containers to the Kafka exporter pods\n    ## Example:\n    ## initContainers:\n    ##   - name: your-image-name\n    ##     image: your-image\n    ##     imagePullPolicy: Always\n    ##     ports:\n    ##       - name: portname\n    ##         containerPort: 1234\n    ##\n    initContainers: []\n    ## Service configuration\n    ##\n    service:\n      ## @param metrics.kafka.service.type Kubernetes service type (`ClusterIP`, `NodePort` or `LoadBalancer`) for Kafka Exporter\n      ##\n      type: ClusterIP\n      ## @param metrics.kafka.service.port Kafka Exporter Prometheus port\n      ##\n      port: 9308\n      ## @param metrics.kafka.service.nodePort Kubernetes HTTP node port\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n      ##\n      nodePort: \"\"\n      ## @param metrics.kafka.service.loadBalancerIP loadBalancerIP if service type is `LoadBalancer`\n      ## Set the LoadBalancer service type to internal only\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n      ##\n      loadBalancerIP: \"\"\n      ## @param metrics.kafka.service.loadBalancerSourceRanges Load Balancer sources\n      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n      ## Example:\n      ## loadBalancerSourceRanges:\n      ## - 10.10.10.0/24\n      ##\n      loadBalancerSourceRanges: []\n      ## @param metrics.kafka.service.clusterIP Static clusterIP or None for headless services\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\n      ##\n      clusterIP: \"\"\n      ## @param metrics.kafka.service.annotations [object] Annotations for the Kafka Exporter Prometheus metrics service\n      ##\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"{{ .Values.metrics.kafka.service.port }}\"\n        prometheus.io/path: \"/metrics\"\n  ## Prometheus JMX Exporter: exposes the majority of Kafkas metrics\n  ##\n  jmx:\n    ## @param metrics.jmx.enabled Whether or not to expose JMX metrics to Prometheus\n    ##\n    enabled: false\n    ## Bitnami JMX exporter image\n    ## ref: https://hub.docker.com/r/bitnami/jmx-exporter/tags/\n    ## @param metrics.jmx.image.registry JMX exporter image registry\n    ## @param metrics.jmx.image.repository JMX exporter image repository\n    ## @param metrics.jmx.image.tag JMX exporter image tag (immutable tags are recommended)\n    ## @param metrics.jmx.image.pullPolicy JMX exporter image pull policy\n    ## @param metrics.jmx.image.pullSecrets Specify docker-registry secret names as an array\n    ##\n    image:\n      registry: docker.io\n      repository: bitnami/jmx-exporter\n      tag: 0.16.1-debian-10-r149\n      ## Specify a imagePullPolicy\n      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n      ##\n      pullPolicy: IfNotPresent\n      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n      ## Example:\n      ## pullSecrets:\n      ##   - myRegistryKeySecretName\n      ##\n      pullSecrets: []\n\n    ## Prometheus JMX Exporter Containers' Security Context\n    ## @param metrics.jmx.containerSecurityContext.enabled Enable Prometheus JMX Exporter Containers' Security Context\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n    ## Example:\n    ##   containerSecurityContext:\n    ##     enabled: true\n    ##     capabilities:\n    ##       drop: [\"NET_RAW\"]\n    ##     readOnlyRootFilesystem: true\n    ##\n    containerSecurityContext:\n      enabled: false\n    ## Prometheus JMX Exporter' resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## We usually recommend not to specify default resources and to leave this as a conscious\n    ## choice for the user. This also increases chances charts run on environments with little\n    ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n    ## @param metrics.jmx.resources.limits JMX Exporter container resource limits\n    ## @param metrics.jmx.resources.requests JMX Exporter container resource requests\n    ##\n    resources:\n      ## Example:\n      ## limits:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      limits: {}\n      ## Examples:\n      ## requests:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      requests: {}\n    ## Service configuration\n    ##\n    service:\n      ## @param metrics.jmx.service.type Kubernetes service type (`ClusterIP`, `NodePort` or `LoadBalancer`) for JMX Exporter\n      ##\n      type: ClusterIP\n      ## @param metrics.jmx.service.port JMX Exporter Prometheus port\n      ##\n      port: 5556\n      ## @param metrics.jmx.service.nodePort Kubernetes HTTP node port\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n      ##\n      nodePort: \"\"\n      ## @param metrics.jmx.service.loadBalancerIP loadBalancerIP if service type is `LoadBalancer`\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n      ##\n      loadBalancerIP: \"\"\n      ## @param metrics.jmx.service.loadBalancerSourceRanges Load Balancer sources\n      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n      ## Example:\n      ## loadBalancerSourceRanges:\n      ## - 10.10.10.0/24\n      ##\n      loadBalancerSourceRanges: []\n      ## @param metrics.jmx.service.clusterIP Static clusterIP or None for headless services\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\n      ##\n      clusterIP: \"\"\n      ## @param metrics.jmx.service.annotations [object] Annotations for the JMX Exporter Prometheus metrics service\n      ##\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"{{ .Values.metrics.jmx.service.port }}\"\n        prometheus.io/path: \"/\"\n    ## @param metrics.jmx.whitelistObjectNames Allows setting which JMX objects you want to expose to via JMX stats to JMX Exporter\n    ## Only whitelisted values will be exposed via JMX Exporter. They must also be exposed via Rules. To expose all metrics\n    ## (warning its crazy excessive and they aren't formatted in a prometheus style) (1) `whitelistObjectNames: []`\n    ## (2) commented out above `overrideConfig`.\n    ##\n    whitelistObjectNames:\n      - kafka.controller:*\n      - kafka.server:*\n      - java.lang:*\n      - kafka.network:*\n      - kafka.log:*\n    ## @param metrics.jmx.config [string] Configuration file for JMX exporter\n    ## Specify content for jmx-kafka-prometheus.yml. Evaluated as a template\n    ##\n    ## Credits to the incubator/kafka chart for the JMX configuration.\n    ## https://github.com/helm/charts/tree/master/incubator/kafka\n    ##\n    config: |-\n      jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi\n      lowercaseOutputName: true\n      lowercaseOutputLabelNames: true\n      ssl: false\n      {{- if .Values.metrics.jmx.whitelistObjectNames }}\n      whitelistObjectNames: [\"{{ join \"\\\",\\\"\" .Values.metrics.jmx.whitelistObjectNames }}\"]\n      {{- end }}\n    ## @param metrics.jmx.existingConfigmap Name of existing ConfigMap with JMX exporter configuration\n    ## NOTE: This will override metrics.jmx.config\n    ##\n    existingConfigmap: \"\"\n  ## Prometheus Operator ServiceMonitor configuration\n  ##\n  serviceMonitor:\n    ## @param metrics.serviceMonitor.enabled if `true`, creates a Prometheus Operator ServiceMonitor (requires `metrics.kafka.enabled` or `metrics.jmx.enabled` to be `true`)\n    ##\n    enabled: false\n    ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running\n    ##\n    namespace: \"\"\n    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint\n    ##\n    interval: \"\"\n    ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint\n    ##\n    scrapeTimeout: \"\"\n    ## @param metrics.serviceMonitor.selector ServiceMonitor selector labels\n    ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration\n    ## e.g:\n    ## selector:\n    ##   prometheus: my-prometheus\n    ##\n    selector: {}\n    ## @param metrics.serviceMonitor.relabelings Relabel configuration for the metrics\n    ##\n    relabelings: []\n    ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion\n    ##\n    metricRelabelings: []\n\n## @section Kafka provisioning parameters\n\n## Kafka provisioning\n##\nprovisioning:\n  ## @param provisioning.enabled Enable kafka provisioning Job\n  ##\n  enabled: false\n  ## @param provisioning.numPartitions Default number of partitions for topics when unspecified.\n  numPartitions: 1\n  ## @param provisioning.replicationFactor Default replication factor for topics when unspecified.\n  replicationFactor: 1\n  ## @param provisioning.schedulerName Name of the k8s scheduler (other than default) for kafka provisioning\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  schedulerName: \"\"\n  ## @param provisioning.podAnnotations Provisioning Pod annotations.\n  ##\n  podAnnotations: {}\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param provisioning.resources.limits The resources limits for the container\n  ## @param provisioning.resources.requests The requested resources for the container\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 250m\n    ##    memory: 1Gi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##    cpu: 250m\n    ##    memory: 256Mi\n    requests: {}\n  ## @param provisioning.command Override provisioning container command\n  ##\n  command: []\n  ## @param provisioning.args Override provisioning container arguments\n  ##\n  args: []\n  ## @param provisioning.topics Kafka provisioning topics\n  ## - name: topic-name\n  ##   partitions: 1\n  ##   replicationFactor: 1\n  ##   ## https://kafka.apache.org/documentation/#topicconfigs\n  ##   config:\n  ##     max.message.bytes: 64000\n  ##     flush.messages: 1\n  ##\n  topics: []\n\n## @section Zookeeper chart parameters\n\n## Zookeeper chart configuration\n## https://github.com/bitnami/charts/blob/master/bitnami/zookeeper/values.yaml\n##\nzookeeper:\n  ## @param zookeeper.enabled Switch to enable or disable the Zookeeper helm chart\n  ##\n  enabled: true\n  auth:\n    ## @param zookeeper.auth.enabled Enable Zookeeper auth\n    ##\n    enabled: false\n    ## @param zookeeper.auth.clientUser User that will use Zookeeper clients to auth\n    ##\n    clientUser: \"\"\n    ## @param zookeeper.auth.clientPassword Password that will use Zookeeper clients to auth\n    ##\n    clientPassword: \"\"\n    ## @param zookeeper.auth.serverUsers Comma, semicolon or whitespace separated list of user to be created. Specify them as a string, for example: \"user1,user2,admin\"\n    ##\n    serverUsers: \"\"\n    ## @param zookeeper.auth.serverPasswords Comma, semicolon or whitespace separated list of passwords to assign to users when created. Specify them as a string, for example: \"pass4user1, pass4user2, pass4admin\"\n    ##\n    serverPasswords: \"\"\n## This value is only used when zookeeper.enabled is set to false\n##\nexternalZookeeper:\n  ## @param externalZookeeper.servers Server or list of external Zookeeper servers to use\n  ##\n  servers: []\n"
            ],
            "verify": false,
            "version": "14.8.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.istio_install.helm_release.istio_base",
            "module.istio_install.helm_release.istio_ingress",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.minio",
            "module.kube.kubernetes_namespace.mlflow",
            "module.kube.kubernetes_namespace.spark"
          ]
        }
      ]
    },
    {
      "module": "module.keycloak_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "keycloak",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "keycloak",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "keycloak",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "15.0.2",
                "chart": "keycloak",
                "name": "keycloak",
                "namespace": "keycloak",
                "revision": 1,
                "values": "{\"affinity\":{},\"args\":[],\"auth\":{\"adminPassword\":\"admin\",\"adminUser\":\"lakehouse_admin\",\"createAdminUser\":true,\"existingSecret\":\"\",\"existingSecretPerPassword\":{},\"managementPassword\":\"\",\"managementUser\":\"manager\",\"tls\":{\"autoGenerated\":false,\"enabled\":false,\"existingSecret\":\"\",\"jksSecret\":\"\",\"keystoreFilename\":\"\",\"keystorePassword\":\"\",\"resources\":{\"limits\":{},\"requests\":{}},\"truststoreFilename\":\"\",\"truststorePassword\":\"\"}},\"autoscaling\":{\"enabled\":false,\"maxReplicas\":11,\"minReplicas\":1,\"targetCPU\":\"\",\"targetMemory\":\"\"},\"cache\":{\"authOwnersCount\":1,\"ownersCount\":1},\"clusterDomain\":\"cluster.local\",\"command\":[],\"commonAnnotations\":{},\"commonLabels\":{},\"configuration\":\"\",\"containerPorts\":{\"http\":8080,\"https\":8443},\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"existingConfigmap\":\"\",\"externalDatabase\":{\"database\":\"bitnami_keycloak\",\"existingSecret\":\"\",\"host\":\"\",\"password\":\"\",\"port\":5432,\"user\":\"bn_keycloak\"},\"extraDeploy\":[],\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraStartupArgs\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"storageClass\":\"\"},\"hostAliases\":[],\"image\":{\"debug\":false,\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/keycloak\",\"tag\":\"15.0.2-debian-10-r94\"},\"ingress\":{\"annotations\":{},\"apiVersion\":\"\",\"enabled\":false,\"existingSecret\":\"\",\"extraHosts\":[],\"extraTls\":[],\"hostname\":\"keycloak.local\",\"ingressClassName\":\"\",\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"secrets\":[],\"servicePort\":\"https\",\"tls\":false},\"initContainers\":[],\"initdbScripts\":{},\"initdbScriptsConfigMap\":\"\",\"keycloakConfigCli\":{\"annotations\":{\"helm.sh/hook\":\"post-install,post-upgrade,post-rollback\",\"helm.sh/hook-delete-policy\":\"hook-succeeded,before-hook-creation\",\"helm.sh/hook-weight\":\"5\"},\"args\":[],\"backoffLimit\":1,\"command\":[],\"configuration\":{},\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"enabled\":false,\"existingConfigmap\":\"\",\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/keycloak-config-cli\",\"tag\":\"4.3.0-debian-10-r54\"},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"resources\":{\"limits\":{},\"requests\":{}}},\"kubeVersion\":\"\",\"lifecycleHooks\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"httpGet\":{\"path\":\"/auth/\",\"port\":\"http\"},\"initialDelaySeconds\":300,\"periodSeconds\":1,\"successThreshold\":1,\"timeoutSeconds\":5},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{\"prometheus.io/port\":\"{{ .Values.metrics.service.port }}\",\"prometheus.io/scrape\":\"true\"},\"port\":9990},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"namespace\":\"\",\"relabellings\":[],\"scrapeTimeout\":\"\"}},\"nameOverride\":\"\",\"networkPolicy\":{\"additionalRules\":{},\"allowExternal\":true,\"enabled\":false},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"pdb\":{\"create\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"postgresql\":{\"enabled\":true,\"existingSecret\":\"\",\"persistence\":{\"enabled\":true},\"postgresqlDatabase\":\"bitnami_keycloak\",\"postgresqlPassword\":\"\",\"postgresqlUsername\":\"bn_keycloak\"},\"priorityClassName\":\"\",\"proxyAddressForwarding\":true,\"rbac\":{\"create\":false,\"rules\":[]},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"httpGet\":{\"path\":\"/auth/realms/master\",\"port\":\"http\"},\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicaCount\":1,\"resources\":{\"limits\":{},\"requests\":{}},\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalTrafficPolicy\":\"Cluster\",\"httpsPort\":443,\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"http\":\"\",\"https\":\"\"},\"port\":80,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"automountServiceAccountToken\":false,\"create\":true,\"name\":\"\"},\"serviceDiscovery\":{\"enabled\":false,\"properties\":[],\"protocol\":\"kubernetes.KUBE_PING\",\"transportStack\":\"tcp\"},\"sidecars\":[],\"startupProbe\":{\"enabled\":false,\"failureThreshold\":60,\"httpGet\":{\"path\":\"/auth/\",\"port\":\"http\"},\"initialDelaySeconds\":30,\"periodSeconds\":5,\"successThreshold\":1,\"timeoutSeconds\":1},\"tolerations\":[],\"updateStrategy\":{\"type\":\"RollingUpdate\"}}",
                "version": "5.2.2"
              }
            ],
            "name": "keycloak",
            "namespace": "keycloak",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.bitnami.com/bitnami",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1000,
            "values": [
              "## @section Global parameters\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass\n\n## @param global.imageRegistry Global Docker image registry\n## @param global.imagePullSecrets Global Docker registry secret names as an array\n## @param global.storageClass Global StorageClass for Persistent Volume(s)\n##\nglobal:\n  imageRegistry: \"\"\n  ## E.g.\n  ## imagePullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  imagePullSecrets: []\n  storageClass: \"\"\n\n## @section Common parameters\n\n## @param kubeVersion Force target Kubernetes version (using Helm capabilities if not set)\n##\nkubeVersion: \"\"\n## @param nameOverride String to partially override keycloak.fullname\n##\nnameOverride: \"\"\n## @param fullnameOverride String to fully override keycloak.fullname\n##\nfullnameOverride: \"\"\n## @param hostAliases Add deployment host aliases\n## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n##\nhostAliases: []\n## @param commonLabels Labels to add to all deployed objects\n##\ncommonLabels: {}\n## @param commonAnnotations Annotations to add to all deployed objects\n##\ncommonAnnotations: {}\n## @param clusterDomain Default Kubernetes cluster domain\n##\nclusterDomain: cluster.local\n## @param extraDeploy Array of extra objects to deploy with the release\n##\nextraDeploy: []\n\n## @section Keycloak parameters\n\n## Bitnami Keycloak image version\n## ref: https://hub.docker.com/r/bitnami/keycloak/tags/\n## @param image.registry Keycloak image registry\n## @param image.repository Keycloak image repository\n## @param image.tag Keycloak image tag (immutable tags are recommended)\n## @param image.pullPolicy Keycloak image pull policy\n## @param image.pullSecrets Specify docker-registry secret names as an array\n## @param image.debug Specify if debug logs should be enabled\n##\nimage:\n  registry: docker.io\n  repository: bitnami/keycloak\n  tag: 15.0.2-debian-10-r94\n  ## Specify a imagePullPolicy\n  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n  ##\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## Example:\n  ## pullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  pullSecrets: []\n  ## Set to true if you would like to see extra information on logs\n  ##\n  debug: false\n## Keycloak authentication parameters\n## ref: https://github.com/bitnami/bitnami-docker-keycloak#admin-credentials\n##\nauth:\n  ## @param auth.createAdminUser Create administrator user on boot\n  ##\n  createAdminUser: true\n  ## @param auth.adminUser Keycloak administrator user\n  ##\n  adminUser: lakehouse_admin\n  ## @param auth.adminPassword Keycloak administrator password for the new user\n  ##\n  adminPassword: \"admin\"\n  ## @param auth.managementUser Wildfly management user\n  ##\n  managementUser: manager\n  ## @param auth.managementPassword Wildfly management password\n  ##\n  managementPassword: \"\"\n  ## @param auth.existingSecret An already existing secret containing auth info\n  ## e.g:\n  ## existingSecret:\n  ##   name: mySecret\n  ##   keyMapping:\n  ##     admin-password: myPasswordKey\n  ##     management-password: myManagementPasswordKey\n  ##     database-password: myDatabasePasswordKey\n  ##     tls-keystore-password: myTlsKeystorePasswordKey\n  ##     tls-truestore-password: myTlsTruestorePasswordKey\n  ##\n  existingSecret: \"\"\n  ## @param auth.existingSecretPerPassword Override `existingSecret` and other secret values\n  ## e.g:\n  ## existingSecretPerPassword:\n  ##   keyMapping:\n  ##     adminPassword: KEYCLOAK_ADMIN_PASSWORD\n  ##     managementPassword: KEYCLOAK_MANAGEMENT_PASSWORD\n  ##     databasePassword: password\n  ##     tlsKeystorePassword: JKS_KEYSTORE_TRUSTSTORE_PASSWORD\n  ##     tlsTruststorePassword: JKS_KEYSTORE_TRUSTSTORE_PASSWORD\n  ##   adminPassword:\n  ##     name: keycloak-test2.credentials ## release-name\n  ##   managementPassword:\n  ##     name: keycloak-test2.credentials\n  ##   databasePassword:\n  ##     name: keycloak.pocwatt-keycloak-cluster.credentials\n  ##   tlsKeystorePassword:\n  ##     name: keycloak-test2.credentials\n  ##   tlsTruststorePassword:\n  ##     name: keycloak-test2.credentials\n  ##\n  existingSecretPerPassword: {}\n  ## TLS encryption parameters\n  ## ref: https://github.com/bitnami/bitnami-docker-keycloak#tls-encryption\n  ##\n  tls:\n    ## @param auth.tls.enabled Enable TLS encryption\n    ##\n    enabled: false\n    ## @param auth.tls.autoGenerated Generate automatically self-signed TLS certificates. Currently only supports PEM certificates\n    ##\n    autoGenerated: false\n    ## @param auth.tls.existingSecret Existing secret containing the TLS certificates per Keycloak replica\n    ## Create this secret following the steps below:\n    ## 1) Generate your trustore and keystore files (more info at https://www.keycloak.org/docs/latest/server_installation/#_setting_up_ssl)\n    ## 2) Rename your truststore to `keycloak.truststore.jks`.\n    ## 3) Rename your keystores to `keycloak-X.keystore.jks` where X is the ID of each Keycloak replica\n    ## 4) Run the command below where SECRET_NAME is the name of the secret you want to create:\n    ##       kubectl create secret generic SECRET_NAME --from-file=./keycloak.truststore.jks --from-file=./keycloak-0.keystore.jks --from-file=./keycloak-1.keystore.jks ...\n    ##\n    existingSecret: \"\"\n    ## @param auth.tls.truststoreFilename Truststore specific filename inside the existing secret\n    ## Note: Setting up this value, you will use the same trustore file in all the replicas\n    ##\n    truststoreFilename: \"\"\n    ## @param auth.tls.keystoreFilename Keystore specific filename inside the existing secret\n    ## Note: Setting up this value, you will use the same trustore file in all the replicas\n    ##\n    keystoreFilename: \"\"\n    ## @param auth.tls.jksSecret DEPRECATED. Use `auth.tls.existingSecret` instead\n    ##\n    jksSecret: \"\"\n    ## @param auth.tls.keystorePassword Password to access the keystore when it's password-protected\n    ##\n    keystorePassword: \"\"\n    ## @param auth.tls.truststorePassword Password to access the truststore when it's password-protected\n    ##\n    truststorePassword: \"\"\n    ## Init containers' resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## We usually recommend not to specify default resources and to leave this as a conscious\n    ## choice for the user. This also increases chances charts run on environments with little\n    ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n    ## @param auth.tls.resources.limits The resources limits for the TLS init container\n    ## @param auth.tls.resources.requests The requested resources for the TLS init container\n    ##\n    resources:\n      ## Example:\n      ## limits:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      limits: {}\n      ## Examples:\n      ## requests:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      requests: {}\n## @param proxyAddressForwarding Enable Proxy Address Forwarding\n## ref: https://www.keycloak.org/docs/latest/server_installation/#_setting-up-a-load-balancer-or-proxy\n##\nproxyAddressForwarding: true\n## Keycloak Service Discovery settings\n## ref: https://github.com/bitnami/bitnami-docker-keycloak#cluster-configuration\n##\nserviceDiscovery:\n  ## @param serviceDiscovery.enabled Enable Service Discovery for Keycloak (required if `replicaCount` \u003e `1`)\n  ##\n  enabled: false\n  ## @param serviceDiscovery.protocol Sets the protocol that Keycloak nodes would use to discover new peers\n  ## Available protocols can be found at http://www.jgroups.org/javadoc3/org/jgroups/protocols/\n  ##\n  protocol: kubernetes.KUBE_PING\n  ## @param serviceDiscovery.properties Properties for the discovery protocol set in `serviceDiscovery.protocol` parameter\n  ## List of key=\u003evalue pairs\n  ## Example:\n  ## properties:\n  ##   - datasource_jndi_name=\u003e\"java:jboss/datasources/KeycloakDS\"\n  ##   - initialize_sql=\u003e\"CREATE TABLE IF NOT EXISTS JGROUPSPING ( own_addr varchar(200) NOT NULL, cluster_name varchar(200) NOT NULL, created timestamp default current_timestamp, ping_data BYTEA, constraint PK_JGROUPSPING PRIMARY KEY (own_addr, cluster_name))\"\n  ##\n  properties: []\n  ## @param serviceDiscovery.transportStack Transport stack for the discovery protocol set in `serviceDiscovery.protocol` parameter\n  ##\n  transportStack: tcp\n## Keycloak cache settings\n## ref: https://github.com/bitnami/bitnami-docker-keycloak#cluster-configuration\n##\ncache:\n  ## @param cache.ownersCount Number of nodes that will replicate cached data\n  ##\n  ownersCount: 1\n  ## @param cache.authOwnersCount Number of nodes that will replicate cached authentication data\n  ##\n  authOwnersCount: 1\n## @param configuration Keycloak Configuration. Auto-generated based on other parameters when not specified\n## Specify content for standalone-ha.xml\n## NOTE: This will override configuring Keycloak based on environment variables (including those set by the chart)\n## The standalone-ha.xml is auto-generated based on other parameters when this parameter is not specified\n##\n## Example:\n## configuration: |-\n##    foo: bar\n##    baz:\n##\nconfiguration: \"\"\n## @param existingConfigmap Name of existing ConfigMap with Keycloak configuration\n## NOTE: When it's set the configuration parameter is ignored\n##\nexistingConfigmap: \"\"\n## @param extraStartupArgs Extra default startup args\n##\nextraStartupArgs: \"\"\n## @param initdbScripts Dictionary of initdb scripts\n## Specify dictionary of scripts to be run at first boot\n## ref: https://github.com/bitnami/bitnami-docker-keycloak#initializing-a-new-instance\n## Example:\n## initdbScripts:\n##   my_init_script.sh: |\n##      #!/bin/bash\n##      echo \"Do something.\"\n##\ninitdbScripts: {}\n## @param initdbScriptsConfigMap ConfigMap with the initdb scripts (Note: Overrides `initdbScripts`)\n##\ninitdbScriptsConfigMap: \"\"\n## @param command Override default container command (useful when using custom images)\n##\ncommand: []\n## @param args Override default container args (useful when using custom images)\n##\nargs: []\n## @param extraEnvVars Extra environment variables to be set on Keycloak container\n## Example:\n## extraEnvVars:\n##   - name: FOO\n##     value: \"bar\"\n##\nextraEnvVars: []\n## @param extraEnvVarsCM Name of existing ConfigMap containing extra env vars\n##\nextraEnvVarsCM: \"\"\n## @param extraEnvVarsSecret Name of existing Secret containing extra env vars\n##\nextraEnvVarsSecret: \"\"\n\n## @section keycloak-config-cli parameters\n\n## Configuration for keycloak-config-cli\n## ref: https://github.com/adorsys/keycloak-config-cli\n##\nkeycloakConfigCli:\n  ## @param keycloakConfigCli.enabled Whether to enable keycloak-config-cli\n  ##\n  enabled: false\n  ## Bitnami keycloak-config-cli image\n  ## ref: https://hub.docker.com/r/bitnami/keycloak-config-cli/tags/\n  ## @param keycloakConfigCli.image.registry keycloak-config-cli container image registry\n  ## @param keycloakConfigCli.image.repository keycloak-config-cli container image repository\n  ## @param keycloakConfigCli.image.tag keycloak-config-cli container image tag\n  ## @param keycloakConfigCli.image.pullPolicy keycloak-config-cli container image pull policy\n  ## @param keycloakConfigCli.image.pullSecrets keycloak-config-cli container image pull secrets\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/keycloak-config-cli\n    tag: 4.3.0-debian-10-r54\n    ## Specify a imagePullPolicy\n    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n    ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n    ##\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets.\n    ## Secrets must be manually created in the namespace.\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## e.g:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## @param keycloakConfigCli.annotations [object] Annotations for keycloak-config-cli job\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n  ##\n  annotations:\n    helm.sh/hook: \"post-install,post-upgrade,post-rollback\"\n    helm.sh/hook-delete-policy: \"hook-succeeded,before-hook-creation\"\n    helm.sh/hook-weight: \"5\"\n  ## @param keycloakConfigCli.command Command for running the container (set to default if not set). Use array form\n  ##\n  command: []\n  ## @param keycloakConfigCli.args Args for running the container (set to default if not set). Use array form\n  ##\n  args: []\n  ## @param keycloakConfigCli.hostAliases Job pod host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ##\n  hostAliases: []\n  ## Keycloak config CLI resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param keycloakConfigCli.resources.limits The resources limits for the keycloak-config-cli container\n  ## @param keycloakConfigCli.resources.requests The requested resources for the keycloak-config-cli container\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 200m\n    ##    memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##    cpu: 200m\n    ##    memory: 10Mi\n    requests: {}\n  ## keycloak-config-cli containers' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param keycloakConfigCli.containerSecurityContext.enabled Enabled keycloak-config-cli containers' Security Context\n  ## @param keycloakConfigCli.containerSecurityContext.runAsUser Set keycloak-config-cli container's Security Context runAsUser\n  ## @param keycloakConfigCli.containerSecurityContext.runAsNonRoot Set keycloak-config-cli container's Security Context runAsNonRoot\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1001\n    runAsNonRoot: true\n  ## keycloak-config-cli pods' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n  ## @param keycloakConfigCli.podSecurityContext.enabled Enabled keycloak-config-cli pods' Security Context\n  ## @param keycloakConfigCli.podSecurityContext.fsGroup Set keycloak-config-cli pod's Security Context fsGroup\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1001\n  ## @param keycloakConfigCli.backoffLimit Number of retries before considering a Job as failed\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-backoff-failure-policy\n  ##\n  backoffLimit: 1\n  ## @param keycloakConfigCli.podLabels Pod extra labels\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n  ##\n  podLabels: {}\n  ## @param keycloakConfigCli.podAnnotations Annotations for job pod\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n  ##\n  podAnnotations: {}\n  ## @param keycloakConfigCli.extraEnvVars Additional environment variables to set\n  ## Example:\n  ## extraEnvVars:\n  ##   - name: FOO\n  ##     value: \"bar\"\n  ##\n  extraEnvVars: []\n  ## @param keycloakConfigCli.extraEnvVarsCM ConfigMap with extra environment variables\n  ##\n  extraEnvVarsCM: \"\"\n  ## @param keycloakConfigCli.extraEnvVarsSecret Secret with extra environment variables\n  ##\n  extraEnvVarsSecret: \"\"\n  ## @param keycloakConfigCli.extraVolumes Extra volumes to add to the job\n  ##\n  extraVolumes: []\n  ## @param keycloakConfigCli.extraVolumeMounts Extra volume mounts to add to the container\n  ##\n  extraVolumeMounts: []\n  ## @param keycloakConfigCli.configuration keycloak-config-cli realms configuration\n  ## NOTE: nil keys will be considered files to import locally\n  ## Example:\n  ## configuration:\n  ##   realm1.json: |\n  ##     {\n  ##       \"realm\": \"realm1\",\n  ##       \"clients\": []\n  ##     }\n  ##   files/realm2.yaml:\n  ##   realm3.yaml: |\n  ##     realm: realm3\n  ##     clients: []\n  ##\n  configuration: {}\n  ## @param keycloakConfigCli.existingConfigmap ConfigMap with keycloak-config-cli configuration. This will override `keycloakConfigCli.config`\n  ## NOTE: This will override keycloakConfigCli.configuration\n  ##\n  existingConfigmap: \"\"\n\n## @section Keycloak deployment/statefulset parameters\n\n## @param replicaCount Number of Keycloak replicas to deploy\n##\nreplicaCount: 1\n## @param containerPorts [object] Keycloak container ports to open\n##\ncontainerPorts:\n  http: 8080\n  https: 8443\n## Keycloak containers' SecurityContext\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n## @param podSecurityContext.enabled Enabled Keykloak pods' Security Context\n## @param podSecurityContext.fsGroup Set Keykloak pod's Security Context fsGroup\n##\npodSecurityContext:\n  enabled: true\n  fsGroup: 1001\n## Keycloak pods' Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n## @param containerSecurityContext.enabled Enabled Keykloak containers' Security Context\n## @param containerSecurityContext.runAsUser Set Keykloak container's Security Context runAsUser\n## @param containerSecurityContext.runAsNonRoot Set Keykloak container's Security Context runAsNonRoot\n##\ncontainerSecurityContext:\n  enabled: true\n  runAsUser: 1001\n  runAsNonRoot: true\n## Keycloak resource requests and limits\n## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n## We usually recommend not to specify default resources and to leave this as a conscious\n## choice for the user. This also increases chances charts run on environments with little\n## resources, such as Minikube. If you do want to specify resources, uncomment the following\n## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n## @param resources.limits The resources limits for the Keycloak container\n## @param resources.requests The requested resources for the Keycloak container\n##\nresources:\n  ## Example:\n  ## limits:\n  ##    cpu: 200m\n  ##    memory: 256Mi\n  limits: {}\n  ## Examples:\n  ## requests:\n  ##    cpu: 200m\n  ##    memory: 10Mi\n  requests: {}\n## Configure extra options for startup probe\n## When enabling this, make sure to set initialDelaySeconds to 0 for livenessProbe and readinessProbe\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n## @param startupProbe.enabled Enable startupProbe\n## @param startupProbe.httpGet.path Request path for startupProbe\n## @param startupProbe.httpGet.port Port for startupProbe\n## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n## @param startupProbe.periodSeconds Period seconds for startupProbe\n## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe\n## @param startupProbe.failureThreshold Failure threshold for startupProbe\n## @param startupProbe.successThreshold Success threshold for startupProbe\n##\nstartupProbe:\n  enabled: false\n  httpGet:\n    path: /auth/\n    port: http\n  initialDelaySeconds: 30\n  periodSeconds: 5\n  timeoutSeconds: 1\n  failureThreshold: 60\n  successThreshold: 1\n## Configure extra options for liveness probe\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n## @param livenessProbe.enabled Enable livenessProbe\n## @param livenessProbe.httpGet.path Request path for livenessProbe\n## @param livenessProbe.httpGet.port Port for livenessProbe\n## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n## @param livenessProbe.periodSeconds Period seconds for livenessProbe\n## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n## @param livenessProbe.failureThreshold Failure threshold for livenessProbe\n## @param livenessProbe.successThreshold Success threshold for livenessProbe\n##\nlivenessProbe:\n  enabled: true\n  httpGet:\n    path: /auth/\n    port: http\n  initialDelaySeconds: 300\n  periodSeconds: 1\n  timeoutSeconds: 5\n  failureThreshold: 3\n  successThreshold: 1\n## Configure extra options for readiness probe\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n## @param readinessProbe.enabled Enable readinessProbe\n## @param readinessProbe.httpGet.path Request path for readinessProbe\n## @param readinessProbe.httpGet.port Port for readinessProbe\n## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n## @param readinessProbe.periodSeconds Period seconds for readinessProbe\n## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n## @param readinessProbe.failureThreshold Failure threshold for readinessProbe\n## @param readinessProbe.successThreshold Success threshold for readinessProbe\n##\nreadinessProbe:\n  enabled: true\n  httpGet:\n    path: /auth/realms/master\n    port: http\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 1\n  failureThreshold: 3\n  successThreshold: 1\n## @param customStartupProbe Custom Startup probes for Keycloak\n##\ncustomStartupProbe: {}\n## @param customLivenessProbe Custom Liveness probes for Keycloak\n##\ncustomLivenessProbe: {}\n## @param customReadinessProbe Custom Rediness probes Keycloak\n##\ncustomReadinessProbe: {}\n## Strategy to use to update Pods\n##\nupdateStrategy:\n  ## @param updateStrategy.type StrategyType\n  ## Can be set to RollingUpdate or OnDelete\n  ##\n  type: RollingUpdate\n## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAffinityPreset: \"\"\n## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAntiAffinityPreset: soft\n## Node affinity preset\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n##\nnodeAffinityPreset:\n  ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ##\n  type: \"\"\n  ## @param nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set.\n  ## E.g.\n  ## key: \"kubernetes.io/e2e-az-name\"\n  ##\n  key: \"\"\n  ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.\n  ## E.g.\n  ## values:\n  ##   - e2e-az1\n  ##   - e2e-az2\n  ##\n  values: []\n## @param affinity Affinity for pod assignment\n## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n##\naffinity: {}\n## @param nodeSelector Node labels for pod assignment\n## ref: https://kubernetes.io/docs/user-guide/node-selection/\n##\nnodeSelector: {}\n## @param tolerations Tolerations for pod assignment\n## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n##\ntolerations: []\n## @param podLabels Extra labels for Keycloak pods\n## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n##\npodLabels: {}\n## @param podAnnotations Annotations for Keycloak pods\n## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n##\npodAnnotations: {}\n## @param priorityClassName Keycloak pods' priority.\n## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n##\npriorityClassName: \"\"\n## @param lifecycleHooks LifecycleHooks to set additional configuration at startup\n##\nlifecycleHooks: {}\n## @param extraVolumes Optionally specify extra list of additional volumes for Keycloak pods\n##\nextraVolumes: []\n## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for Keycloak container(s)\n##\nextraVolumeMounts: []\n## @param initContainers Add additional init containers to the Keycloak pods\n## Example:\n## initContainers:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\ninitContainers: []\n## @param sidecars Add additional sidecar containers to the Keycloak pods\n## Example:\n## sidecars:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\nsidecars: []\n\n## @section Exposure parameters\n\n## Service configuration\n##\nservice:\n  ## @param service.type Kubernetes service type\n  ##\n  type: ClusterIP\n  ## @param service.port Service HTTP port\n  ##\n  port: 80\n  ## @param service.httpsPort HTTPS Port\n  ##\n  httpsPort: 443\n  ## @param service.nodePorts [object] Specify the nodePort values for the LoadBalancer and NodePort service types.\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n  ##\n  nodePorts:\n    http: \"\"\n    https: \"\"\n  ## @param service.clusterIP Keycloak service clusterIP IP\n  ## e.g:\n  ## clusterIP: None\n  ##\n  clusterIP: \"\"\n  ## @param service.loadBalancerIP loadBalancerIP for the SuiteCRM Service (optional, cloud specific)\n  ## ref: https://kubernetes.io/docs/user-guide/services/#type-loadbalancer\n  ##\n  loadBalancerIP: \"\"\n  ## @param service.loadBalancerSourceRanges Address that are allowed when service is LoadBalancer\n  ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n  ## Example:\n  ## loadBalancerSourceRanges:\n  ##   - 10.10.10.0/24\n  ##\n  loadBalancerSourceRanges: []\n  ## @param service.externalTrafficPolicy Enable client source IP preservation\n  ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n  ##\n  externalTrafficPolicy: Cluster\n  ## @param service.annotations Annotations for Keycloak service\n  ##\n  annotations: {}\n## Ingress configuration\n##\ningress:\n  ## @param ingress.enabled Enable ingress controller resource\n  ##\n  enabled: false\n  ## DEPRECATED: Use ingress.annotations instead of ingress.certManager\n  ## certManager: false\n  ##\n\n  ## @param ingress.hostname Default host for the ingress resource\n  ##\n  hostname: keycloak.local\n  ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)\n  ##\n  apiVersion: \"\"\n  ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)\n  ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster\n  ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/\n  ##\n  ingressClassName: \"\"\n  ## @param ingress.path Ingress path\n  ##\n  path: /\n  ## @param ingress.pathType Ingress path type\n  ##\n  pathType: ImplementationSpecific\n  ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.\n  ## For a full list of possible ingress annotations, please see\n  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md\n  ## Use this parameter to set the required annotations for cert-manager, see\n  ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations\n  ##\n  ## e.g:\n  ## annotations:\n  ##   kubernetes.io/ingress.class: nginx\n  ##   cert-manager.io/cluster-issuer: cluster-issuer-name\n  ##\n  annotations: {}\n  ## @param ingress.tls Enable TLS configuration for the hostname defined at `ingress.hostname` parameter\n  ## TLS certificates will be retrieved from a TLS secret with name: {{- printf \"%s-tls\" .Values.ingress.hostname }}\n  ## You can use the ingress.secrets parameter to create this TLS secret, relay on cert-manager to create it, or\n  ## let the chart create self-signed certificates for you\n  ##\n  tls: false\n  ## @param ingress.extraHosts The list of additional hostnames to be covered with this ingress record.\n  ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array\n  ## Example:\n  ## extraHosts:\n  ##   - name: keycloak.local\n  ##     path: /\n  ##\n  extraHosts: []\n  ## @param ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.\n  ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls\n  ## Example:\n  ## extraTls:\n  ## - hosts:\n  ##     - keycloak.local\n  ##   secretName: keycloak.local-tls\n  ##\n  extraTls: []\n  ## @param ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets\n  ## key and certificate should start with -----BEGIN CERTIFICATE----- or -----BEGIN RSA PRIVATE KEY-----\n  ## name should line up with a secretName set further up\n  ##\n  ## If it is not set and you're using cert-manager, this is unneeded, as it will create the secret for you\n  ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created\n  ## It is also possible to create and manage the certificates outside of this helm chart\n  ## Please see README.md for more information\n  ##\n  ## Example\n  ## secrets:\n  ##   - name: aspnet-core.local-tls\n  ##     key: \"\"\n  ##     certificate: \"\"\n  ##\n  secrets: []\n  ## @param ingress.existingSecret It is you own the certificate as secret.\n  existingSecret: \"\"\n  ## @param ingress.servicePort Service port to be used\n  ## Default is http. Alternative is https.\n  ##\n  servicePort: https\n## Network Policy configuration\n## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/\n##\nnetworkPolicy:\n  ## @param networkPolicy.enabled Enable the default NetworkPolicy policy\n  ##\n  enabled: false\n  ## @param networkPolicy.allowExternal Don't require client label for connections\n  ## The Policy model to apply. When set to false, only pods with the correct\n  ## client label will have network access to the ports Keycloak is listening\n  ## on. When true, Keycloak will accept connections from any source\n  ## (with the correct destination port).\n  ##\n  allowExternal: true\n  ## @param networkPolicy.additionalRules Additional NetworkPolicy rules\n  ## Note that all rules are OR-ed.\n  ## Example:\n  ## additionalRules:\n  ##   - matchLabels:\n  ##       - role: frontend\n  ##   - matchExpressions:\n  ##       - key: role\n  ##         operator: In\n  ##         values:\n  ##           - frontend\n  ##\n  additionalRules: {}\n\n## @section RBAC parameter\n## Specifies whether a ServiceAccount should be created\n##\nserviceAccount:\n  ## @param serviceAccount.create Enable the creation of a ServiceAccount for Keycloak pods\n  ##\n  create: true\n  ## @param serviceAccount.name Name of the created ServiceAccount\n  ## If not set and create is true, a name is generated using the fullname template\n  ##\n  name: \"\"\n  ## @param serviceAccount.automountServiceAccountToken Auto-mount the service account token in the pod\n  ##\n  automountServiceAccountToken: false\n## Specifies whether RBAC resources should be created\n##\nrbac:\n  ## @param rbac.create Whether to create and use RBAC resources or not\n  ##\n  create: false\n  ## @param rbac.rules Custom RBAC rules\n  ## Example:\n  ## rules:\n  ##   - apiGroups:\n  ##       - \"\"\n  ##     resources:\n  ##       - pods\n  ##     verbs:\n  ##       - get\n  ##       - list\n  ##\n  rules: []\n\n## @section Other parameters\n\n## Keycloak Pod Disruption Budget configuration\n## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n##\npdb:\n  ## @param pdb.create Enable/disable a Pod Disruption Budget creation\n  ##\n  create: false\n  ## @param pdb.minAvailable Minimum number/percentage of pods that should remain scheduled\n  ##\n  minAvailable: 1\n  ## @param pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable\n  ##\n  maxUnavailable: \"\"\n## Keycloak Autoscaling configuration\n## @param autoscaling.enabled Enable autoscaling for Keycloak\n## @param autoscaling.minReplicas Minimum number of Keycloak replicas\n## @param autoscaling.maxReplicas Maximum number of Keycloak replicas\n## @param autoscaling.targetCPU Target CPU utilization percentage\n## @param autoscaling.targetMemory Target Memory utilization percentage\n##\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 11\n  targetCPU: \"\"\n  targetMemory: \"\"\n\n## @section Metrics parameters\n\n## Metrics configuration\n##\nmetrics:\n  ## @param metrics.enabled Enable exposing Keycloak statistics\n  ## ref: https://github.com/bitnami/bitnami-docker-keycloak#enabling-statistics\n  ##\n  enabled: false\n  ## Keycloak metrics service parameters\n  ##\n  service:\n    ## @param metrics.service.port Service HTTP management port\n    ##\n    port: 9990\n    ## @param metrics.service.annotations [object] Annotations for enabling prometheus to access the metrics endpoints\n    ##\n    annotations:\n      prometheus.io/scrape: \"true\"\n      prometheus.io/port: \"{{ .Values.metrics.service.port }}\"\n  ## Prometheus Operator ServiceMonitor configuration\n  ##\n  serviceMonitor:\n    ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using PrometheusOperator\n    ##\n    enabled: false\n    ## @param metrics.serviceMonitor.namespace Namespace which Prometheus is running in\n    ##\n    namespace: \"\"\n    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped\n    ##\n    interval: 30s\n    ## @param metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended\n    ## e.g:\n    ##   scrapeTimeout: 30s\n    ##\n    scrapeTimeout: \"\"\n    ## @param metrics.serviceMonitor.relabellings Specify Metric Relabellings to add to the scrape endpoint\n    ##\n    relabellings: []\n    ## @param metrics.serviceMonitor.honorLabels honorLabels chooses the metric's labels on collisions with target labels\n    ##\n    honorLabels: false\n    ## @param metrics.serviceMonitor.additionalLabels Used to pass Labels that are required by the installed Prometheus Operator\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec\n    ##\n    additionalLabels: {}\n\n## @section Database parameters\n\n## PostgreSQL chart configuration\n## ref: https://github.com/bitnami/charts/blob/master/bitnami/postgresql/values.yaml\n##\npostgresql:\n  ## @param postgresql.enabled Deploy a PostgreSQL server to satisfy the applications database requirements\n  ##\n  enabled: true\n  ## @param postgresql.postgresqlUsername Keycloak PostgreSQL user (has superuser privileges if username is `postgres`)\n  ## ref: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#setting-the-root-password-on-first-run\n  ##\n  postgresqlUsername: bn_keycloak\n  ## @param postgresql.postgresqlPassword Keycloak PostgreSQL password - ignored if existingSecret is provided\n  ## Defaults to a random 10-character alphanumeric string if not set\n  ## ref: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#setting-the-root-password-on-first-run\n  ##\n  postgresqlPassword: \"\"\n  ## @param postgresql.postgresqlDatabase Name of the database to create\n  ## ref: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#creating-a-database-on-first-run\n  ##\n  postgresqlDatabase: bitnami_keycloak\n  ## @param postgresql.existingSecret Use an existing secret file with the PostgreSQL password\n  ##\n  existingSecret: \"\"\n  ## Enable persistence using Persistent Volume Claims\n  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes\n  ##\n  persistence:\n    ## @param postgresql.persistence.enabled Enable PostgreSQL persistence using PVC\n    ##\n    enabled: true\n## External database configuration\n##\nexternalDatabase:\n  ## @param externalDatabase.host Host of the external database\n  ##\n  host: \"\"\n  ## @param externalDatabase.port Database port\n  ##\n  port: 5432\n  ## @param externalDatabase.user non admin username for Keycloak Database\n  ##\n  user: bn_keycloak\n  ## @param externalDatabase.password Database password\n  ##\n  password: \"\"\n  ## @param externalDatabase.database Database name\n  ##\n  database: bitnami_keycloak\n  ## @param externalDatabase.existingSecret Use an existing secret file with the external PostgreSQL credentials\n  ##\n  existingSecret: \"\"\n"
            ],
            "verify": false,
            "version": "5.2.2",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.minio",
            "module.kube.kubernetes_namespace.mlflow",
            "module.istio_install.helm_release.istio_ingress",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.istio_system",
            "module.istio_install.helm_release.istio_base",
            "module.kube.kubernetes_namespace.mariadb",
            "module.ldap_install.helm_release.openldap",
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.spark"
          ]
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "argo-events",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "argo-events",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "istio-injection": "enabled"
                },
                "name": "argo-events",
                "resource_version": "156735",
                "uid": "041365a2-a5f3-44ec-894c-076cbbb1d601"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "istio_system",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "istio-system",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "istio-system",
                "resource_version": "156726",
                "uid": "6c8101ad-ff32-4b96-8945-6304d1d7b83c"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "jupyterhub",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "jupyterhub",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "istio-injection": "enabled"
                },
                "name": "jupyterhub",
                "resource_version": "156732",
                "uid": "4e8b62d1-0027-495e-ad92-37a2a35ab007"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "kafka",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "kafka",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "istio-injection": "enabled"
                },
                "name": "kafka",
                "resource_version": "231635",
                "uid": "9609009d-4867-4e13-88ba-94fce9f84ef8"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "keycloak",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "keycloak",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "istio-injection": "enabled"
                },
                "name": "keycloak",
                "resource_version": "156734",
                "uid": "f6256727-b1cc-46d5-978d-d958212c723e"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "ldap",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "ldap",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "istio-injection": "enabled"
                },
                "name": "ldap",
                "resource_version": "156731",
                "uid": "3cca2187-760e-410e-9127-5a67f87f132e"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "mariadb",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "mariadb",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "istio-injection": "enabled"
                },
                "name": "mariadb",
                "resource_version": "156729",
                "uid": "970ecee9-b24f-4af1-863d-855d661ce0cd"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "minio",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "minio",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "istio-injection": "enabled"
                },
                "name": "minio",
                "resource_version": "156727",
                "uid": "ecb11411-9314-46e2-bffb-56a5168e4ea2"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "mlflow",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "mlflow",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "istio-injection": "enabled"
                },
                "name": "mlflow",
                "resource_version": "156728",
                "uid": "4f65ca01-52ba-44a8-a709-00439846ea1a"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.kube",
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "spark",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "spark",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "istio-injection": "enabled"
                },
                "name": "spark",
                "resource_version": "156730",
                "uid": "5eed71bf-5905-4ae4-9f15-fdc1cc241ee5"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "module": "module.ldap_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "openldap",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "openldap-stack-ha",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "openldap",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.4.57",
                "chart": "openldap-stack-ha",
                "name": "openldap",
                "namespace": "ldap",
                "revision": 1,
                "values": "{\"adminPassword\":\"admin\",\"configPassword\":\"admin\",\"customFileSets\":[],\"customTLS\":{\"CA\":{\"enabled\":false},\"enabled\":false,\"secret\":\"\"},\"env\":{\"CONTAINER_LOG_LEVEL\":\"4\",\"KEEP_EXISTING_CONFIG\":\"false\",\"LDAP_BACKEND\":\"mdb\",\"LDAP_DOMAIN\":\"lakehouse.home\",\"LDAP_LOG_LEVEL\":\"256\",\"LDAP_ORGANISATION\":\"lakehouse\",\"LDAP_READONLY_USER\":\"false\",\"LDAP_READONLY_USER_PASSWORD\":\"readonly\",\"LDAP_READONLY_USER_USERNAME\":\"readonly\",\"LDAP_REMOVE_CONFIG_AFTER_SETUP\":\"true\",\"LDAP_RFC2307BIS_SCHEMA\":\"false\",\"LDAP_SSL_HELPER_PREFIX\":\"ldap\",\"LDAP_TLS\":\"true\",\"LDAP_TLS_CA_CRT_FILENAME\":\"ca.crt\",\"LDAP_TLS_CIPHER_SUITE\":\"NORMAL\",\"LDAP_TLS_CRT_FILENAME\":\"tls.crt\",\"LDAP_TLS_DH_PARAM_FILENAME\":\"dhparam.pem\",\"LDAP_TLS_ENFORCE\":\"false\",\"LDAP_TLS_KEY_FILENAME\":\"tls.key\",\"LDAP_TLS_PROTOCOL_MIN\":\"3.0\",\"LDAP_TLS_REQCERT\":\"never\",\"LDAP_TLS_VERIFY_CLIENT\":\"never\"},\"existingSecret\":\"\",\"extraLabels\":{},\"extraVolumeMounts\":null,\"extraVolumes\":null,\"image\":{\"pullPolicy\":\"Always\",\"repository\":\"osixia/openldap\",\"tag\":\"1.5.0\"},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":10,\"initialDelaySeconds\":20,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"logLevel\":\"info\",\"ltb-passwd\":{\"enabled\":true,\"ingress\":{\"enabled\":false},\"ldap\":{\"bindDN\":\"cn=admin,dc=lakehouse,dc=fr\",\"bindPWKey\":\"LDAP_ADMIN_PASSWORD\",\"searchBase\":\"dc=lakehouse,dc=fr\",\"server\":\"ldap://openldap-openldap-stack-ha\"}},\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"enabled\":true,\"size\":\"8Gi\"},\"phpldapadmin\":{\"enabled\":true,\"env\":{\"PHPLDAPADMIN_LDAP_HOSTS\":\"openldap-openldap-stack-ha\"},\"ingress\":{\"enabled\":false}},\"podAnnotations\":{},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":10,\"initialDelaySeconds\":20,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicaCount\":1,\"replication\":{\"clusterName\":\"cluster.local\",\"enabled\":true,\"interval\":\"00:00:00:10\",\"retry\":60,\"starttls\":\"critical\",\"timeout\":1,\"tls_reqcert\":\"never\"},\"resources\":{},\"service\":{\"annotations\":{},\"externalIPs\":[],\"ldapPort\":389,\"sessionAffinity\":\"None\",\"sslLdapPort\":636,\"type\":\"ClusterIP\"},\"startupProbe\":{\"enabled\":false,\"failureThreshold\":30,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"test\":{\"enabled\":false,\"image\":{\"repository\":\"dduportal/bats\",\"tag\":\"0.4.0\"}},\"tolerations\":[],\"updateStrategy\":{}}",
                "version": "2.1.6"
              }
            ],
            "name": "openldap",
            "namespace": "ldap",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://jp-gouin.github.io/helm-openldap",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1000,
            "values": [
              "# Default values for openldap.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\nreplicaCount: 1\nupdateStrategy: {}\n  # When a StatefulSet's .spec.updateStrategy.type is set to OnDelete, \n  # the StatefulSet controller will not automatically update the Pods\n  # in a StatefulSet. Users must manually delete Pods to cause the\n  # controller to create new Pods that reflect modifications made\n  # to a StatefulSet's .spec.template.\n  # \n  # type: OnDelete\n  # \n  # or\n  # \n  # When a StatefulSet's .spec.updateStrategy.type is set to RollingUpdate,\n  # the StatefulSet controller will delete and recreate each Pod in the StatefulSet.\n  # It will proceed in the same order as Pod termination (from the largest ordinal \n  # to the smallest), updating each Pod one at a time. It will wait until an updated\n  # Pod is Running and Ready prior to updating its predecessor.\n  # \n  # type: RollingUpdate\n  # rollingUpdate:\n  #   partition: 1\nimage:\n  # From repository https://github.com/osixia/docker-openldap\n  repository: osixia/openldap\n  tag: 1.5.0\n  pullPolicy: Always\n  # pullSecret: harbor\n\n# Set the container log level\n# Valid log levels: none, error, warning, info (default), debug, trace\nlogLevel: info\n\n# Specifies an existing secret to be used for admin and config user passwords\nexistingSecret: \"\"\n\n# Settings for enabling TLS with custom certificate\n# need a secret with tls.crt, tls.key and ca.crt keys with associated files\n# Ref: https://kubernetes.io/docs/tasks/configmap-secret/managing-secret-using-kubectl/#create-a-secret\ncustomTLS:\n  enabled: false\n  secret: \"\"  # The name of a kubernetes.io/tls type secret to use for TLS\n  CA:\n    enabled: false\n## Add additional labels to all resources\nextraLabels: {}\n## Add additional annotations to pods\npodAnnotations: {}\nservice:\n  annotations: {}\n\n  ldapPort: 389\n  sslLdapPort: 636\n\n  ## If service type NodePort, define the value here\n  #ldapPortNodePort:\n  #sslLdapPortNodePort:\n  ## List of IP addresses at which the service is available\n  ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n  ##\n  externalIPs: []\n\n  #loadBalancerIP: \n  #loadBalancerSourceRanges: []\n  type: ClusterIP\n  sessionAffinity: None\n\n# Additional volumes to be mounted to pod\nextraVolumes:\n  # - name: ca-certs\n  #   hostPath:\n  #     path: /etc/ssl/certs/ca-bundle.crt\n  #     type: File\n\nextraVolumeMounts:\n  #- name: ca-certs\n  #  readOnly: true\n  #  mountPath: \"/etc/ssl/certs/ca-certificates.crt\"\n\n# Default configuration for openldap as environment variables. These get injected directly in the container.\n# Use the env variables from https://github.com/osixia/docker-openldap#beginner-guide\nenv:\n LDAP_LOG_LEVEL: \"256\"\n LDAP_ORGANISATION: \"lakehouse\"\n LDAP_DOMAIN: \"lakehouse.home\"\n LDAP_READONLY_USER: \"false\"\n LDAP_READONLY_USER_USERNAME: \"readonly\"\n LDAP_READONLY_USER_PASSWORD: \"readonly\"\n LDAP_RFC2307BIS_SCHEMA: \"false\"\n LDAP_BACKEND: \"mdb\"\n LDAP_TLS: \"true\"\n LDAP_TLS_CRT_FILENAME: \"tls.crt\"\n LDAP_TLS_KEY_FILENAME: \"tls.key\"\n LDAP_TLS_DH_PARAM_FILENAME: \"dhparam.pem\"\n LDAP_TLS_CA_CRT_FILENAME: \"ca.crt\"\n LDAP_TLS_ENFORCE: \"false\"\n CONTAINER_LOG_LEVEL: \"4\"\n LDAP_TLS_REQCERT: \"never\"\n KEEP_EXISTING_CONFIG: \"false\"\n LDAP_REMOVE_CONFIG_AFTER_SETUP: \"true\"\n LDAP_SSL_HELPER_PREFIX: \"ldap\"\n LDAP_TLS_VERIFY_CLIENT: \"never\"\n LDAP_TLS_PROTOCOL_MIN: \"3.0\"\n LDAP_TLS_CIPHER_SUITE: \"NORMAL\"\n\n  \n\n# Default Passwords to use, stored as a secret.\n# You can override these at install time with\n# helm install openldap --set openldap.adminPassword=\u003cpasswd\u003e,openldap.configPassword=\u003cpasswd\u003e\nadminPassword: admin\nconfigPassword: admin\n\n# Custom openldap configuration files used to override default settings\n# customLdifFiles:\n  # 01-default-users.ldif: |-\n    # Predefine users here\n\n# Custom files with provided contents to be added in container.\ncustomFileSets: []\n#- name: fileset1\n#  targetPath: /container/service/slapd/assets/config/bootstrap/ldif\n#  files:\n#  - filename: 03-memberOf.ldif\n#    content: |\n#      dn: cn=module{0},cn=config\n#      changetype: modify\n#      add: olcModuleLoad\n#      olcModuleLoad: memberof\n\nreplication:\n  enabled: true    \n  # Enter the name of your cluster, defaults to \"cluster.local\"\n  clusterName: \"cluster.local\"\n  retry: 60\n  timeout: 1\n  interval: 00:00:00:10\n  starttls: \"critical\"\n  tls_reqcert: \"never\"\n## Persist data to a persistent volume\npersistence:\n  enabled: true\n  ## database data Persistent Volume Storage Class\n  ## If defined, storageClassName: \u003cstorageClass\u003e\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n  ##   GKE, AWS \u0026 OpenStack)\n  ##\n  # storageClass: \"standard-singlewriter\"\n  # existingClaim: openldap-pvc\n  accessModes:\n    - ReadWriteOnce\n  size: 8Gi\n\nlivenessProbe:\n  enabled: true\n  initialDelaySeconds: 20\n  periodSeconds: 10\n  timeoutSeconds: 1\n  successThreshold: 1\n  failureThreshold: 10\nreadinessProbe:\n  enabled: true\n  initialDelaySeconds: 20\n  periodSeconds: 10\n  timeoutSeconds: 1\n  successThreshold: 1\n  failureThreshold: 10\nstartupProbe:\n  enabled: false\n  initialDelaySeconds: 0\n  periodSeconds: 10\n  timeoutSeconds: 1\n  successThreshold: 1\n  failureThreshold: 30\n\nresources: {}\n\nnodeSelector: {}\n\ntolerations: []\n\ntest:\n  enabled: false\n  image:\n    repository: dduportal/bats\n    tag: 0.4.0\nltb-passwd:\n  enabled : true\n  ingress:\n    enabled: false\n  ldap:\n    server: ldap://openldap-openldap-stack-ha\n    searchBase: dc=lakehouse,dc=fr\n    bindDN: cn=admin,dc=lakehouse,dc=fr\n    bindPWKey: LDAP_ADMIN_PASSWORD\n\nphpldapadmin:\n  enabled: true\n  ingress:\n    enabled: false\n  env:\n    PHPLDAPADMIN_LDAP_HOSTS: openldap-openldap-stack-ha"
            ],
            "verify": false,
            "version": "2.1.6",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mlflow",
            "module.kube.kubernetes_namespace.argo-events",
            "module.istio_install.helm_release.istio_ingress",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.minio",
            "module.istio_install.helm_release.istio_base",
            "module.kube.kubernetes_namespace.spark"
          ]
        }
      ]
    },
    {
      "module": "module.mariadb_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "mariadb",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "mariadb",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "mariadb",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "10.5.13",
                "chart": "mariadb",
                "name": "mariadb",
                "namespace": "mariadb",
                "revision": 1,
                "values": "{\"architecture\":\"standalone\",\"auth\":{\"customPasswordFiles\":{},\"database\":\"my_database\",\"existingSecret\":\"\",\"forcePassword\":false,\"password\":\"\",\"replicationPassword\":\"\",\"replicationUser\":\"replicator\",\"rootPassword\":\"admin\",\"usePasswordFiles\":false,\"username\":\"\"},\"clusterDomain\":\"cluster.local\",\"commonAnnotations\":{},\"commonLabels\":{},\"diagnosticMode\":{\"args\":[\"infinity\"],\"command\":[\"sleep\"],\"enabled\":false},\"extraDeploy\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"storageClass\":\"\"},\"image\":{\"debug\":false,\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/mariadb\",\"tag\":\"10.5.13-debian-10-r32\"},\"initdbScripts\":{},\"initdbScriptsConfigMap\":\"\",\"kubeVersion\":\"\",\"metrics\":{\"annotations\":{\"prometheus.io/port\":\"9104\",\"prometheus.io/scrape\":\"true\"},\"containerSecurityContext\":{\"enabled\":false},\"enabled\":false,\"extraArgs\":{\"primary\":[],\"secondary\":[]},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/mysqld-exporter\",\"tag\":\"0.13.0-debian-10-r183\"},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":120,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{\"limits\":{},\"requests\":{}},\"serviceMonitor\":{\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"jobLabel\":\"\",\"labels\":{},\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scrapeTimeout\":\"\",\"selector\":{}}},\"nameOverride\":\"\",\"networkPolicy\":{\"egressRules\":{\"customRules\":{},\"denyConnectionsToExternal\":false},\"enabled\":false,\"ingressRules\":{\"primaryAccessOnlyFrom\":{\"customRules\":{},\"enabled\":false,\"namespaceSelector\":{},\"podSelector\":{}},\"secondaryAccessOnlyFrom\":{\"customRules\":{},\"enabled\":false,\"namespaceSelector\":{},\"podSelector\":{}}},\"metrics\":{\"enabled\":false,\"namespaceSelector\":{},\"podSelector\":{}}},\"primary\":{\"affinity\":{},\"args\":[],\"command\":[],\"configuration\":\"[mysqld]\\nskip-name-resolve\\nexplicit_defaults_for_timestamp\\nbasedir=/opt/bitnami/mariadb\\nplugin_dir=/opt/bitnami/mariadb/plugin\\nport=3306\\nsocket=/opt/bitnami/mariadb/tmp/mysql.sock\\ntmpdir=/opt/bitnami/mariadb/tmp\\nmax_allowed_packet=16M\\nbind-address=::\\npid-file=/opt/bitnami/mariadb/tmp/mysqld.pid\\nlog-error=/opt/bitnami/mariadb/logs/mysqld.log\\ncharacter-set-server=UTF8\\ncollation-server=utf8_general_ci\\n\\n[client]\\nport=3306\\nsocket=/opt/bitnami/mariadb/tmp/mysql.sock\\ndefault-character-set=UTF8\\nplugin_dir=/opt/bitnami/mariadb/plugin\\n\\n[manager]\\nport=3306\\nsocket=/opt/bitnami/mariadb/tmp/mysql.sock\\npid-file=/opt/bitnami/mariadb/tmp/mysqld.pid\",\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"existingConfigmap\":\"\",\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraFlags\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"initContainers\":[],\"lifecycleHooks\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":120,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"pdb\":{\"create\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":true,\"existingClaim\":\"\",\"selector\":{},\"size\":\"8Gi\",\"storageClass\":\"\",\"subPath\":\"\"},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podManagementPolicy\":\"\",\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{\"limits\":{},\"requests\":{}},\"revisionHistoryLimit\":10,\"rollingUpdatePartition\":\"\",\"schedulerName\":\"\",\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalTrafficPolicy\":\"Cluster\",\"extraPorts\":[],\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"mysql\":\"\"},\"ports\":{\"mysql\":3306},\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{},\"type\":\"ClusterIP\"},\"sidecars\":[],\"startupProbe\":{\"enabled\":false,\"failureThreshold\":10,\"initialDelaySeconds\":120,\"periodSeconds\":15,\"successThreshold\":1,\"timeoutSeconds\":5},\"startupWaitOptions\":{},\"tolerations\":[],\"topologySpreadConstraints\":{},\"updateStrategy\":{\"type\":\"RollingUpdate\"}},\"rbac\":{\"create\":false},\"schedulerName\":\"\",\"secondary\":{\"affinity\":{},\"args\":[],\"command\":[],\"configuration\":\"[mysqld]\\nskip-name-resolve\\nexplicit_defaults_for_timestamp\\nbasedir=/opt/bitnami/mariadb\\nport=3306\\nsocket=/opt/bitnami/mariadb/tmp/mysql.sock\\ntmpdir=/opt/bitnami/mariadb/tmp\\nmax_allowed_packet=16M\\nbind-address=0.0.0.0\\npid-file=/opt/bitnami/mariadb/tmp/mysqld.pid\\nlog-error=/opt/bitnami/mariadb/logs/mysqld.log\\ncharacter-set-server=UTF8\\ncollation-server=utf8_general_ci\\n\\n[client]\\nport=3306\\nsocket=/opt/bitnami/mariadb/tmp/mysql.sock\\ndefault-character-set=UTF8\\n\\n[manager]\\nport=3306\\nsocket=/opt/bitnami/mariadb/tmp/mysql.sock\\npid-file=/opt/bitnami/mariadb/tmp/mysqld.pid\",\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"existingConfigmap\":\"\",\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraFlags\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"initContainers\":[],\"lifecycleHooks\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":120,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"pdb\":{\"create\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":true,\"selector\":{},\"size\":\"8Gi\",\"storageClass\":\"\",\"subPath\":\"\"},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podManagementPolicy\":\"\",\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicaCount\":1,\"resources\":{\"limits\":{},\"requests\":{}},\"revisionHistoryLimit\":10,\"rollingUpdatePartition\":\"\",\"schedulerName\":\"\",\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalTrafficPolicy\":\"Cluster\",\"extraPorts\":[],\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"mysql\":\"\"},\"ports\":{\"mysql\":3306},\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{},\"type\":\"ClusterIP\"},\"sidecars\":[],\"startupProbe\":{\"enabled\":false,\"failureThreshold\":10,\"initialDelaySeconds\":120,\"periodSeconds\":15,\"successThreshold\":1,\"timeoutSeconds\":5},\"startupWaitOptions\":{},\"tolerations\":[],\"topologySpreadConstraints\":{},\"updateStrategy\":{\"type\":\"RollingUpdate\"}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":false,\"create\":true,\"name\":\"\"},\"volumePermissions\":{\"enabled\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/bitnami-shell\",\"tag\":\"10-debian-10-r279\"},\"resources\":{\"limits\":{},\"requests\":{}}}}",
                "version": "10.1.1"
              }
            ],
            "name": "mariadb",
            "namespace": "mariadb",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.bitnami.com/bitnami",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1000,
            "values": [
              "## @section Global parameters\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass\n\n## @param global.imageRegistry Global Docker Image registry\n## @param global.imagePullSecrets Global Docker registry secret names as an array\n## @param global.storageClass Global storage class for dynamic provisioning\n##\nglobal:\n  imageRegistry: \"\"\n  ## E.g.\n  ## imagePullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  imagePullSecrets: []\n  storageClass: \"\"\n\n## @section Common parameters\n\n## @param kubeVersion Force target Kubernetes version (using Helm capabilities if not set)\n##\nkubeVersion: \"\"\n## @param nameOverride String to partially override mariadb.fullname\n##\nnameOverride: \"\"\n## @param fullnameOverride String to fully override mariadb.fullname\n##\nfullnameOverride: \"\"\n## @param clusterDomain Default Kubernetes cluster domain\n##\nclusterDomain: cluster.local\n## @param commonAnnotations Common annotations to add to all MariaDB resources (sub-charts are not considered)\n##\ncommonAnnotations: {}\n## @param commonLabels Common labels to add to all MariaDB resources (sub-charts are not considered)\n##\ncommonLabels: {}\n## @param schedulerName Name of the scheduler (other than default) to dispatch pods\n## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n##\nschedulerName: \"\"\n## @param extraDeploy Array of extra objects to deploy with the release (evaluated as a template)\n##\nextraDeploy: []\n\n## Enable diagnostic mode in the deployment\n##\ndiagnosticMode:\n  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)\n  ##\n  enabled: false\n  ## @param diagnosticMode.command Command to override all containers in the deployment\n  ##\n  command:\n    - sleep\n  ## @param diagnosticMode.args Args to override all containers in the deployment\n  ##\n  args:\n    - infinity\n\n## @section MariaDB common parameters\n\n## Bitnami MariaDB image\n## ref: https://hub.docker.com/r/bitnami/mariadb/tags/\n## @param image.registry MariaDB image registry\n## @param image.repository MariaDB image repository\n## @param image.tag MariaDB image tag (immutable tags are recommended)\n## @param image.pullPolicy MariaDB image pull policy\n## @param image.pullSecrets Specify docker-registry secret names as an array\n## @param image.debug Specify if debug logs should be enabled\n##\nimage:\n  registry: docker.io\n  repository: bitnami/mariadb\n  tag: 10.5.13-debian-10-r32\n  ## Specify a imagePullPolicy\n  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n  ##\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## Example:\n  ## pullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  pullSecrets: []\n  ## Set to true if you would like to see extra information on logs\n  ## It turns BASH and/or NAMI debugging in the image\n  ##\n  debug: false\n## @param architecture MariaDB architecture (`standalone` or `replication`)\n##\narchitecture: standalone\n## MariaDB Authentication parameters\n##\nauth:\n  ## @param auth.rootPassword Password for the `root` user. Ignored if existing secret is provided.\n  ## ref: https://github.com/bitnami/bitnami-docker-mariadb#setting-the-root-password-on-first-run\n  ##\n  rootPassword: \"admin\"\n  ## @param auth.database Name for a custom database to create\n  ## ref: https://github.com/bitnami/bitnami-docker-mariadb/blob/master/README.md#creating-a-database-on-first-run\n  ##\n  database: my_database\n  ## @param auth.username Name for a custom user to create\n  ## ref: https://github.com/bitnami/bitnami-docker-mariadb/blob/master/README.md#creating-a-database-user-on-first-run\n  ##\n  username: \"\"\n  ## @param auth.password Password for the new user. Ignored if existing secret is provided\n  ##\n  password: \"\"\n  ## @param auth.replicationUser MariaDB replication user\n  ## ref: https://github.com/bitnami/bitnami-docker-mariadb#setting-up-a-replication-cluster\n  ##\n  replicationUser: replicator\n  ## @param auth.replicationPassword MariaDB replication user password. Ignored if existing secret is provided\n  ## ref: https://github.com/bitnami/bitnami-docker-mariadb#setting-up-a-replication-cluster\n  ##\n  replicationPassword: \"\"\n  ## @param auth.existingSecret Use existing secret for password details (`auth.rootPassword`, `auth.password`, `auth.replicationPassword` will be ignored and picked up from this secret). The secret has to contain the keys `mariadb-root-password`, `mariadb-replication-password` and `mariadb-password`\n  ##\n  existingSecret: \"\"\n  ## @param auth.forcePassword Force users to specify required passwords\n  ##\n  forcePassword: false\n  ## @param auth.usePasswordFiles Mount credentials as a files instead of using an environment variable\n  ##\n  usePasswordFiles: false\n  ## @param auth.customPasswordFiles Use custom password files when `auth.usePasswordFiles` is set to `true`. Define path for keys `root` and `user`, also define `replicator` if `architecture` is set to `replication`\n  ## Example:\n  ## customPasswordFiles:\n  ##   root: /vault/secrets/mariadb-root\n  ##   user: /vault/secrets/mariadb-user\n  ##   replicator: /vault/secrets/mariadb-replicator\n  ##\n  customPasswordFiles: {}\n## @param initdbScripts Dictionary of initdb scripts\n## Specify dictionary of scripts to be run at first boot\n## Example:\n## initdbScripts:\n##   my_init_script.sh: |\n##      #!/bin/bash\n##      echo \"Do something.\"\n##\ninitdbScripts: {}\n## @param initdbScriptsConfigMap ConfigMap with the initdb scripts (Note: Overrides `initdbScripts`)\n##\ninitdbScriptsConfigMap: \"\"\n\n## @section MariaDB Primary parameters\n\n## Mariadb Primary parameters\n##\nprimary:\n  ## @param primary.command Override default container command on MariaDB Primary container(s) (useful when using custom images)\n  ##\n  command: []\n  ## @param primary.args Override default container args on MariaDB Primary container(s) (useful when using custom images)\n  ##\n  args: []\n  ## @param primary.lifecycleHooks for the MariaDB Primary container(s) to automate configuration before or after startup\n  ##\n  lifecycleHooks: {}\n  ## @param primary.hostAliases Add deployment host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ##\n  hostAliases: []\n  ## @param primary.configuration [string] MariaDB Primary configuration to be injected as ConfigMap\n  ## ref: https://mysql.com/kb/en/mysql/configuring-mysql-with-mycnf/#example-of-configuration-file\n  ##\n  configuration: |-\n    [mysqld]\n    skip-name-resolve\n    explicit_defaults_for_timestamp\n    basedir=/opt/bitnami/mariadb\n    plugin_dir=/opt/bitnami/mariadb/plugin\n    port=3306\n    socket=/opt/bitnami/mariadb/tmp/mysql.sock\n    tmpdir=/opt/bitnami/mariadb/tmp\n    max_allowed_packet=16M\n    bind-address=::\n    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid\n    log-error=/opt/bitnami/mariadb/logs/mysqld.log\n    character-set-server=UTF8\n    collation-server=utf8_general_ci\n\n    [client]\n    port=3306\n    socket=/opt/bitnami/mariadb/tmp/mysql.sock\n    default-character-set=UTF8\n    plugin_dir=/opt/bitnami/mariadb/plugin\n\n    [manager]\n    port=3306\n    socket=/opt/bitnami/mariadb/tmp/mysql.sock\n    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid\n  ## @param primary.existingConfigmap Name of existing ConfigMap with MariaDB Primary configuration.\n  ## NOTE: When it's set the 'configuration' parameter is ignored\n  ##\n  existingConfigmap: \"\"\n  ## @param primary.updateStrategy.type MariaDB primary statefulset strategy type\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n  ##\n  updateStrategy:\n    ## StrategyType\n    ## Can be set to RollingUpdate or OnDelete\n    ##\n    type: RollingUpdate\n  ## @param primary.rollingUpdatePartition Partition update strategy for Mariadb Primary statefulset\n  ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions\n  ##\n  rollingUpdatePartition: \"\"\n  ## @param primary.podAnnotations Additional pod annotations for MariaDB primary pods\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n  ##\n  podAnnotations: {}\n  ## @param primary.podLabels Extra labels for MariaDB primary pods\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n  ##\n  podLabels: {}\n  ## @param primary.podAffinityPreset MariaDB primary pod affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAffinityPreset: \"\"\n  ## @param primary.podAntiAffinityPreset MariaDB primary pod anti-affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAntiAffinityPreset: soft\n  ## Mariadb Primary node affinity preset\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n  ##\n  nodeAffinityPreset:\n    ## @param primary.nodeAffinityPreset.type MariaDB primary node affinity preset type. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`\n    ##\n    type: \"\"\n    ## @param primary.nodeAffinityPreset.key MariaDB primary node label key to match Ignored if `primary.affinity` is set.\n    ## E.g.\n    ## key: \"kubernetes.io/e2e-az-name\"\n    ##\n    key: \"\"\n    ## @param primary.nodeAffinityPreset.values MariaDB primary node label values to match. Ignored if `primary.affinity` is set.\n    ## E.g.\n    ## values:\n    ##   - e2e-az1\n    ##   - e2e-az2\n    ##\n    values: []\n  ## @param primary.affinity Affinity for MariaDB primary pods assignment\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set\n  ##\n  affinity: {}\n  ## @param primary.nodeSelector Node labels for MariaDB primary pods assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param primary.tolerations Tolerations for MariaDB primary pods assignment\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## @param primary.schedulerName Name of the k8s scheduler (other than default)\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  schedulerName: \"\"\n  ## @param primary.podManagementPolicy podManagementPolicy to manage scaling operation of MariaDB primary pods\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies\n  ##\n  podManagementPolicy: \"\"\n  ## @param primary.topologySpreadConstraints Topology Spread Constraints for MariaDB primary pods assignment\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## E.g.\n  ## topologySpreadConstraints:\n  ##   - maxSkew: 1\n  ##     topologyKey: topology.kubernetes.io/zone\n  ##     whenUnsatisfiable: DoNotSchedule\n  ##\n  topologySpreadConstraints: {}\n  ## @param primary.priorityClassName Priority class for MariaDB primary pods assignment\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n  ##\n  priorityClassName: \"\"\n  ## MariaDB primary Pod security context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n  ## @param primary.podSecurityContext.enabled Enable security context for MariaDB primary pods\n  ## @param primary.podSecurityContext.fsGroup Group ID for the mounted volumes' filesystem\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1001\n  ## MariaDB primary container security context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param primary.containerSecurityContext.enabled MariaDB primary container securityContext\n  ## @param primary.containerSecurityContext.runAsUser User ID for the MariaDB primary container\n  ## @param primary.containerSecurityContext.runAsNonRoot Set Controller container's Security Context runAsNonRoot\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1001\n    runAsNonRoot: true\n  ## MariaDB primary container's resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param primary.resources.limits The resources limits for MariaDB primary containers\n  ## @param primary.resources.requests The requested resources for MariaDB primary containers\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 100m\n    ##    memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##    cpu: 100m\n    ##    memory: 256Mi\n    requests: {}\n  ## Configure extra options for MariaDB primary containers' liveness, readiness and startup probes\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)\n  ## @param primary.startupProbe.enabled Enable startupProbe\n  ## @param primary.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n  ## @param primary.startupProbe.periodSeconds Period seconds for startupProbe\n  ## @param primary.startupProbe.timeoutSeconds Timeout seconds for startupProbe\n  ## @param primary.startupProbe.failureThreshold Failure threshold for startupProbe\n  ## @param primary.startupProbe.successThreshold Success threshold for startupProbe\n  ##\n  startupProbe:\n    enabled: false\n    initialDelaySeconds: 120\n    periodSeconds: 15\n    timeoutSeconds: 5\n    failureThreshold: 10\n    successThreshold: 1\n  ## Configure extra options for liveness probe\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param primary.livenessProbe.enabled Enable livenessProbe\n  ## @param primary.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param primary.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param primary.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param primary.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param primary.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 120\n    periodSeconds: 10\n    timeoutSeconds: 1\n    failureThreshold: 3\n    successThreshold: 1\n  ## @param primary.readinessProbe.enabled Enable readinessProbe\n  ## @param primary.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param primary.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param primary.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param primary.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param primary.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 1\n    failureThreshold: 3\n    successThreshold: 1\n  ## @param primary.customStartupProbe Override default startup probe for MariaDB primary containers\n  ##\n  customStartupProbe: {}\n  ## @param primary.customLivenessProbe Override default liveness probe for MariaDB primary containers\n  ##\n  customLivenessProbe: {}\n  ## @param primary.customReadinessProbe Override default readiness probe for MariaDB primary containers\n  ##\n  customReadinessProbe: {}\n  ## @param primary.startupWaitOptions Override default builtin startup wait check options for MariaDB primary containers\n  ## `bitnami/mariadb` Docker image has built-in startup check mechanism,\n  ## which periodically checks if MariaDB service has started up and stops it\n  ## if all checks have failed after X tries. Use these to control these checks.\n  ## ref: https://github.com/bitnami/bitnami-docker-mariadb/pull/240\n  ## Example (with default options):\n  ## startupWaitOptions:\n  ##   retries: 300\n  ##   waitTime: 2\n  ##\n  startupWaitOptions: {}\n  ## @param primary.extraFlags MariaDB primary additional command line flags\n  ## Can be used to specify command line flags, for example:\n  ## E.g.\n  ## extraFlags: \"--max-connect-errors=1000 --max_connections=155\"\n  ##\n  extraFlags: \"\"\n  ## @param primary.extraEnvVars Extra environment variables to be set on MariaDB primary containers\n  ## E.g.\n  ## extraEnvVars:\n  ##  - name: TZ\n  ##    value: \"Europe/Paris\"\n  ##\n  extraEnvVars: []\n  ## @param primary.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for MariaDB primary containers\n  ##\n  extraEnvVarsCM: \"\"\n  ## @param primary.extraEnvVarsSecret Name of existing Secret containing extra env vars for MariaDB primary containers\n  ##\n  extraEnvVarsSecret: \"\"\n  ## Enable persistence using Persistent Volume Claims\n  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/\n  ##\n  persistence:\n    ## @param primary.persistence.enabled Enable persistence on MariaDB primary replicas using a `PersistentVolumeClaim`. If false, use emptyDir\n    ##\n    enabled: true\n    ## @param primary.persistence.existingClaim Name of an existing `PersistentVolumeClaim` for MariaDB primary replicas\n    ## NOTE: When it's set the rest of persistence parameters are ignored\n    ##\n    existingClaim: \"\"\n    ## @param primary.persistence.subPath Subdirectory of the volume to mount at\n    ##\n    subPath: \"\"\n    ## @param primary.persistence.storageClass MariaDB primary persistent volume storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    storageClass: \"\"\n    ## @param primary.persistence.annotations MariaDB primary persistent volume claim annotations\n    ##\n    annotations: {}\n    ## @param primary.persistence.accessModes MariaDB primary persistent volume access Modes\n    ##\n    accessModes:\n      - ReadWriteOnce\n    ## @param primary.persistence.size MariaDB primary persistent volume size\n    ##\n    size: 8Gi\n    ## @param primary.persistence.selector Selector to match an existing Persistent Volume\n    ## selector:\n    ##   matchLabels:\n    ##     app: my-app\n    ##\n    selector: {}\n  ## @param primary.extraVolumes Optionally specify extra list of additional volumes to the MariaDB Primary pod(s)\n  ##\n  extraVolumes: []\n  ## @param primary.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the MariaDB Primary container(s)\n  ##\n  extraVolumeMounts: []\n  ## @param primary.initContainers Add additional init containers for the MariaDB Primary pod(s)\n  ##\n  initContainers: []\n  ## @param primary.sidecars Add additional sidecar containers for the MariaDB Primary pod(s)\n  ##\n  sidecars: []\n  ## MariaDB Primary Service parameters\n  ##\n  service:\n    ## @param primary.service.type MariaDB Primary Kubernetes service type\n    ##\n    type: ClusterIP\n    ## @param primary.service.ports.mysql MariaDB Primary Kubernetes service port\n    ##\n    ports:\n      mysql: 3306\n    ## @param primary.service.nodePorts.mysql MariaDB Primary Kubernetes service node port\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n    ##\n    nodePorts:\n      mysql: \"\"\n    ## @param primary.service.clusterIP MariaDB Primary Kubernetes service clusterIP IP\n    ##\n    clusterIP: \"\"\n    ## @param primary.service.loadBalancerIP MariaDB Primary loadBalancerIP if service type is `LoadBalancer`\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n    ##\n    loadBalancerIP: \"\"\n    ## @param primary.service.externalTrafficPolicy Enable client source IP preservation\n    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n    ##\n    externalTrafficPolicy: Cluster\n    ## @param primary.service.loadBalancerSourceRanges Address that are allowed when MariaDB Primary service is LoadBalancer\n    ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n    ## E.g.\n    ## loadBalancerSourceRanges:\n    ##   - 10.10.10.0/24\n    ##\n    loadBalancerSourceRanges: []\n    ## @param primary.service.extraPorts Extra ports to expose (normally used with the `sidecar` value)\n    ##\n    extraPorts: []\n    ## @param primary.service.annotations Provide any additional annotations which may be required\n    ##\n    annotations: {}\n    ## @param primary.service.sessionAffinity Session Affinity for Kubernetes service, can be \"None\" or \"ClientIP\"\n    ## If \"ClientIP\", consecutive client requests will be directed to the same Pod\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies\n    ##\n    sessionAffinity: None\n    ## @param primary.service.sessionAffinityConfig Additional settings for the sessionAffinity\n    ## sessionAffinityConfig:\n    ##   clientIP:\n    ##     timeoutSeconds: 300\n    sessionAffinityConfig: {}\n  ## MariaDB primary Pod Disruption Budget configuration\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  ##\n  pdb:\n    ## @param primary.pdb.create Enable/disable a Pod Disruption Budget creation for MariaDB primary pods\n    ##\n    create: false\n    ## @param primary.pdb.minAvailable Minimum number/percentage of MariaDB primary pods that must still be available after the eviction\n    ##\n    minAvailable: 1\n    ## @param primary.pdb.maxUnavailable Maximum number/percentage of MariaDB primary pods that can be unavailable after the eviction\n    ##\n    maxUnavailable: \"\"\n  ## @param primary.revisionHistoryLimit Maximum number of revisions that will be maintained in the StatefulSet\n  ##\n  revisionHistoryLimit: 10\n\n## @section MariaDB Secondary parameters\n\n## Mariadb Secondary parameters\n##\nsecondary:\n  ## @param secondary.replicaCount Number of MariaDB secondary replicas\n  ##\n  replicaCount: 1\n  ## @param secondary.command Override default container command on MariaDB Secondary container(s) (useful when using custom images)\n  ##\n  command: []\n  ## @param secondary.args Override default container args on MariaDB Secondary container(s) (useful when using custom images)\n  ##\n  args: []\n  ## @param secondary.lifecycleHooks for the MariaDB Secondary container(s) to automate configuration before or after startup\n  ##\n  lifecycleHooks: {}\n  ## @param secondary.hostAliases Add deployment host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ##\n  hostAliases: []\n  ## @param secondary.configuration [string] MariaDB Secondary configuration to be injected as ConfigMap\n  ## ref: https://mysql.com/kb/en/mysql/configuring-mysql-with-mycnf/#example-of-configuration-file\n  ##\n  configuration: |-\n    [mysqld]\n    skip-name-resolve\n    explicit_defaults_for_timestamp\n    basedir=/opt/bitnami/mariadb\n    port=3306\n    socket=/opt/bitnami/mariadb/tmp/mysql.sock\n    tmpdir=/opt/bitnami/mariadb/tmp\n    max_allowed_packet=16M\n    bind-address=0.0.0.0\n    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid\n    log-error=/opt/bitnami/mariadb/logs/mysqld.log\n    character-set-server=UTF8\n    collation-server=utf8_general_ci\n\n    [client]\n    port=3306\n    socket=/opt/bitnami/mariadb/tmp/mysql.sock\n    default-character-set=UTF8\n\n    [manager]\n    port=3306\n    socket=/opt/bitnami/mariadb/tmp/mysql.sock\n    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid\n  ## @param secondary.existingConfigmap Name of existing ConfigMap with MariaDB Secondary configuration.\n  ## NOTE: When it's set the 'configuration' parameter is ignored\n  ##\n  existingConfigmap: \"\"\n  ## @param secondary.updateStrategy.type MariaDB secondary statefulset strategy type\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n  ##\n  updateStrategy:\n    ## StrategyType\n    ## Can be set to RollingUpdate or OnDelete\n    ##\n    type: RollingUpdate\n  ## @param secondary.rollingUpdatePartition Partition update strategy for Mariadb Secondary statefulset\n  ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions\n  ##\n  rollingUpdatePartition: \"\"\n  ## @param secondary.podAnnotations Additional pod annotations for MariaDB secondary pods\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n  ##\n  podAnnotations: {}\n  ## @param secondary.podLabels Extra labels for MariaDB secondary pods\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n  ##\n  podLabels: {}\n  ## @param secondary.podAffinityPreset MariaDB secondary pod affinity preset. Ignored if `secondary.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAffinityPreset: \"\"\n  ## @param secondary.podAntiAffinityPreset MariaDB secondary pod anti-affinity preset. Ignored if `secondary.affinity` is set. Allowed values: `soft` or `hard`\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAntiAffinityPreset: soft\n  ## Mariadb Secondary node affinity preset\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n  ##\n  nodeAffinityPreset:\n    ## @param secondary.nodeAffinityPreset.type MariaDB secondary node affinity preset type. Ignored if `secondary.affinity` is set. Allowed values: `soft` or `hard`\n    ##\n    type: \"\"\n    ## @param secondary.nodeAffinityPreset.key MariaDB secondary node label key to match Ignored if `secondary.affinity` is set.\n    ## E.g.\n    ## key: \"kubernetes.io/e2e-az-name\"\n    ##\n    key: \"\"\n    ## @param secondary.nodeAffinityPreset.values MariaDB secondary node label values to match. Ignored if `secondary.affinity` is set.\n    ## E.g.\n    ## values:\n    ##   - e2e-az1\n    ##   - e2e-az2\n    ##\n    values: []\n  ## @param secondary.affinity Affinity for MariaDB secondary pods assignment\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set\n  ##\n  affinity: {}\n  ## @param secondary.nodeSelector Node labels for MariaDB secondary pods assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param secondary.tolerations Tolerations for MariaDB secondary pods assignment\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## @param secondary.topologySpreadConstraints Topology Spread Constraints for MariaDB secondary pods assignment\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## E.g.\n  ## topologySpreadConstraints:\n  ##   - maxSkew: 1\n  ##     topologyKey: topology.kubernetes.io/zone\n  ##     whenUnsatisfiable: DoNotSchedule\n  ##\n  topologySpreadConstraints: {}\n  ## @param secondary.priorityClassName Priority class for MariaDB secondary pods assignment\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n  ##\n  priorityClassName: \"\"\n  ## @param secondary.schedulerName Name of the k8s scheduler (other than default)\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  schedulerName: \"\"\n  ## @param secondary.podManagementPolicy podManagementPolicy to manage scaling operation of MariaDB secondary pods\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies\n  ##\n  podManagementPolicy: \"\"\n  ## MariaDB secondary Pod security context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n  ## @param secondary.podSecurityContext.enabled Enable security context for MariaDB secondary pods\n  ## @param secondary.podSecurityContext.fsGroup Group ID for the mounted volumes' filesystem\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1001\n  ## MariaDB secondary container security context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param secondary.containerSecurityContext.enabled MariaDB secondary container securityContext\n  ## @param secondary.containerSecurityContext.runAsUser User ID for the MariaDB secondary container\n  ## @param secondary.containerSecurityContext.runAsNonRoot Set Controller container's Security Context runAsNonRoot\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1001\n    runAsNonRoot: true\n  ## MariaDB secondary container's resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param secondary.resources.limits The resources limits for MariaDB secondary containers\n  ## @param secondary.resources.requests The requested resources for MariaDB secondary containers\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 100m\n    ##    memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##    cpu: 100m\n    ##    memory: 256Mi\n    requests: {}\n  ## Configure extra options for MariaDB Secondary containers' liveness, readiness and startup probes\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)\n  ## @param secondary.startupProbe.enabled Enable startupProbe\n  ## @param secondary.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n  ## @param secondary.startupProbe.periodSeconds Period seconds for startupProbe\n  ## @param secondary.startupProbe.timeoutSeconds Timeout seconds for startupProbe\n  ## @param secondary.startupProbe.failureThreshold Failure threshold for startupProbe\n  ## @param secondary.startupProbe.successThreshold Success threshold for startupProbe\n  ##\n  startupProbe:\n    enabled: false\n    initialDelaySeconds: 120\n    periodSeconds: 15\n    timeoutSeconds: 5\n    failureThreshold: 10\n    successThreshold: 1\n  ## Configure extra options for liveness probe\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param secondary.livenessProbe.enabled Enable livenessProbe\n  ## @param secondary.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param secondary.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param secondary.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param secondary.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param secondary.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 120\n    periodSeconds: 10\n    timeoutSeconds: 1\n    failureThreshold: 3\n    successThreshold: 1\n  ## @param secondary.readinessProbe.enabled Enable readinessProbe\n  ## @param secondary.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param secondary.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param secondary.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param secondary.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param secondary.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 1\n    failureThreshold: 3\n    successThreshold: 1\n  ## @param secondary.customStartupProbe Override default startup probe for MariaDB secondary containers\n  ##\n  customStartupProbe: {}\n  ## @param secondary.customLivenessProbe Override default liveness probe for MariaDB secondary containers\n  ##\n  customLivenessProbe: {}\n  ## @param secondary.customReadinessProbe Override default readiness probe for MariaDB secondary containers\n  ##\n  customReadinessProbe: {}\n  ## @param secondary.startupWaitOptions Override default builtin startup wait check options for MariaDB secondary containers\n  ## `bitnami/mariadb` Docker image has built-in startup check mechanism,\n  ## which periodically checks if MariaDB service has started up and stops it\n  ## if all checks have failed after X tries. Use these to control these checks.\n  ## ref: https://github.com/bitnami/bitnami-docker-mariadb/pull/240\n  ## Example (with default options):\n  ## startupWaitOptions:\n  ##   retries: 300\n  ##   waitTime: 2\n  ##\n  startupWaitOptions: {}\n  ## @param secondary.extraFlags MariaDB secondary additional command line flags\n  ## Can be used to specify command line flags, for example:\n  ## E.g.\n  ## extraFlags: \"--max-connect-errors=1000 --max_connections=155\"\n  ##\n  extraFlags: \"\"\n  ## @param secondary.extraEnvVars Extra environment variables to be set on MariaDB secondary containers\n  ## E.g.\n  ## extraEnvVars:\n  ##  - name: TZ\n  ##    value: \"Europe/Paris\"\n  ##\n  extraEnvVars: []\n  ## @param secondary.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for MariaDB secondary containers\n  ##\n  extraEnvVarsCM: \"\"\n  ## @param secondary.extraEnvVarsSecret Name of existing Secret containing extra env vars for MariaDB secondary containers\n  ##\n  extraEnvVarsSecret: \"\"\n  ## Enable persistence using Persistent Volume Claims\n  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/\n  ##\n  persistence:\n    ## @param secondary.persistence.enabled Enable persistence on MariaDB secondary replicas using a `PersistentVolumeClaim`\n    ##\n    enabled: true\n    ## @param secondary.persistence.subPath Subdirectory of the volume to mount at\n    ##\n    subPath: \"\"\n    ## @param secondary.persistence.storageClass MariaDB secondary persistent volume storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    storageClass: \"\"\n    ## @param secondary.persistence.annotations MariaDB secondary persistent volume claim annotations\n    ##\n    annotations: {}\n    ## @param secondary.persistence.accessModes MariaDB secondary persistent volume access Modes\n    ##\n    accessModes:\n      - ReadWriteOnce\n    ## @param secondary.persistence.size MariaDB secondary persistent volume size\n    ##\n    size: 8Gi\n    ## @param secondary.persistence.selector Selector to match an existing Persistent Volume\n    ## selector:\n    ##   matchLabels:\n    ##     app: my-app\n    ##\n    selector: {}\n  ## @param secondary.extraVolumes Optionally specify extra list of additional volumes to the MariaDB secondary pod(s)\n  ##\n  extraVolumes: []\n  ## @param secondary.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the MariaDB secondary container(s)\n  ##\n  extraVolumeMounts: []\n  ## @param secondary.initContainers Add additional init containers for the MariaDB secondary pod(s)\n  ##\n  initContainers: []\n  ## @param secondary.sidecars Add additional sidecar containers for the MariaDB secondary pod(s)\n  ##\n  sidecars: []\n  ## MariaDB Secondary Service parameters\n  ##\n  service:\n    ## @param secondary.service.type MariaDB secondary Kubernetes service type\n    ##\n    type: ClusterIP\n    ## @param secondary.service.ports.mysql MariaDB secondary Kubernetes service port\n    ##\n    ports:\n      mysql: 3306\n    ## @param secondary.service.nodePorts.mysql MariaDB secondary Kubernetes service node port\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n    ##\n    nodePorts:\n      mysql: \"\"\n    ## @param secondary.service.clusterIP MariaDB secondary Kubernetes service clusterIP IP\n    ## e.g:\n    ## clusterIP: None\n    ##\n    clusterIP: \"\"\n    ## @param secondary.service.loadBalancerIP MariaDB secondary loadBalancerIP if service type is `LoadBalancer`\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n    ##\n    loadBalancerIP: \"\"\n    ## @param secondary.service.externalTrafficPolicy Enable client source IP preservation\n    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n    ##\n    externalTrafficPolicy: Cluster\n    ## @param secondary.service.loadBalancerSourceRanges Address that are allowed when MariaDB secondary service is LoadBalancer\n    ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n    ## E.g.\n    ## loadBalancerSourceRanges:\n    ##   - 10.10.10.0/24\n    ##\n    loadBalancerSourceRanges: []\n    ## @param secondary.service.extraPorts Extra ports to expose (normally used with the `sidecar` value)\n    ##\n    extraPorts: []\n    ## @param secondary.service.annotations Provide any additional annotations which may be required\n    ##\n    annotations: {}\n    ## @param secondary.service.sessionAffinity Session Affinity for Kubernetes service, can be \"None\" or \"ClientIP\"\n    ## If \"ClientIP\", consecutive client requests will be directed to the same Pod\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies\n    ##\n    sessionAffinity: None\n    ## @param secondary.service.sessionAffinityConfig Additional settings for the sessionAffinity\n    ## sessionAffinityConfig:\n    ##   clientIP:\n    ##     timeoutSeconds: 300\n    sessionAffinityConfig: {}\n  ## MariaDB secondary Pod Disruption Budget configuration\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  ##\n  pdb:\n    ## @param secondary.pdb.create Enable/disable a Pod Disruption Budget creation for MariaDB secondary pods\n    ##\n    create: false\n    ## @param secondary.pdb.minAvailable Minimum number/percentage of MariaDB secondary pods that should remain scheduled\n    ##\n    minAvailable: 1\n    ## @param secondary.pdb.maxUnavailable Maximum number/percentage of MariaDB secondary pods that may be made unavailable\n    ##\n    maxUnavailable: \"\"\n  ## @param secondary.revisionHistoryLimit Maximum number of revisions that will be maintained in the StatefulSet\n  ##\n  revisionHistoryLimit: 10\n\n## @section RBAC parameters\n\n## MariaDB pods ServiceAccount\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n##\nserviceAccount:\n  ## @param serviceAccount.create Enable the creation of a ServiceAccount for MariaDB pods\n  ##\n  create: true\n  ## @param serviceAccount.name Name of the created ServiceAccount\n  ## If not set and create is true, a name is generated using the mariadb.fullname template\n  ##\n  name: \"\"\n  ## @param serviceAccount.annotations Annotations for MariaDB Service Account\n  ##\n  annotations: {}\n  ## @param serviceAccount.automountServiceAccountToken Automount service account token for the server service account\n  ##\n  automountServiceAccountToken: false\n## Role Based Access\n## ref: https://kubernetes.io/docs/admin/authorization/rbac/\n##\nrbac:\n  ## @param rbac.create Whether to create and use RBAC resources or not\n  ##\n  create: false\n\n## @section Volume Permissions parameters\n\n## Init containers parameters:\n## volumePermissions: Change the owner and group of the persistent volume mountpoint to runAsUser:fsGroup values from the securityContext section.\n##\nvolumePermissions:\n  ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to `runAsUser:fsGroup`\n  ##\n  enabled: false\n  ## @param volumePermissions.image.registry Init container volume-permissions image registry\n  ## @param volumePermissions.image.repository Init container volume-permissions image repository\n  ## @param volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)\n  ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy\n  ## @param volumePermissions.image.pullSecrets Specify docker-registry secret names as an array\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/bitnami-shell\n    tag: 10-debian-10-r279\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## Example:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## @param volumePermissions.resources.limits Init container volume-permissions resource limits\n  ## @param volumePermissions.resources.requests Init container volume-permissions resource requests\n  ##\n  resources:\n    limits: {}\n    requests: {}\n\n## @section Metrics parameters\n\n## Mysqld Prometheus exporter parameters\n##\nmetrics:\n  ## @param metrics.enabled Start a side-car prometheus exporter\n  ##\n  enabled: false\n  ## @param metrics.image.registry Exporter image registry\n  ## @param metrics.image.repository Exporter image repository\n  ## @param metrics.image.tag Exporter image tag (immutable tags are recommended)\n  ## @param metrics.image.pullPolicy Exporter image pull policy\n  ## @param metrics.image.pullSecrets Specify docker-registry secret names as an array\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/mysqld-exporter\n    tag: 0.13.0-debian-10-r183\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## Example:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## @param metrics.annotations [object] Annotations for the Exporter pod\n  ##\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9104\"\n  ## @param metrics.extraArgs [object] Extra args to be passed to mysqld_exporter\n  ## ref: https://github.com/prometheus/mysqld_exporter/\n  ## E.g.\n  ## - --collect.auto_increment.columns\n  ## - --collect.binlog_size\n  ## - --collect.engine_innodb_status\n  ## - --collect.engine_tokudb_status\n  ## - --collect.global_status\n  ## - --collect.global_variables\n  ## - --collect.info_schema.clientstats\n  ## - --collect.info_schema.innodb_metrics\n  ## - --collect.info_schema.innodb_tablespaces\n  ## - --collect.info_schema.innodb_cmp\n  ## - --collect.info_schema.innodb_cmpmem\n  ## - --collect.info_schema.processlist\n  ## - --collect.info_schema.processlist.min_time\n  ## - --collect.info_schema.query_response_time\n  ## - --collect.info_schema.tables\n  ## - --collect.info_schema.tables.databases\n  ## - --collect.info_schema.tablestats\n  ## - --collect.info_schema.userstats\n  ## - --collect.perf_schema.eventsstatements\n  ## - --collect.perf_schema.eventsstatements.digest_text_limit\n  ## - --collect.perf_schema.eventsstatements.limit\n  ## - --collect.perf_schema.eventsstatements.timelimit\n  ## - --collect.perf_schema.eventswaits\n  ## - --collect.perf_schema.file_events\n  ## - --collect.perf_schema.file_instances\n  ## - --collect.perf_schema.indexiowaits\n  ## - --collect.perf_schema.tableiowaits\n  ## - --collect.perf_schema.tablelocks\n  ## - --collect.perf_schema.replication_group_member_stats\n  ## - --collect.slave_status\n  ## - --collect.slave_hosts\n  ## - --collect.heartbeat\n  ## - --collect.heartbeat.database\n  ## - --collect.heartbeat.table\n  ##\n  extraArgs:\n    primary: []\n    secondary: []\n  ## MariaDB metrics container Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param metrics.containerSecurityContext.enabled Enable security context for MariaDB metrics container\n  ## Example:\n  ##   containerSecurityContext:\n  ##     enabled: true\n  ##     capabilities:\n  ##       drop: [\"NET_RAW\"]\n  ##     readOnlyRootFilesystem: true\n  ##\n  containerSecurityContext:\n    enabled: false\n  ## Mysqld Prometheus exporter resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param metrics.resources.limits The resources limits for MariaDB prometheus exporter containers\n  ## @param metrics.resources.requests The requested resources for MariaDB prometheus exporter containers\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 100m\n    ##    memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##    cpu: 100m\n    ##    memory: 256Mi\n    requests: {}\n  ## Configure extra options for liveness probe\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param metrics.livenessProbe.enabled Enable livenessProbe\n  ## @param metrics.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param metrics.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param metrics.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param metrics.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param metrics.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 120\n    periodSeconds: 10\n    timeoutSeconds: 1\n    successThreshold: 1\n    failureThreshold: 3\n  ## Configure extra options for readiness probe\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param metrics.readinessProbe.enabled Enable readinessProbe\n  ## @param metrics.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param metrics.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param metrics.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param metrics.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param metrics.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 1\n    successThreshold: 1\n    failureThreshold: 3\n  ## Prometheus Service Monitor\n  ## ref: https://github.com/coreos/prometheus-operator\n  ##\n  serviceMonitor:\n    ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using PrometheusOperator\n    ##\n    enabled: false\n    ## @param metrics.serviceMonitor.namespace Namespace which Prometheus is running in\n    ##\n    namespace: \"\"\n    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.\n    ##\n    jobLabel: \"\"\n    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped\n    ##\n    interval: 30s\n    ## @param metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended\n    ## e.g:\n    ## scrapeTimeout: 30s\n    ##\n    scrapeTimeout: \"\"\n    ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    ## @param metrics.serviceMonitor.honorLabels honorLabels chooses the metric's labels on collisions with target labels\n    ##\n    honorLabels: false\n    ## @param metrics.serviceMonitor.selector ServiceMonitor selector labels\n    ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration\n    ##\n    ## selector:\n    ##   prometheus: my-prometheus\n    ##\n    selector: {}\n    ## @param metrics.serviceMonitor.labels Extra labels for the ServiceMonitor\n    ##\n    labels: {}\n\n## @section NetworkPolicy parameters\n\n## Add networkpolicies\n##\nnetworkPolicy:\n  ## @param networkPolicy.enabled Enable network policies\n  ##\n  enabled: false\n  ## @param networkPolicy.metrics.enabled Enable network policy for metrics (prometheus)\n  ## @param networkPolicy.metrics.namespaceSelector [object] Monitoring namespace selector labels. These labels will be used to identify the prometheus' namespace.\n  ## @param networkPolicy.metrics.podSelector [object] Monitoring pod selector labels. These labels will be used to identify the Prometheus pods.\n  ##\n  metrics:\n    enabled: false\n    ## e.g:\n    ## podSelector:\n    ##   label: monitoring\n    ##\n    podSelector: {}\n    ## e.g:\n    ## namespaceSelector:\n    ##   label: monitoring\n    ##\n    namespaceSelector: {}\n  ## @param networkPolicy.ingressRules.primaryAccessOnlyFrom.enabled Enable ingress rule that makes primary mariadb nodes only accessible from a particular origin.\n  ## @param networkPolicy.ingressRules.primaryAccessOnlyFrom.namespaceSelector [object] Namespace selector label that is allowed to access the primary node. This label will be used to identified the allowed namespace(s).\n  ## @param networkPolicy.ingressRules.primaryAccessOnlyFrom.podSelector [object] Pods selector label that is allowed to access the primary node. This label will be used to identified the allowed pod(s).\n  ## @param networkPolicy.ingressRules.primaryAccessOnlyFrom.customRules [object] Custom network policy for the primary node.\n  ## @param networkPolicy.ingressRules.secondaryAccessOnlyFrom.enabled Enable ingress rule that makes primary mariadb nodes only accessible from a particular origin.\n  ## @param networkPolicy.ingressRules.secondaryAccessOnlyFrom.namespaceSelector [object] Namespace selector label that is allowed to acces the secondary nodes. This label will be used to identified the allowed namespace(s).\n  ## @param networkPolicy.ingressRules.secondaryAccessOnlyFrom.podSelector [object] Pods selector label that is allowed to access the secondary nodes. This label will be used to identified the allowed pod(s).\n  ## @param networkPolicy.ingressRules.secondaryAccessOnlyFrom.customRules [object] Custom network policy for the secondary nodes.\n  ##\n  ingressRules:\n    ## Allow access to the primary node only from the indicated:\n    primaryAccessOnlyFrom:\n      enabled: false\n      ## e.g:\n      ## namespaceSelector:\n      ##   label: ingress\n      ##\n      namespaceSelector: {}\n      ## e.g:\n      ## podSelector:\n      ##   label: access\n      ##\n      podSelector: {}\n      ## custom ingress rules\n      ## e.g:\n      ## customRules:\n      ##   - from:\n      ##       - namespaceSelector:\n      ##           matchLabels:\n      ##             label: example\n      customRules: {}\n\n    ## Allow access to the secondary node only from the indicated:\n    secondaryAccessOnlyFrom:\n      enabled: false\n      ## e.g:\n      ## namespaceSelector:\n      ##   label: ingress\n      ##\n      namespaceSelector: {}\n      ## e.g:\n      ## podSelector:\n      ##   label: access\n      ##\n      podSelector: {}\n      ## custom ingress rules\n      ## e.g:\n      ## CustomRules:\n      ##   - from:\n      ##       - namespaceSelector:\n      ##           matchLabels:\n      ##             label: example\n      customRules: {}\n\n  ## @param networkPolicy.egressRules.denyConnectionsToExternal Enable egress rule that denies outgoing traffic outside the cluster, except for DNS (port 53).\n  ## @param networkPolicy.egressRules.customRules [object] Custom network policy rule\n  ##\n  egressRules:\n    # Deny connections to external. This is not compatible with an external database.\n    denyConnectionsToExternal: false\n    ## Additional custom egress rules\n    ## e.g:\n    ## customRules:\n    ##   - to:\n    ##       - namespaceSelector:\n    ##           matchLabels:\n    ##             label: example\n    customRules: {}\n"
            ],
            "verify": false,
            "version": "10.1.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.kube.kubernetes_namespace.spark",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.minio",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mlflow",
            "module.istio_install.helm_release.istio_base",
            "module.istio_install.helm_release.istio_ingress",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.argo-events"
          ]
        }
      ]
    },
    {
      "module": "module.minio_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "minio",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "minio",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "minio",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2021.12.27",
                "chart": "minio",
                "name": "minio",
                "namespace": "minio",
                "revision": 1,
                "values": "{\"affinity\":{},\"apiIngress\":{\"annotations\":{},\"apiVersion\":\"\",\"enabled\":false,\"extraHosts\":[],\"extraPaths\":[],\"extraTls\":[],\"hostname\":\"minio.local\",\"ingressClassName\":\"\",\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"secrets\":[],\"selfSigned\":false,\"servicePort\":\"minio-api\",\"tls\":false},\"args\":[],\"auth\":{\"existingSecret\":\"\",\"forceNewKeys\":false,\"forcePassword\":false,\"rootPassword\":\"minioadmin\",\"rootUser\":\"minioadmin\",\"useCredentialsFiles\":false},\"clientImage\":{\"registry\":\"docker.io\",\"repository\":\"bitnami/minio-client\",\"tag\":\"2021.12.20-debian-10-r6\"},\"clusterDomain\":\"cluster.local\",\"command\":[],\"commonAnnotations\":{},\"commonLabels\":{},\"containerPorts\":{\"api\":9000,\"console\":9001},\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"defaultBuckets\":\"\",\"deployment\":{\"updateStrategy\":{\"type\":\"Recreate\"}},\"disableWebUI\":false,\"extraDeploy\":[],\"extraEnvVars\":{},\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"gateway\":{\"auth\":{\"azure\":{\"accessKey\":\"\",\"secretKey\":\"\",\"serviceEndpoint\":\"\",\"storageAccountKey\":\"\",\"storageAccountKeyExistingSecret\":\"\",\"storageAccountKeyExistingSecretKey\":\"\",\"storageAccountName\":\"\",\"storageAccountNameExistingSecret\":\"\",\"storageAccountNameExistingSecretKey\":\"\"},\"gcs\":{\"accessKey\":\"\",\"keyJSON\":\"\",\"projectID\":\"\",\"secretKey\":\"\"},\"nas\":{\"accessKey\":\"\",\"secretKey\":\"\"},\"s3\":{\"accessKey\":\"\",\"secretKey\":\"\",\"serviceEndpoint\":\"https://s3.amazonaws.com\"}},\"autoscaling\":{\"enabled\":false,\"maxReplicas\":\"4\",\"minReplicas\":\"4\",\"targetCPU\":\"\",\"targetMemory\":\"\"},\"enabled\":false,\"priorityClassName\":\"\",\"replicaCount\":4,\"type\":\"s3\",\"updateStrategy\":{\"type\":\"Recreate\"}},\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"storageClass\":\"\"},\"hostAliases\":[],\"image\":{\"debug\":false,\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/minio\",\"tag\":\"2021.12.27-debian-10-r0\"},\"ingress\":{\"annotations\":{},\"apiVersion\":\"\",\"enabled\":false,\"extraHosts\":[],\"extraPaths\":[],\"extraTls\":[],\"hostname\":\"minio.local\",\"ingressClassName\":\"\",\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"secrets\":[],\"selfSigned\":false,\"servicePort\":\"minio-console\",\"tls\":false},\"initContainers\":[],\"kubeVersion\":\"\",\"lifecycleHooks\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":5,\"initialDelaySeconds\":5,\"periodSeconds\":5,\"successThreshold\":1,\"timeoutSeconds\":5},\"metrics\":{\"prometheusAuthType\":\"public\",\"serviceMonitor\":{\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"jobLabel\":\"\",\"labels\":{},\"metricRelabelings\":[],\"namespace\":\"\",\"path\":\"/minio/v2/metrics/cluster\",\"relabelings\":[],\"scrapeTimeout\":\"\",\"selector\":{}}},\"mode\":\"distributed\",\"nameOverride\":\"\",\"networkPolicy\":{\"allowExternal\":true,\"enabled\":false,\"extraFromClauses\":{}},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"pdb\":{\"create\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":true,\"existingClaim\":\"\",\"mountPath\":\"/data\",\"size\":\"8Gi\",\"storageClass\":\"\"},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"priorityClassName\":\"\",\"provisioning\":{\"args\":[],\"buckets\":[],\"command\":[],\"config\":[],\"enabled\":false,\"extraVolumeMounts\":[],\"extraVolumes\":[],\"groups\":[],\"podAnnotations\":{},\"resources\":{\"limits\":{},\"requests\":{}},\"schedulerName\":\"\",\"users\":[]},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":5,\"initialDelaySeconds\":5,\"periodSeconds\":5,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{\"limits\":{},\"requests\":{}},\"schedulerName\":\"\",\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalTrafficPolicy\":\"Cluster\",\"extraPorts\":[],\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"api\":\"\",\"console\":\"\"},\"ports\":{\"api\":9000,\"console\":9001},\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"sidecars\":[],\"startupProbe\":{\"enabled\":false,\"failureThreshold\":60,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"statefulset\":{\"drivesPerNode\":1,\"podManagementPolicy\":\"Parallel\",\"replicaCount\":4,\"updateStrategy\":{\"type\":\"RollingUpdate\"},\"zones\":1},\"tls\":{\"enabled\":false,\"existingSecret\":\"\",\"mountPath\":\"\"},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumePermissions\":{\"containerSecurityContext\":{\"runAsUser\":0},\"enabled\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/bitnami-shell\",\"tag\":\"10-debian-10-r292\"},\"resources\":{\"limits\":{},\"requests\":{}}}}",
                "version": "9.2.8"
              }
            ],
            "name": "minio",
            "namespace": "minio",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.bitnami.com/bitnami",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1000,
            "values": [
              "## @section Global parameters\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass\n\n## @param global.imageRegistry Global Docker image registry\n## @param global.imagePullSecrets Global Docker registry secret names as an array\n## @param global.storageClass Global StorageClass for Persistent Volume(s)\n##\nglobal:\n  imageRegistry: \"\"\n  ## e.g.\n  ## imagePullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  imagePullSecrets: []\n  storageClass: \"\"\n\n## @section Common parameters\n\n## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)\n##\nnameOverride: \"\"\n## @param fullnameOverride String to fully override common.names.fullname template\n##\nfullnameOverride: \"\"\n## @param commonLabels Labels to add to all deployed objects\n##\ncommonLabels: {}\n## @param commonAnnotations Annotations to add to all deployed objects\n##\ncommonAnnotations: {}\n## @param kubeVersion Force target Kubernetes version (using Helm capabilities if not set)\n##\nkubeVersion: \"\"\n## @param clusterDomain Default Kubernetes cluster domain\n##\nclusterDomain: cluster.local\n## @param extraDeploy Array of extra objects to deploy with the release\n##\nextraDeploy: []\n\n## @section MinIO\u0026reg; parameters\n\n## Bitnami MinIO\u0026reg; image version\n## ref: https://hub.docker.com/r/bitnami/minio/tags/\n## @param image.registry MinIO\u0026reg; image registry\n## @param image.repository MinIO\u0026reg; image repository\n## @param image.tag MinIO\u0026reg; image tag (immutable tags are recommended)\n## @param image.pullPolicy Image pull policy\n## @param image.pullSecrets Specify docker-registry secret names as an array\n## @param image.debug Specify if debug logs should be enabled\n##\nimage:\n  registry: docker.io\n  repository: bitnami/minio\n  tag: 2021.12.27-debian-10-r0\n  ## Specify a imagePullPolicy\n  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n  ##\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## e.g:\n  ## pullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  pullSecrets: []\n  ## Set to true if you would like to see extra information on logs\n  ##\n  debug: false\n## Bitnami MinIO\u0026reg; Client image version\n## ref: https://hub.docker.com/r/bitnami/minio-client/tags/\n## @param clientImage.registry MinIO\u0026reg; Client image registry\n## @param clientImage.repository MinIO\u0026reg; Client image repository\n## @param clientImage.tag MinIO\u0026reg; Client image tag (immutable tags are recommended)\n##\nclientImage:\n  registry: docker.io\n  repository: bitnami/minio-client\n  tag: 2021.12.20-debian-10-r6\n## @param mode MinIO\u0026reg; server mode (`standalone` or `distributed`)\n## ref: https://docs.minio.io/docs/distributed-minio-quickstart-guide\n##\nmode: distributed\n## MinIO\u0026reg; authentication parameters\n##\nauth:\n  ## @param auth.rootUser MinIO\u0026reg; root username\n  ##\n  rootUser: minioadmin\n  ## @param auth.rootPassword Password for MinIO\u0026reg; root user\n  ##\n  rootPassword: \"minioadmin\"\n  ## @param auth.existingSecret Use existing secret for credentials details (`auth.rootUser` and `auth.rootPassword` will be ignored and picked up from this secret). The secret has to contain the keys `root-user` and `root-password`)\n  ##\n  existingSecret: \"\"\n  ## @param auth.forcePassword Force users to specify required passwords\n  ##\n  forcePassword: false\n  ## @param auth.useCredentialsFiles Mount credentials as a files instead of using an environment variable\n  ##\n  useCredentialsFiles: false\n  ## @param auth.forceNewKeys Force root credentials (user and password) to be reconfigured every time they change in the secrets\n  ##\n  forceNewKeys: false\n## @param defaultBuckets Comma, semi-colon or space separated list of buckets to create at initialization (only in standalone mode)\n## e.g:\n## defaultBuckets: \"my-bucket, my-second-bucket\"\n##\ndefaultBuckets: \"\"\n## @param disableWebUI Disable MinIO\u0026reg; Web UI\n## ref: https://github.com/minio/minio/tree/master/docs/config/#browser\n##\ndisableWebUI: false\n## Enable tls in front of MinIO\u0026reg; containers.\n##\ntls:\n  ## @param tls.enabled Enable tls in front of the container\n  ##\n  enabled: false\n  ## @param tls.existingSecret Name of an existing secret holding the certificate information\n  ##\n  existingSecret: \"\"\n  ## @param tls.mountPath The mount path where the secret will be located\n  ## Custom mount path where the certificates will be located, if empty will default to /certs\n  mountPath: \"\"\n## @param extraEnvVars Extra environment variables to be set on MinIO\u0026reg; container\n## e.g:\n## extraEnvVars:\n##   - name: FOO\n##     value: \"bar\"\n##\nextraEnvVars: {}\n## @param extraEnvVarsCM ConfigMap with extra environment variables\n##\nextraEnvVarsCM: \"\"\n## @param extraEnvVarsSecret Secret with extra environment variables\n##\nextraEnvVarsSecret: \"\"\n## @param command Default container command (useful when using custom images). Use array form\n##\ncommand: []\n## @param args Default container args (useful when using custom images). Use array form\n##\nargs: []\n\n## @section MinIO\u0026reg; deployment/statefulset parameters\n\n## @param schedulerName Specifies the schedulerName, if it's nil uses kube-scheduler\n## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n##\nschedulerName: \"\"\n## MinIO\u0026reg; deployment parameters\n## Only when 'mode' is 'standalone' or 'gateway.enabled' is 'true'\n##\ndeployment:\n  ## @param deployment.updateStrategy.type Deployment strategy type\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n  ## e.g:\n  ## updateStrategy:\n  ##  type: RollingUpdate\n  ##  rollingUpdate:\n  ##    maxSurge: 25%\n  ##    maxUnavailable: 25%\n  ##\n  updateStrategy:\n    type: Recreate\n## MinIO\u0026reg; statefulset parameters\n## Only when mode is 'distributed'\n##\nstatefulset:\n  ## @param statefulset.updateStrategy.type StatefulSet strategy type\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n  ## e.g:\n  ## updateStrategy:\n  ##  type: RollingUpdate\n  ##  rollingUpdate:\n  ##    maxSurge: 25%\n  ##    maxUnavailable: 25%\n  ##\n  updateStrategy:\n    type: RollingUpdate\n  ## @param statefulset.podManagementPolicy StatefulSet controller supports relax its ordering guarantees while preserving its uniqueness and identity guarantees. There are two valid pod management policies: OrderedReady and Parallel\n  ## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy\n  ##\n  podManagementPolicy: Parallel\n  ## @param statefulset.replicaCount Number of pods per zone (only for MinIO\u0026reg; distributed mode). Should be even and `\u003e= 4`\n  ##\n  replicaCount: 4\n  ## @param statefulset.zones Number of zones (only for MinIO\u0026reg; distributed mode)\n  ##\n  zones: 1\n  ## @param statefulset.drivesPerNode Number of drives attached to every node (only for MinIO\u0026reg; distributed mode)\n  ##\n  drivesPerNode: 1\n\n## MinIO\u0026reg; provisioning\n##\nprovisioning:\n  ## @param provisioning.enabled Enable MinIO\u0026reg; provisioning Job\n  ##\n  enabled: false\n  ## @param provisioning.schedulerName Name of the k8s scheduler (other than default) for MinIO\u0026reg; provisioning\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  schedulerName: \"\"\n  ## @param provisioning.podAnnotations Provisioning Pod annotations.\n  ##\n  podAnnotations: {}\n  ## @param provisioning.command Default provisioning container command (useful when using custom images). Use array form\n  ##\n  command: []\n  ## @param provisioning.args Default provisioning container args (useful when using custom images). Use array form\n  ##\n  args: []\n  ## @param provisioning.extraVolumes Optionally specify extra list of additional volumes for MinIO\u0026reg; provisioning pod\n  ##\n  extraVolumes: []\n  ## @param provisioning.extraVolumeMounts Optionally specify extra list of additional volumeMounts for MinIO\u0026reg; provisioning container\n  ##\n  extraVolumeMounts: []\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param provisioning.resources.limits The resources limits for the container\n  ## @param provisioning.resources.requests The requested resources for the container\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 100m\n    ##    memory: 64Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##    cpu: 200m\n    ##    memory: 128Mi\n    requests: {}\n  ## @param provisioning.users MinIO\u0026reg; users provisioning\n  ## https://docs.min.io/docs/minio-admin-complete-guide.html#user\n  ## e.g.\n  ## users:\n  ##   - username: test-username\n  ##     password: test-password\n  ##     disabled: false\n  ##     policies:\n  ##       - readwrite\n  ##       - consoleAdmin\n  ##       - diagnostics\n  users: []\n  ## @param provisioning.groups MinIO\u0026reg; groups provisioning\n  ## https://docs.min.io/docs/minio-admin-complete-guide.html#group\n  ## e.g.\n  ## groups\n  ##   - name: test-group\n  ##     disabled: false\n  ##     members:\n  ##       - test-username\n  ##     policies:\n  ##       - readwrite\n  groups: []\n  ## @param provisioning.buckets MinIO\u0026reg; buckets, lifecycle, quota and tags provisioning\n  ## Buckets https://docs.min.io/docs/minio-client-complete-guide.html#mb\n  ## Lifecycle https://docs.min.io/docs/minio-client-complete-guide.html#ilm\n  ## Quotas https://docs.min.io/docs/minio-admin-complete-guide.html#bucket\n  ## Tags https://docs.min.io/docs/minio-client-complete-guide.html#tag\n  ## e.g.\n  ## buckets:\n  ##   - name: test-bucket\n  ##     region: us-east-1\n  ##     # Only when mode is 'distributed'\n  ##     # ref: https://docs.minio.io/docs/distributed-minio-quickstart-guide\n  ##     withLock: true\n  ##     # Only when mode is 'distributed'\n  ##     # ref: https://docs.minio.io/docs/distributed-minio-quickstart-guide\n  ##     lifecycle:\n  ##       - id: TestPrefix7dRetention\n  ##         prefix: test-prefix\n  ##         disabled: false\n  ##         expiry:\n  ##           days: 7\n  ##           # Days !OR! date\n  ##           # date: \"2021-11-11T00:00:00Z\"\n  ##           nonconcurrentDays: 3\n  ##     # Only when mode is 'distributed'\n  ##     # ref: https://docs.minio.io/docs/distributed-minio-quickstart-guide\n  ##     quota:\n  ##       # hard, fifo or clear(+ omit size)\n  ##       type: hard\n  ##       size: 10GiB\n  ##     tags:\n  ##       key1: value1\n  buckets: []\n  ## @param provisioning.config MinIO\u0026reg; config provisioning\n  ## https://docs.min.io/docs/minio-server-configuration-guide.html\n  ## e.g.\n  ## config:\n  ##   - name: region\n  ##     options:\n  ##       name: us-east-1\n  config: []\n\n## @param hostAliases MinIO\u0026reg; pod host aliases\n## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n##\nhostAliases: []\n## @param containerPorts.api MinIO\u0026reg; container port to open for MinIO\u0026reg; API\n## @param containerPorts.console MinIO\u0026reg; container port to open for MinIO\u0026reg; Console\n##\ncontainerPorts:\n  api: 9000\n  console: 9001\n## MinIO\u0026reg; pod Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n## @param podSecurityContext.enabled Enable pod Security Context\n## @param podSecurityContext.fsGroup Group ID for the container\n##\npodSecurityContext:\n  enabled: true\n  fsGroup: 1001\n## MinIO\u0026reg; container Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n## @param containerSecurityContext.enabled Enable container Security Context\n## @param containerSecurityContext.runAsUser User ID for the container\n## @param containerSecurityContext.runAsNonRoot Avoid running as root User\n##\ncontainerSecurityContext:\n  enabled: true\n  runAsUser: 1001\n  runAsNonRoot: true\n## @param podLabels Extra labels for MinIO\u0026reg; pods\n## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n##\npodLabels: {}\n## @param podAnnotations Annotations for MinIO\u0026reg; pods\n## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n##\npodAnnotations: {}\n## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAffinityPreset: \"\"\n## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAntiAffinityPreset: soft\n## Node affinity preset\n## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n##\nnodeAffinityPreset:\n  ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ##\n  type: \"\"\n  ## @param nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set.\n  ## E.g.\n  ## key: \"kubernetes.io/e2e-az-name\"\n  ##\n  key: \"\"\n  ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.\n  ## E.g.\n  ## values:\n  ##   - e2e-az1\n  ##   - e2e-az2\n  ##\n  values: []\n## @param affinity Affinity for pod assignment. Evaluated as a template.\n## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n## Note: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set\n##\naffinity: {}\n## @param nodeSelector Node labels for pod assignment. Evaluated as a template.\n## ref: https://kubernetes.io/docs/user-guide/node-selection/\n##\nnodeSelector: {}\n## @param tolerations Tolerations for pod assignment. Evaluated as a template.\n## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n##\ntolerations: []\n## @param topologySpreadConstraints Topology Spread Constraints for MinIO\u0026reg; pods assignment spread across your cluster among failure-domains\n## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods\n##\ntopologySpreadConstraints: []\n## @param priorityClassName MinIO\u0026reg; pods' priorityClassName\n##\npriorityClassName: \"\"\n## MinIO\u0026reg; containers' resource requests and limits\n## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n## We usually recommend not to specify default resources and to leave this as a conscious\n## choice for the user. This also increases chances charts run on environments with little\n## resources, such as Minikube. If you do want to specify resources, uncomment the following\n## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n## @param resources.limits The resources limits for the MinIO\u0026reg; container\n## @param resources.requests The requested resources for the MinIO\u0026reg; container\n##\nresources:\n  ## Example:\n  ## limits:\n  ##    cpu: 250m\n  ##    memory: 256Mi\n  limits: {}\n  ## Examples:\n  ## requests:\n  ##    cpu: 250m\n  ##    memory: 256Mi\n  requests: {}\n## Configure extra options for liveness probe\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n## @param livenessProbe.enabled Enable livenessProbe\n## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n## @param livenessProbe.periodSeconds Period seconds for livenessProbe\n## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n## @param livenessProbe.failureThreshold Failure threshold for livenessProbe\n## @param livenessProbe.successThreshold Success threshold for livenessProbe\n##\nlivenessProbe:\n  enabled: true\n  initialDelaySeconds: 5\n  periodSeconds: 5\n  timeoutSeconds: 5\n  successThreshold: 1\n  failureThreshold: 5\n## Configure extra options for readiness probe\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n## @param readinessProbe.enabled Enable readinessProbe\n## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n## @param readinessProbe.periodSeconds Period seconds for readinessProbe\n## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n## @param readinessProbe.failureThreshold Failure threshold for readinessProbe\n## @param readinessProbe.successThreshold Success threshold for readinessProbe\n##\nreadinessProbe:\n  enabled: true\n  initialDelaySeconds: 5\n  periodSeconds: 5\n  timeoutSeconds: 1\n  successThreshold: 1\n  failureThreshold: 5\n## Configure extra options for startupProbe probe\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n## @param startupProbe.enabled Enable startupProbe\n## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n## @param startupProbe.periodSeconds Period seconds for startupProbe\n## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe\n## @param startupProbe.failureThreshold Failure threshold for startupProbe\n## @param startupProbe.successThreshold Success threshold for startupProbe\n##\nstartupProbe:\n  enabled: false\n  initialDelaySeconds: 0\n  periodSeconds: 10\n  timeoutSeconds: 5\n  successThreshold: 1\n  failureThreshold: 60\n## @param customLivenessProbe Override default liveness probe\n##\ncustomLivenessProbe: {}\n## @param customReadinessProbe Override default readiness probe\n##\ncustomReadinessProbe: {}\n## @param customStartupProbe Override default startup probe\n##\ncustomStartupProbe: {}\n## @param lifecycleHooks for the MinIO\u0026reg container(s) to automate configuration before or after startup\n##\nlifecycleHooks: {}\n## @param extraVolumes Optionally specify extra list of additional volumes for MinIO\u0026reg; pods\n##\nextraVolumes: []\n## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for MinIO\u0026reg; container(s)\n##\nextraVolumeMounts: []\n## @param initContainers Add additional init containers to the MinIO\u0026reg; pods\n## e.g:\n## initContainers:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\ninitContainers: []\n## @param sidecars Add additional sidecar containers to the MinIO\u0026reg; pods\n## e.g:\n## sidecars:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\nsidecars: []\n\n## @section Traffic exposure parameters\n\n## MinIO\u0026reg; Service properties\n##\nservice:\n  ## @param service.type MinIO\u0026reg; service type\n  ##\n  type: ClusterIP\n  ## @param service.ports.api MinIO\u0026reg; API service port\n  ## @param service.ports.console MinIO\u0026reg; Console service port\n  ##\n  ports:\n    api: 9000\n    console: 9001\n  ## @param service.nodePorts.api Specify the MinIO\u0026reg API nodePort value for the LoadBalancer and NodePort service types\n  ## @param service.nodePorts.console Specify the MinIO\u0026reg Console nodePort value for the LoadBalancer and NodePort service types\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n  ##\n  nodePorts:\n    api: \"\"\n    console: \"\"\n  ## @param service.clusterIP Service Cluster IP\n  ## e.g.:\n  ## clusterIP: None\n  ##\n  clusterIP: \"\"\n  ## @param service.loadBalancerIP loadBalancerIP if service type is `LoadBalancer` (optional, cloud specific)\n  ## ref: https://kubernetes.io/docs/user-guide/services/#type-loadbalancer\n  ##\n  loadBalancerIP: \"\"\n  ## @param service.loadBalancerSourceRanges Addresses that are allowed when service is LoadBalancer\n  ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n  ## e.g:\n  ## loadBalancerSourceRanges:\n  ##   - 10.10.10.0/24\n  ##\n  loadBalancerSourceRanges: []\n  ## @param service.externalTrafficPolicy Enable client source IP preservation\n  ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n  ##\n  externalTrafficPolicy: Cluster\n  ## @param service.extraPorts Extra ports to expose in the service (normally used with the `sidecar` value)\n  ##\n  extraPorts: []\n  ## @param service.annotations Annotations for MinIO\u0026reg; service\n  ## This can be used to set the LoadBalancer service type to internal only.\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n  ##\n  annotations: {}\n## Configure the ingress resource that allows you to access the\n## MinIO\u0026reg; Console. Set up the URL\n## ref: https://kubernetes.io/docs/user-guide/ingress/\n##\ningress:\n  ## @param ingress.enabled Enable ingress controller resource\n  ##\n  enabled: false\n  ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)\n  ##\n  apiVersion: \"\"\n  ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)\n  ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster.\n  ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/\n  ##\n  ingressClassName: \"\"\n  ## @param ingress.hostname Default host for the ingress resource\n  ##\n  hostname: minio.local\n  ## @param ingress.path The Path to MinIO\u0026reg;. You may need to set this to '/*' in order to use this with ALB ingress controllers.\n  ##\n  path: /\n  ## @param ingress.pathType Ingress path type\n  ##\n  pathType: ImplementationSpecific\n  ## @param ingress.servicePort Service port to be used\n  ## Default is http. Alternative is https.\n  ##\n  servicePort: minio-console\n  ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.\n  ## For a full list of possible ingress annotations, please see\n  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md\n  ## Use this parameter to set the required annotations for cert-manager, see\n  ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations\n  ##\n  ## e.g:\n  ## annotations:\n  ##   kubernetes.io/ingress.class: nginx\n  ##   cert-manager.io/cluster-issuer: cluster-issuer-name\n  ##\n  annotations: {}\n  ## @param ingress.tls Enable TLS configuration for the hostname defined at `ingress.hostname` parameter\n  ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf \"%s-tls\" .Values.ingress.hostname }}`\n  ## You can:\n  ##   - Use the `ingress.secrets` parameter to create this TLS secret\n  ##   - Relay on cert-manager to create it by setting the corresponding annotations\n  ##   - Relay on Helm to create self-signed certificates by setting `ingress.selfSigned=true`\n  ##\n  tls: false\n  ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm\n  ##\n  selfSigned: false\n  ## @param ingress.extraHosts The list of additional hostnames to be covered with this ingress record.\n  ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array\n  ## e.g:\n  ## extraHosts:\n  ##   - name: minio.local\n  ##     path: /\n  ##\n  extraHosts: []\n  ## @param ingress.extraPaths Any additional paths that may need to be added to the ingress under the main host\n  ## For example: The ALB ingress controller requires a special rule for handling SSL redirection.\n  ## extraPaths:\n  ## - path: /*\n  ##   backend:\n  ##     serviceName: ssl-redirect\n  ##     servicePort: use-annotation\n  ##\n  extraPaths: []\n  ## @param ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.\n  ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls\n  ## e.g:\n  ## extraTls:\n  ## - hosts:\n  ##     - minio.local\n  ##   secretName: minio.local-tls\n  ##\n  extraTls: []\n  ## @param ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets\n  ## key and certificate are expected in PEM format\n  ## name should line up with a secretName set further up\n  ##\n  ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates\n  ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days\n  ## It is also possible to create and manage the certificates outside of this helm chart\n  ## Please see README.md for more information\n  ##\n  ## Example\n  ## secrets:\n  ##   - name: minio.local-tls\n  ##     key: \"\"\n  ##     certificate: \"\"\n  ##\n  secrets: []\n\n## Configure the ingress resource that allows you to access the\n## MinIO\u0026reg; API. Set up the URL\n## ref: https://kubernetes.io/docs/user-guide/ingress/\n##\napiIngress:\n  ## @param apiIngress.enabled Enable ingress controller resource\n  ##\n  enabled: false\n  ## @param apiIngress.apiVersion Force Ingress API version (automatically detected if not set)\n  ##\n  apiVersion: \"\"\n  ## @param apiIngress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)\n  ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster.\n  ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/\n  ##\n  ingressClassName: \"\"\n  ## @param apiIngress.hostname Default host for the ingress resource\n  ##\n  hostname: minio.local\n  ## @param apiIngress.path The Path to MinIO\u0026reg;. You may need to set this to '/*' in order to use this with ALB ingress controllers.\n  ##\n  path: /\n  ## @param apiIngress.pathType Ingress path type\n  ##\n  pathType: ImplementationSpecific\n  ## @param apiIngress.servicePort Service port to be used\n  ## Default is http. Alternative is https.\n  ##\n  servicePort: minio-api\n  ## @param apiIngress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.\n  ## For a full list of possible ingress annotations, please see\n  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md\n  ## Use this parameter to set the required annotations for cert-manager, see\n  ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations\n  ##\n  ## e.g:\n  ## annotations:\n  ##   kubernetes.io/ingress.class: nginx\n  ##   cert-manager.io/cluster-issuer: cluster-issuer-name\n  ##\n  annotations: {}\n  ## @param apiIngress.tls Enable TLS configuration for the hostname defined at `apiIngress.hostname` parameter\n  ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf \"%s-tls\" .Values.apiIngress.hostname }}`\n  ## You can:\n  ##   - Use the `ingress.secrets` parameter to create this TLS secret\n  ##   - Relay on cert-manager to create it by setting the corresponding annotations\n  ##   - Relay on Helm to create self-signed certificates by setting `ingress.selfSigned=true`\n  ##\n  tls: false\n  ## @param apiIngress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm\n  ##\n  selfSigned: false\n  ## @param apiIngress.extraHosts The list of additional hostnames to be covered with this ingress record.\n  ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array\n  ## e.g:\n  ## extraHosts:\n  ##   - name: minio.local\n  ##     path: /\n  ##\n  extraHosts: []\n  ## @param apiIngress.extraPaths Any additional paths that may need to be added to the ingress under the main host\n  ## For example: The ALB ingress controller requires a special rule for handling SSL redirection.\n  ## extraPaths:\n  ## - path: /*\n  ##   backend:\n  ##     serviceName: ssl-redirect\n  ##     servicePort: use-annotation\n  ##\n  extraPaths: []\n  ## @param apiIngress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.\n  ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls\n  ## e.g:\n  ## extraTls:\n  ## - hosts:\n  ##     - minio.local\n  ##   secretName: minio.local-tls\n  ##\n  extraTls: []\n  ## @param apiIngress.secrets If you're providing your own certificates, please use this to add the certificates as secrets\n  ## key and certificate are expected in PEM format\n  ## name should line up with a secretName set further up\n  ##\n  ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates\n  ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days\n  ## It is also possible to create and manage the certificates outside of this helm chart\n  ## Please see README.md for more information\n  ##\n  ## Example\n  ## secrets:\n  ##   - name: minio.local-tls\n  ##     key: \"\"\n  ##     certificate: \"\"\n  ##\n  secrets: []\n\n## NetworkPolicy parameters\n##\nnetworkPolicy:\n  ## @param networkPolicy.enabled Enable the default NetworkPolicy policy\n  ##\n  enabled: false\n  ## @param networkPolicy.allowExternal Don't require client label for connections\n  ## When set to false, only pods with the correct client label will have network access to the port MinIO\u0026reg; is\n  ## listening on. When true, MinIO\u0026reg; will accept connections from any source (with the correct destination port).\n  ##\n  allowExternal: true\n  ## @param networkPolicy.extraFromClauses Allows to add extra 'from' clauses to the NetworkPolicy\n  extraFromClauses: {}\n  ## Example\n  ## extraFromClauses:\n  ## - podSelector:\n  ##     matchLabels:\n  ##       a: b\n\n## @section Persistence parameters\n\n## Enable persistence using Persistent Volume Claims\n## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/\n##\npersistence:\n  ## @param persistence.enabled Enable MinIO\u0026reg; data persistence using PVC. If false, use emptyDir\n  ##\n  enabled: true\n  ## @param persistence.storageClass PVC Storage Class for MinIO\u0026reg; data volume\n  ## If defined, storageClassName: \u003cstorageClass\u003e\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n  ##   GKE, AWS \u0026 OpenStack)\n  ##\n  storageClass: \"\"\n  ## @param persistence.mountPath Data volume mount path\n  ##\n  mountPath: /data\n  ## @param persistence.accessModes PVC Access Modes for MinIO\u0026reg; data volume\n  ##\n  accessModes:\n    - ReadWriteOnce\n  ## @param persistence.size PVC Storage Request for MinIO\u0026reg; data volume\n  ##\n  size: 8Gi\n  ## @param persistence.annotations Annotations for the PVC\n  ##\n  annotations: {}\n  ## @param persistence.existingClaim Name of an existing PVC to use (only in `standalone` mode)\n  ##\n  existingClaim: \"\"\n\n## @section Volume Permissions parameters\n\n## Init containers parameters:\n## volumePermissions: Change the owner and group of the persistent volume mountpoint to runAsUser:fsGroup values from the securityContext section.\n##\nvolumePermissions:\n  ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to `runAsUser:fsGroup`\n  ##\n  enabled: false\n  ## @param volumePermissions.image.registry Init container volume-permissions image registry\n  ## @param volumePermissions.image.repository Init container volume-permissions image repository\n  ## @param volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)\n  ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy\n  ## @param volumePermissions.image.pullSecrets Specify docker-registry secret names as an array\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/bitnami-shell\n    tag: 10-debian-10-r292\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets.\n    ## Secrets must be manually created in the namespace.\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## e.g:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## Init container' resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param volumePermissions.resources.limits Init container volume-permissions resource limits\n  ## @param volumePermissions.resources.requests Init container volume-permissions resource requests\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 500m\n    ##    memory: 1Gi\n    limits: {}\n    requests: {}\n  ## Init container' Security Context\n  ## Note: the chown of the data folder is done to containerSecurityContext.runAsUser\n  ## and not the below volumePermissions.containerSecurityContext.runAsUser\n  ## @param volumePermissions.containerSecurityContext.runAsUser User ID for the init container\n  ##\n  containerSecurityContext:\n    runAsUser: 0\n\n## @section RBAC parameters\n\n## Specifies whether a ServiceAccount should be created\n##\nserviceAccount:\n  ## @param serviceAccount.create Enable the creation of a ServiceAccount for MinIO\u0026reg; pods\n  ##\n  create: true\n  ## @param serviceAccount.name Name of the created ServiceAccount\n  ## If not set and create is true, a name is generated using the common.names.fullname template\n  ##\n  name: \"\"\n  ## @param serviceAccount.automountServiceAccountToken Enable/disable auto mounting of the service account token\n  ##\n  automountServiceAccountToken: true\n  ## @param serviceAccount.annotations Custom annotations for MinIO\u0026reg; ServiceAccount\n  ##\n  annotations: {}\n\n## @section Other parameters\n\n## MinIO\u0026reg; Pod Disruption Budget configuration in distributed mode.\n## If MinIO\u0026reg; Gateway is enabled, creates a Pod Disruption Budget for the Gateway instead (mutually exclusive with distributed mode).\n## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n##\npdb:\n  ## @param pdb.create Enable/disable a Pod Disruption Budget creation\n  ##\n  create: false\n  ## @param pdb.minAvailable Minimum number/percentage of pods that must still be available after the eviction\n  ##\n  minAvailable: 1\n  ## @param pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable after the eviction\n  ##\n  maxUnavailable: \"\"\n\n## @section Metrics parameters\n\nmetrics:\n  ## @param metrics.prometheusAuthType Authentication mode for Prometheus (`jwt` or `public`)\n  ## To allow public access without authentication for prometheus metrics set environment as follows.\n  ##\n  prometheusAuthType: public\n  ## Prometheus Operator ServiceMonitor configuration\n  ##\n  serviceMonitor:\n    ## @param metrics.serviceMonitor.enabled If the operator is installed in your cluster, set to true to create a Service Monitor Entry\n    ##\n    enabled: false\n    ## @param metrics.serviceMonitor.namespace Namespace which Prometheus is running in\n    ##\n    namespace: \"\"\n    ## @param metrics.serviceMonitor.labels Extra labels for the ServiceMonitor\n    ##\n    labels: {}\n    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in Prometheus\n    ##\n    jobLabel: \"\"\n    ## @param metrics.serviceMonitor.path HTTP path to scrape for metrics\n    ##\n    path: /minio/v2/metrics/cluster\n    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped\n    ##\n    interval: 30s\n    ## @param metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended\n    ## e.g:\n    ## scrapeTimeout: 30s\n    scrapeTimeout: \"\"\n    ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion\n    ##\n    metricRelabelings: []\n    ## @param metrics.serviceMonitor.relabelings Metrics relabelings to add to the scrape endpoint, applied before scraping\n    ##\n    relabelings: []\n    ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint\n    ##\n    honorLabels: false\n    ## @param metrics.serviceMonitor.selector Prometheus instance selector labels\n    ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration\n    ##\n    selector: {}\n\n## @section Gateway parameters\n\ngateway:\n  ## @param gateway.enabled Use MinIO\u0026reg; as Gateway for other storage systems\n  ##\n  enabled: false\n  ## @param gateway.type Gateway type. Supported types are: `azure`, `gcs`, `nas`, `s3`\n  ## ref: https://docs.minio.io/docs/minio-gateway-for-azure\n  ## ref: https://docs.minio.io/docs/minio-gateway-for-gcs\n  ## ref: https://docs.minio.io/docs/minio-gateway-for-nas\n  ## ref: https://docs.minio.io/docs/minio-gateway-for-s3\n  ##\n  type: s3\n  ## @param gateway.replicaCount Number of MinIO\u0026reg; Gateway replicas\n  ##\n  replicaCount: 4\n  ## @param gateway.updateStrategy.type Update strategy type for MinIO\u0026reg; Gateway replicas\n  updateStrategy:\n    type: Recreate\n  ## Autoscaling configuration for MinIO\u0026reg; Gateway. overrides gateway.replicaCount if enabled\n  ## @param gateway.autoscaling.enabled Enable autoscaling for MinIO\u0026reg; Gateway deployment\n  ## @param gateway.autoscaling.minReplicas Minimum number of replicas to scale back\n  ## @param gateway.autoscaling.maxReplicas Maximum number of replicas to scale out\n  ## @param gateway.autoscaling.targetCPU Target CPU utilization percentage\n  ## @param gateway.autoscaling.targetMemory Target Memory utilization percentage\n  autoscaling:\n    enabled: false\n    minReplicas: \"4\"\n    maxReplicas: \"4\"\n    targetCPU: \"\"\n    targetMemory: \"\"\n  ## @param gateway.priorityClassName Pod priority class name for MinIO\u0026reg; Gateway\n  ##\n  priorityClassName: \"\"\n  ## Gateway authentication configuration\n  ##\n  auth:\n    ## Authentication configuration for Azure. Ignored unless type=azure\n    ## @param gateway.auth.azure.accessKey Access key to access MinIO\u0026reg; using Azure Gateway\n    ## @param gateway.auth.azure.secretKey Secret key to access MinIO\u0026reg; using Azure Gateway\n    ## @param gateway.auth.azure.serviceEndpoint Azure Blob Storage custom endpoint\n    ## @param gateway.auth.azure.storageAccountName Azure Storage Account Name to use to access Azure Blob Storage\n    ## @param gateway.auth.azure.storageAccountKey Azure Storage Account Key to use to access Azure Blob Storage\n    ## @param gateway.auth.azure.storageAccountNameExistingSecret Existing Secret name to extract Azure Storage Account Name from to access Azure Blob Storage\n    ## @param gateway.auth.azure.storageAccountNameExistingSecretKey Existing Secret key to extract Azure Storage Account Name from to use to access Azure Blob Storage\n    ## @param gateway.auth.azure.storageAccountKeyExistingSecret Existing Secret name to extract Azure Storage Account Key from to access Azure Blob Storage\n    ## @param gateway.auth.azure.storageAccountKeyExistingSecretKey Existing Secret key to extract Azure Storage Account Key from to use to access Azure Blob Storage\n    ##\n    azure:\n      accessKey: \"\"\n      secretKey: \"\"\n      serviceEndpoint: \"\"\n      storageAccountName: \"\"\n      storageAccountKey: \"\"\n      storageAccountNameExistingSecret: \"\"\n      storageAccountNameExistingSecretKey: \"\"\n      storageAccountKeyExistingSecret: \"\"\n      storageAccountKeyExistingSecretKey: \"\"\n    ## Authentication configuration for GCS. Ignored unless type=gcs\n    ## @param gateway.auth.gcs.accessKey Access key to access MinIO\u0026reg; using GCS Gateway\n    ## @param gateway.auth.gcs.secretKey Secret key to access MinIO\u0026reg; using GCS Gateway\n    ## @param gateway.auth.gcs.keyJSON Service Account key to access GCS\n    ## @param gateway.auth.gcs.projectID GCP Project ID to use\n    ##\n    gcs:\n      accessKey: \"\"\n      secretKey: \"\"\n      keyJSON: \"\"\n      projectID: \"\"\n    ## Authentication configuration for NAS. Ignored unless type=nas\n    ## @param gateway.auth.nas.accessKey Access key to access MinIO\u0026reg; using NAS Gateway\n    ## @param gateway.auth.nas.secretKey Secret key to access MinIO\u0026reg; using NAS Gateway\n    ##\n    nas:\n      accessKey: \"\"\n      secretKey: \"\"\n    ## Authentication configuration for S3. Ignored unless type=s3\n    ## @param gateway.auth.s3.accessKey Access key to use to access AWS S3\n    ## @param gateway.auth.s3.secretKey Secret key to use to access AWS S3\n    ## @param gateway.auth.s3.serviceEndpoint AWS S3 endpoint\n    ##\n    s3:\n      accessKey: \"\"\n      secretKey: \"\"\n      serviceEndpoint: https://s3.amazonaws.com\n"
            ],
            "verify": false,
            "version": "9.2.8",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mlflow",
            "module.istio_install.helm_release.istio_base",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.minio",
            "module.kube.kubernetes_namespace.spark",
            "module.istio_install.helm_release.istio_ingress",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.mariadb"
          ]
        }
      ]
    },
    {
      "module": "module.mlflow_install",
      "mode": "managed",
      "type": "kubernetes_deployment",
      "name": "mlflow_server",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "id": "mlflow/mlflow-server",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 1,
                "labels": {
                  "app": "mlflow-server"
                },
                "name": "mlflow-server",
                "namespace": "mlflow",
                "resource_version": "189598",
                "uid": "2b46de26-ed12-41c6-88ae-d27db3111696"
              }
            ],
            "spec": [
              {
                "min_ready_seconds": 0,
                "paused": false,
                "progress_deadline_seconds": 600,
                "replicas": "1",
                "revision_history_limit": 10,
                "selector": [
                  {
                    "match_expressions": [],
                    "match_labels": {
                      "app": "mlflow-server"
                    }
                  }
                ],
                "strategy": [
                  {
                    "rolling_update": [
                      {
                        "max_surge": "25%",
                        "max_unavailable": "25%"
                      }
                    ],
                    "type": "RollingUpdate"
                  }
                ],
                "template": [
                  {
                    "metadata": [
                      {
                        "annotations": {},
                        "generate_name": "",
                        "generation": 0,
                        "labels": {
                          "app": "mlflow-server"
                        },
                        "name": "",
                        "namespace": "",
                        "resource_version": "",
                        "uid": ""
                      }
                    ],
                    "spec": [
                      {
                        "active_deadline_seconds": 0,
                        "affinity": [],
                        "automount_service_account_token": true,
                        "container": [
                          {
                            "args": [],
                            "command": [
                              "/bin/bash",
                              "-c",
                              "mlflow server --host 0.0.0.0 --backend-store-uri ${BACKEND_URI} --default-artifact-root ${DEFAULT_ARTIFACT_ROOT}"
                            ],
                            "env": [
                              {
                                "name": "MLFLOW_S3_ENDPOINT_URL",
                                "value": "http://submarine-minio-service:9000",
                                "value_from": []
                              },
                              {
                                "name": "AWS_ACCESS_KEY_ID",
                                "value": "submarine_minio",
                                "value_from": []
                              },
                              {
                                "name": "AWS_SECRET_ACCESS_KEY",
                                "value": "submarine_minio",
                                "value_from": []
                              },
                              {
                                "name": "BACKEND_URI",
                                "value": "mysql+pymysql://root:admin@mariadb.mariadb.svc.cluster.local:3306/mlflow_db",
                                "value_from": []
                              },
                              {
                                "name": "DEFAULT_ARTIFACT_ROOT",
                                "value": "s3://mlflow",
                                "value_from": []
                              }
                            ],
                            "env_from": [],
                            "image": "apache/submarine:mlflow-0.6.0",
                            "image_pull_policy": "Always",
                            "lifecycle": [],
                            "liveness_probe": [],
                            "name": "server",
                            "port": [
                              {
                                "container_port": 5000,
                                "host_ip": "",
                                "host_port": 0,
                                "name": "",
                                "protocol": "TCP"
                              }
                            ],
                            "readiness_probe": [],
                            "resources": [
                              {
                                "limits": null,
                                "requests": null
                              }
                            ],
                            "security_context": [],
                            "startup_probe": [],
                            "stdin": false,
                            "stdin_once": false,
                            "termination_message_path": "/dev/termination-log",
                            "termination_message_policy": "File",
                            "tty": false,
                            "volume_mount": [],
                            "working_dir": ""
                          }
                        ],
                        "dns_config": [],
                        "dns_policy": "ClusterFirst",
                        "enable_service_links": true,
                        "host_aliases": [],
                        "host_ipc": false,
                        "host_network": false,
                        "host_pid": false,
                        "hostname": "",
                        "image_pull_secrets": [],
                        "init_container": [],
                        "node_name": "",
                        "node_selector": {},
                        "priority_class_name": "",
                        "readiness_gate": [],
                        "restart_policy": "Always",
                        "security_context": [],
                        "service_account_name": "",
                        "share_process_namespace": false,
                        "subdomain": "",
                        "termination_grace_period_seconds": 30,
                        "toleration": [],
                        "topology_spread_constraint": [],
                        "volume": []
                      }
                    ]
                  }
                ]
              }
            ],
            "timeouts": null,
            "wait_for_rollout": true
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6NjAwMDAwMDAwMDAwLCJ1cGRhdGUiOjYwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "module.istio_install.helm_release.istio_ingress",
            "module.kube.kubernetes_namespace.mlflow",
            "module.istio_install.helm_release.istio_base",
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.keycloak",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mariadb",
            "module.mariadb_install.helm_release.mariadb",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.spark",
            "module.kube.kubernetes_namespace.minio",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.jupyterhub"
          ]
        }
      ]
    },
    {
      "module": "module.mlflow_install",
      "mode": "managed",
      "type": "kubernetes_service",
      "name": "mlflow_server",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "id": "mlflow/mlflow-server",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "app": "mlflow-server"
                },
                "name": "mlflow-server",
                "namespace": "mlflow",
                "resource_version": "157660",
                "uid": "cf99f26b-2a62-45cb-be74-7b7e8dfd2582"
              }
            ],
            "spec": [
              {
                "cluster_ip": "10.43.134.83",
                "external_ips": [],
                "external_name": "",
                "external_traffic_policy": "",
                "health_check_node_port": 0,
                "load_balancer_ip": "",
                "load_balancer_source_ranges": [],
                "port": [
                  {
                    "name": "http",
                    "node_port": 0,
                    "port": 5000,
                    "protocol": "TCP",
                    "target_port": "5000"
                  }
                ],
                "publish_not_ready_addresses": false,
                "selector": {
                  "app": "mlflow-server"
                },
                "session_affinity": "None",
                "type": "ClusterIP"
              }
            ],
            "status": [
              {
                "load_balancer": [
                  {
                    "ingress": []
                  }
                ]
              }
            ],
            "timeouts": null,
            "wait_for_load_balancer": true
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.mlflow",
            "module.kube.kubernetes_namespace.spark",
            "module.mariadb_install.helm_release.mariadb",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.minio",
            "module.istio_install.helm_release.istio_base",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.keycloak",
            "module.istio_install.helm_release.istio_ingress",
            "module.kube.kubernetes_namespace.argo-events"
          ]
        }
      ]
    },
    {
      "module": "module.spark_install",
      "mode": "managed",
      "type": "helm_release",
      "name": "spark",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "spark",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "spark",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "3.2.0",
                "chart": "spark",
                "name": "spark",
                "namespace": "spark",
                "revision": 2,
                "values": "{\"diagnosticMode\":{\"args\":[\"infinity\"],\"command\":[\"sleep\"],\"enabled\":false},\"extraDeploy\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\"},\"hostNetwork\":false,\"image\":{\"debug\":false,\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/spark\",\"tag\":\"3.2.0-debian-10-r33\"},\"ingress\":{\"annotations\":{},\"apiVersion\":\"\",\"enabled\":false,\"extraHosts\":[],\"extraPaths\":[],\"extraTls\":[],\"hostname\":\"spark.local\",\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"secrets\":[],\"tls\":false},\"kubeVersion\":\"\",\"master\":{\"affinity\":{},\"clusterPort\":7077,\"configOptions\":\"\",\"configurationConfigMap\":\"\",\"daemonMemoryLimit\":\"\",\"extraEnvVars\":[],\"extraPodLabels\":{},\"hostAliases\":[],\"initContainers\":[],\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":180,\"periodSeconds\":20,\"successThreshold\":1,\"timeoutSeconds\":5},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"resources\":{\"limits\":{},\"requests\":{}},\"securityContext\":{\"enabled\":true,\"fsGroup\":1001,\"runAsGroup\":0,\"runAsUser\":1001,\"seLinuxOptions\":{}},\"tolerations\":[],\"webPort\":8080},\"metrics\":{\"enabled\":false,\"masterAnnotations\":{\"prometheus.io/path\":\"/metrics/\",\"prometheus.io/port\":\"{{ .Values.master.webPort }}\",\"prometheus.io/scrape\":\"true\"},\"podMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"extraMetricsEndpoints\":[],\"interval\":\"30s\",\"namespace\":\"\",\"scrapeTimeout\":\"\"},\"prometheusRule\":{\"additionalLabels\":{},\"enabled\":false,\"namespace\":\"\",\"rules\":[]},\"workerAnnotations\":{\"prometheus.io/path\":\"/metrics/\",\"prometheus.io/port\":\"{{ .Values.worker.webPort }}\",\"prometheus.io/scrape\":\"true\"}},\"nameOverride\":\"\",\"security\":{\"certificatesSecretName\":\"\",\"passwordsSecretName\":\"\",\"rpc\":{\"authenticationEnabled\":false,\"encryptionEnabled\":false},\"ssl\":{\"autoGenerated\":false,\"enabled\":false,\"existingSecret\":\"\",\"keystorePassword\":\"\",\"needClientAuth\":false,\"protocol\":\"TLSv1.2\",\"resources\":{\"limits\":{},\"requests\":{}},\"truststorePassword\":\"\"},\"storageEncryptionEnabled\":false},\"service\":{\"annotations\":{},\"clusterPort\":7077,\"loadBalancerIP\":\"\",\"nodePorts\":{\"cluster\":\"\",\"web\":\"\"},\"type\":\"ClusterIP\",\"webPort\":80},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"worker\":{\"affinity\":{},\"autoscaling\":{\"CpuTargetPercentage\":50,\"enabled\":true,\"replicasMax\":5},\"clusterPort\":\"\",\"configOptions\":\"\",\"configurationConfigMap\":\"\",\"coreLimit\":\"\",\"daemonMemoryLimit\":\"\",\"dir\":\"\",\"extraEnvVars\":[],\"extraPodLabels\":{},\"extraPorts\":[],\"hostAliases\":[],\"initContainers\":[],\"javaOptions\":\"\",\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":180,\"periodSeconds\":20,\"successThreshold\":1,\"timeoutSeconds\":5},\"memoryLimit\":\"\",\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podManagementPolicy\":\"OrderedReady\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"replicaCount\":2,\"resources\":{\"limits\":{},\"requests\":{}},\"securityContext\":{\"enabled\":true,\"fsGroup\":1001,\"runAsGroup\":0,\"runAsUser\":1001,\"seLinuxOptions\":{}},\"tolerations\":[],\"webPort\":8081}}",
                "version": "5.7.13"
              }
            ],
            "name": "spark",
            "namespace": "spark",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.bitnami.com/bitnami",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 1000,
            "values": [
              "## @section Global parameters\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass\n\n## @param global.imageRegistry Global Docker image registry\n## @param global.imagePullSecrets Global Docker registry secret names as an array\n##\nglobal:\n  imageRegistry: \"\"\n  ## E.g.\n  ## imagePullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  imagePullSecrets: []\n\n## @section Common parameters\n\n## @param kubeVersion Force target Kubernetes version (using Helm capabilities if not set)\n##\nkubeVersion: \"\"\n## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)\n##\nnameOverride: \"\"\n## @param fullnameOverride String to fully override common.names.fullname template\n##\nfullnameOverride: \"\"\n## @param extraDeploy Array of extra objects to deploy with the release\n##\nextraDeploy: []\n\n## Enable diagnostic mode in the deployment\n##\ndiagnosticMode:\n  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)\n  ##\n  enabled: false\n  ## @param diagnosticMode.command Command to override all containers in the deployment\n  ##\n  command:\n    - sleep\n  ## @param diagnosticMode.args Args to override all containers in the deployment\n  ##\n  args:\n    - infinity\n\n## @section Spark parameters\n\n## Bitnami Spark image version\n## ref: https://hub.docker.com/r/bitnami/spark/tags/\n## @param image.registry Spark image registry\n## @param image.repository Spark image repository\n## @param image.tag Spark image tag (immutable tags are recommended)\n## @param image.pullPolicy Spark image pull policy\n## @param image.pullSecrets Specify docker-registry secret names as an array\n## @param image.debug Enable image debug mode\n##\nimage:\n  registry: docker.io\n  repository: bitnami/spark\n  tag: 3.2.0-debian-10-r33\n  ## Specify a imagePullPolicy\n  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n  ##\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## e.g:\n  ## pullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  pullSecrets: []\n  ## Set to true if you would like to see extra information on logs\n  ## It turns BASH and/or NAMI debugging in the image\n  ##\n  debug: false\n## @param hostNetwork Enable HOST Network\n## If hostNetwork is true, then  dnsPolicy is set to ClusterFirstWithHostNet\n##\nhostNetwork: false\n\n## @section RBAC parameters\n##\n\n## Spark pods ServiceAccount\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n##\nserviceAccount:\n  ## @param serviceAccount.create Enable the creation of a ServiceAccount for Spark pods\n  ##\n  create: true\n  ## @param serviceAccount.name The name of the ServiceAccount to use.\n  ## If not set and create is true, a name is generated using the spark.fullname template\n  ##\n  name: \"\"\n  ## @param serviceAccount.annotations Annotations for Spark Service Account\n  ##\n  annotations: {}\n  ## @param serviceAccount.automountServiceAccountToken Automount API credentials for a service account.\n  ##\n  automountServiceAccountToken: true\n\n## @section Spark master parameters\n\n## Spark master specific configuration\n##\nmaster:\n  ## @param master.configurationConfigMap Set a custom configuration by using an existing configMap with the configuration file.\n  ##\n  configurationConfigMap: \"\"\n  ## @param master.webPort Specify the port where the web interface will listen on the master\n  ##\n  webPort: 8080\n  ## @param master.clusterPort Specify the port where the master listens to communicate with workers\n  ##\n  clusterPort: 7077\n  ## @param master.hostAliases Deployment pod host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ##\n  hostAliases: []\n  ## @param master.daemonMemoryLimit Set the memory limit for the master daemon\n  ##\n  daemonMemoryLimit: \"\"\n  ## @param master.configOptions Use a string to set the config options for in the form \"-Dx=y\"\n  ##\n  configOptions: \"\"\n  ## @param master.extraEnvVars Extra environment variables to pass to the master container\n  ## For example:\n  ## extraEnvVars:\n  ##  - name: SPARK_DAEMON_JAVA_OPTS\n  ##    value: -Dx=y\n  ##\n  extraEnvVars: []\n  ## Kubernetes Security Context\n  ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ## @param master.securityContext.enabled Enable security context\n  ## @param master.securityContext.fsGroup Group ID for the container\n  ## @param master.securityContext.runAsUser User ID for the container\n  ## @param master.securityContext.runAsGroup Group ID for the container\n  ## @param master.securityContext.seLinuxOptions SELinux options for the container\n  ##\n  securityContext:\n    enabled: true\n    fsGroup: 1001\n    runAsUser: 1001\n    runAsGroup: 0\n    seLinuxOptions: {}\n  ## @param master.podAnnotations Annotations for pods in StatefulSet\n  ##\n  podAnnotations: {}\n  ## @param master.extraPodLabels Extra labels for pods in StatefulSet\n  ##\n  extraPodLabels: {}\n  ## @param master.podAffinityPreset Spark master pod affinity preset. Ignored if `master.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAffinityPreset: \"\"\n  ## @param master.podAntiAffinityPreset Spark master pod anti-affinity preset. Ignored if `master.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAntiAffinityPreset: soft\n  ## Spark master node affinity preset\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n  ##\n  nodeAffinityPreset:\n    ## @param master.nodeAffinityPreset.type Spark master node affinity preset type. Ignored if `master.affinity` is set. Allowed values: `soft` or `hard`\n    ##\n    type: \"\"\n    ## @param master.nodeAffinityPreset.key Spark master node label key to match Ignored if `master.affinity` is set.\n    ## E.g.\n    ## key: \"kubernetes.io/e2e-az-name\"\n    ##\n    key: \"\"\n    ## @param master.nodeAffinityPreset.values Spark master node label values to match. Ignored if `master.affinity` is set.\n    ## E.g.\n    ## values:\n    ##   - e2e-az1\n    ##   - e2e-az2\n    ##\n    values: []\n  ## @param master.affinity Spark master affinity for pod assignment\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ## Note: master.podAffinityPreset, master.podAntiAffinityPreset, and master.nodeAffinityPreset will be ignored when it's set\n  ##\n  affinity: {}\n  ## @param master.nodeSelector Spark master node labels for pod assignment\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param master.tolerations Spark master tolerations for pod assignment\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## Container resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param master.resources.limits The resources limits for the container\n  ## @param master.resources.requests The requested resources for the container\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 250m\n    ##    memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##    cpu: 250m\n    ##    memory: 256Mi\n    requests: {}\n  ## Configure extra options for liveness probe\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param master.livenessProbe.enabled Enable livenessProbe\n  ## @param master.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param master.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param master.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param master.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param master.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 180\n    periodSeconds: 20\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## Configure extra options for readiness probe\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param master.readinessProbe.enabled Enable readinessProbe\n  ## @param master.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param master.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param master.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param master.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param master.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## @param master.initContainers Add initContainers to the master pods.\n  ## Example:\n  ## initContainers:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  initContainers: []\n\n## @section Spark worker parameters\n\n## Spark worker specific configuration\n##\nworker:\n  ## @param worker.configurationConfigMap Set a custom configuration by using an existing configMap with the configuration file.\n  ##\n  configurationConfigMap: \"\"\n  ## @param worker.webPort Specify the port where the web interface will listen on the worker\n  ##\n  webPort: 8081\n  ## @param worker.clusterPort Specify the port where the worker listens to communicate with the master\n  ##\n  clusterPort: \"\"\n  ## @param worker.hostAliases Add deployment host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ##\n  hostAliases: []\n  ## @param worker.extraPorts Specify the port where the running jobs inside the workers listens\n  ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#containerport-v1-core\n  ## e.g:\n  ## - name: myapp\n  ##   containerPort: 8000\n  ##   protocol: TCP\n  ##\n  extraPorts: []\n  ## @param worker.daemonMemoryLimit Set the memory limit for the worker daemon\n  ##\n  daemonMemoryLimit: \"\"\n  ## @param worker.memoryLimit Set the maximum memory the worker is allowed to use\n  ##\n  memoryLimit: \"\"\n  ## @param worker.coreLimit Se the maximum number of cores that the worker can use\n  ##\n  coreLimit: \"\"\n  ## @param worker.dir Set a custom working directory for the application\n  ##\n  dir: \"\"\n  ## @param worker.javaOptions Set options for the JVM in the form `-Dx=y`\n  ##\n  javaOptions: \"\"\n  ## @param worker.configOptions Set extra options to configure the worker in the form `-Dx=y`\n  ##\n  configOptions: \"\"\n  ## @param worker.extraEnvVars An array to add extra env vars\n  ## For example:\n  ## extraEnvVars:\n  ##  - name: SPARK_DAEMON_JAVA_OPTS\n  ##    value: -Dx=y\n  extraEnvVars: []\n  ## @param worker.replicaCount Number of spark workers (will be the minimum number when autoscaling is enabled)\n  ##\n  replicaCount: 2\n  ## @param worker.podManagementPolicy Statefulset Pod Management Policy Type\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies\n  ##\n  podManagementPolicy: OrderedReady\n  ## Kubernetes Security Context\n  ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ## @param worker.securityContext.enabled Enable security context\n  ## @param worker.securityContext.fsGroup Group ID for the container\n  ## @param worker.securityContext.runAsUser User ID for the container\n  ## @param worker.securityContext.runAsGroup Group ID for the container\n  ## @param worker.securityContext.seLinuxOptions SELinux options for the container\n  ##\n  securityContext:\n    enabled: true\n    fsGroup: 1001\n    runAsUser: 1001\n    runAsGroup: 0\n    seLinuxOptions: {}\n  ## @param worker.podAnnotations Annotations for pods in StatefulSet\n  ##\n  podAnnotations: {}\n  ## @param worker.extraPodLabels Extra labels for pods in StatefulSet\n  ##\n  extraPodLabels: {}\n  ## @param worker.podAffinityPreset Spark worker pod affinity preset. Ignored if `worker.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAffinityPreset: \"\"\n  ## @param worker.podAntiAffinityPreset Spark worker pod anti-affinity preset. Ignored if `worker.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAntiAffinityPreset: soft\n  ## Spark worker node affinity preset\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n  ##\n  nodeAffinityPreset:\n    ## @param worker.nodeAffinityPreset.type Spark worker node affinity preset type. Ignored if `worker.affinity` is set. Allowed values: `soft` or `hard`\n    ##\n    type: \"\"\n    ## @param worker.nodeAffinityPreset.key Spark worker node label key to match Ignored if `worker.affinity` is set.\n    ## E.g.\n    ## key: \"kubernetes.io/e2e-az-name\"\n    ##\n    key: \"\"\n    ## @param worker.nodeAffinityPreset.values Spark worker node label values to match. Ignored if `worker.affinity` is set.\n    ## E.g.\n    ## values:\n    ##   - e2e-az1\n    ##   - e2e-az2\n    ##\n    values: []\n  ## @param worker.affinity Spark worker affinity for pod assignment\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ## Note: worker.podAffinityPreset, worker.podAntiAffinityPreset, and worker.nodeAffinityPreset will be ignored when it's set\n  ##\n  affinity: {}\n  ## @param worker.nodeSelector Spark worker node labels for pod assignment\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param worker.tolerations Spark worker tolerations for pod assignment\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## Container resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## We usually recommend not to specify default resources and to leave this as a conscious\n  ## choice for the user. This also increases chances charts run on environments with little\n  ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  ## @param worker.resources.limits The resources limits for the container\n  ## @param worker.resources.requests The requested resources for the container\n  ##\n  resources:\n    ## Example:\n    ## limits:\n    ##    cpu: 250m\n    ##    memory: 256Mi\n    limits: {}\n    ## Examples:\n    ## requests:\n    ##    cpu: 250m\n    ##    memory: 256Mi\n    requests: {}\n  ## Configure extra options for liveness probe\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param worker.livenessProbe.enabled Enable livenessProbe\n  ## @param worker.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param worker.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param worker.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param worker.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param worker.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 180\n    periodSeconds: 20\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## Configure extra options for readiness probe\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes\n  ## @param worker.readinessProbe.enabled Enable readinessProbe\n  ## @param worker.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param worker.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param worker.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param worker.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param worker.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## @param worker.initContainers Add initContainers to the master pods.\n  ## Example:\n  ## initContainers:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  initContainers: []\n  ## Array to add extra volumes\n  ##\n  ## extraVolumes:\n  ## Array to add extra mounts (normally used with extraVolumes)\n  ##\n  ## extraVolumeMounts: {}\n  ## Autoscaling parameters\n  ##\n  autoscaling:\n    ## @param worker.autoscaling.enabled Enable replica autoscaling depending on CPU\n    ##\n    enabled: true\n    ## @param worker.autoscaling.CpuTargetPercentage Kubernetes HPA CPU target percentage\n    ##\n    CpuTargetPercentage: 50\n    ## @param worker.autoscaling.replicasMax Maximum number of workers when using autoscaling\n    ##\n    replicasMax: 5\n\n## @section Security parameters\n\n## Security configuration\n##\nsecurity:\n  ## @param security.passwordsSecretName Name of the secret that contains all the passwords\n  ## This is optional, by default random passwords are generated\n  ##\n  passwordsSecretName: \"\"\n  ## RPC configuration\n  ## @param security.rpc.authenticationEnabled Enable the RPC authentication\n  ## @param security.rpc.encryptionEnabled Enable the encryption for RPC\n  ##\n  rpc:\n    authenticationEnabled: false\n    encryptionEnabled: false\n  ## @param security.storageEncryptionEnabled Enables local storage encryption\n  ##\n  storageEncryptionEnabled: false\n  ## @param security.certificatesSecretName Name of the secret that contains the certificates.\n  ## It should contains two keys called \"spark-keystore.jks\" and \"spark-truststore.jks\" with the files in JKS format.\n  ## DEPRECATED. Use `security.ssl.existingSecret` instead\n  ##\n  certificatesSecretName: \"\"\n  ## SSL configuration\n  ##\n  ssl:\n    ## @param security.ssl.enabled Enable the SSL configuration\n    ##\n    enabled: false\n    ## @param security.ssl.needClientAuth Enable the client authentication\n    ##\n    needClientAuth: false\n    ## @param security.ssl.protocol Set the SSL protocol\n    ##\n    protocol: TLSv1.2\n    ## @param security.ssl.existingSecret Name of the existing secret containing the TLS certificates\n    ## It should contains two keys called \"spark-keystore.jks\" and \"spark-truststore.jks\" with the files in JKS format.\n    ##\n    existingSecret: \"\"\n    ## @param security.ssl.autoGenerated Create self-signed TLS certificates. Currently only supports PEM certificates\n    ## The Spark container will generate a JKS keystore and trustore using the PEM certificates.\n    ##\n    autoGenerated: false\n    ## @param security.ssl.keystorePassword Set the password of the JKS Keystore\n    ##\n    keystorePassword: \"\"\n    ## @param security.ssl.truststorePassword Truststore password.\n    ##\n    truststorePassword: \"\"\n    ## Container resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## We usually recommend not to specify default resources and to leave this as a conscious\n    ## choice for the user. This also increases chances charts run on environments with little\n    ## resources, such as Minikube. If you do want to specify resources, uncomment the following\n    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n    ## @param security.ssl.resources.limits The resources limits for the container\n    ## @param security.ssl.resources.requests The requested resources for the container\n    ##\n    resources:\n      ## Example:\n      ## limits:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      limits: {}\n      ## Examples:\n      ## requests:\n      ##    cpu: 100m\n      ##    memory: 128Mi\n      requests: {}\n\n## @section Traffic Exposure parameters\n\n## Service parameters\n##\nservice:\n  ## @param service.type Kubernetes Service type\n  ##\n  type: ClusterIP\n  ## @param service.clusterPort Spark cluster port\n  ##\n  clusterPort: 7077\n  ## @param service.webPort Spark client port\n  ##\n  webPort: 80\n  ## Specify the nodePort(s) value(s) for the LoadBalancer and NodePort service types.\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n  ## @param service.nodePorts.cluster Kubernetes cluster node port\n  ## @param service.nodePorts.web Kubernetes web node port\n  ##\n  nodePorts:\n    cluster: \"\"\n    web: \"\"\n  ## @param service.loadBalancerIP Load balancer IP if spark service type is `LoadBalancer`\n  ## Set the LoadBalancer service type to internal only\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n  ##\n  loadBalancerIP: \"\"\n  ## @param service.annotations Annotations for spark service\n  ## This can be used to set the LoadBalancer service type to internal only.\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n  ##\n  annotations: {}\n## Configure the ingress resource that allows you to access the\n## Spark installation. Set up the URL\n## ref: https://kubernetes.io/docs/user-guide/ingress/\n##\ningress:\n  ## @param ingress.enabled Enable ingress controller resource\n  ##\n  enabled: false\n  ## DEPRECATED: Use ingress.annotations instead of ingress.certManager\n  ## certManager: false\n  ##\n\n  ## @param ingress.pathType Ingress path type\n  ##\n  pathType: ImplementationSpecific\n  ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)\n  ##\n  apiVersion: \"\"\n  ## @param ingress.hostname Default host for the ingress resource\n  ##\n  hostname: spark.local\n  ## @param ingress.path The Path to Spark. You may need to set this to '/*' in order to use this with ALB ingress controllers.\n  ##\n  path: /\n  ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.\n  ## For a full list of possible ingress annotations, please see\n  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md\n  ## Use this parameter to set the required annotations for cert-manager, see\n  ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations\n  ##\n  ## e.g:\n  ## annotations:\n  ##   kubernetes.io/ingress.class: nginx\n  ##   cert-manager.io/cluster-issuer: cluster-issuer-name\n  ##\n  annotations: {}\n  ## @param ingress.tls Enable TLS configuration for the hostname defined at ingress.hostname parameter\n  ## TLS certificates will be retrieved from a TLS secret with name: {{- printf \"%s-tls\" .Values.ingress.hostname }}\n  ## You can use the ingress.secrets parameter to create this TLS secret or relay on cert-manager to create it\n  ##\n  tls: false\n  ## @param ingress.extraHosts The list of additional hostnames to be covered with this ingress record.\n  ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array\n  ## extraHosts:\n  ## - name: spark.local\n  ##   path: /\n  ##\n  extraHosts: []\n  ## @param ingress.extraPaths Any additional arbitrary paths that may need to be added to the ingress under the main host.\n  ## For example: The ALB ingress controller requires a special rule for handling SSL redirection.\n  ## extraPaths:\n  ## - path: /*\n  ##   backend:\n  ##     serviceName: ssl-redirect\n  ##     servicePort: use-annotation\n  ##\n  extraPaths: []\n  ## @param ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.\n  ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls\n  ## extraTls:\n  ## - hosts:\n  ##     - spark.local\n  ##   secretName: spark.local-tls\n  ##\n  extraTls: []\n  ## @param ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets\n  ## key and certificate should start with -----BEGIN CERTIFICATE----- or\n  ## -----BEGIN RSA PRIVATE KEY-----\n  ##\n  ## name should line up with a tlsSecret set further up\n  ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set\n  ##\n  ## It is also possible to create and manage the certificates outside of this helm chart\n  ## Please see README.md for more information\n  ## e.g:\n  ## - name: spark.local-tls\n  ##   key:\n  ##   certificate:\n  ##\n  secrets: []\n\n## @section Metrics parameters\n\n## Metrics configuration\n##\nmetrics:\n  ## @param metrics.enabled Start a side-car prometheus exporter\n  ##\n  enabled: false\n  ## @param metrics.masterAnnotations [object] Annotations for the Prometheus metrics on master nodes\n  ##\n  masterAnnotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/path: '/metrics/'\n    prometheus.io/port: '{{ .Values.master.webPort }}'\n  ## @param metrics.workerAnnotations [object] Annotations for the Prometheus metrics on worker nodes\n  ##\n  workerAnnotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/path: '/metrics/'\n    prometheus.io/port: '{{ .Values.worker.webPort }}'\n  ## Prometheus Service Monitor\n  ## ref: https://github.com/coreos/prometheus-operator\n  ##      https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint\n  ##\n  podMonitor:\n    ## @param metrics.podMonitor.enabled If the operator is installed in your cluster, set to true to create a PodMonitor Resource for scraping metrics using PrometheusOperator\n    ##\n    enabled: false\n    ## @param metrics.podMonitor.extraMetricsEndpoints Add metrics endpoints for monitoring the jobs running in the worker nodes\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#podmetricsendpoint\n    ## e.g:\n    ## - port: myapp\n    ##   path: /metrics/\n    ##\n    extraMetricsEndpoints: []\n    ## @param metrics.podMonitor.namespace Specify the namespace in which the podMonitor resource will be created\n    ##\n    namespace: \"\"\n    ## @param metrics.podMonitor.interval Specify the interval at which metrics should be scraped\n    ##\n    interval: 30s\n    ## @param metrics.podMonitor.scrapeTimeout Specify the timeout after which the scrape is ended\n    ## e.g:\n    ## scrapeTimeout: 30s\n    ##\n    scrapeTimeout: \"\"\n    ## @param metrics.podMonitor.additionalLabels Additional labels that can be used so PodMonitors will be discovered by Prometheus\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec\n    ##\n    additionalLabels: {}\n  ## Custom PrometheusRule to be defined\n  ## The value is evaluated as a template, so, for example, the value can depend on .Release or .Chart\n  ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions\n  ##\n  prometheusRule:\n    ## @param metrics.prometheusRule.enabled Set this to true to create prometheusRules for Prometheus\n    ##\n    enabled: false\n    ## @param metrics.prometheusRule.namespace Namespace where the prometheusRules resource should be created\n    ##\n    namespace: \"\"\n    ## @param metrics.prometheusRule.additionalLabels Additional labels that can be used so prometheusRules will be discovered by Prometheus\n    ##\n    additionalLabels: {}\n    ## @param metrics.prometheusRule.rules Custom Prometheus [rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)\n    ## These are just examples rules, please adapt them to your needs.\n    ## Make sure to constraint the rules to the current postgresql service.\n    ## rules:\n    ##   - alert: HugeReplicationLag\n    ##     expr: pg_replication_lag{service=\"{{ template \"postgresql.fullname\" . }}-metrics\"} / 3600 \u003e 1\n    ##     for: 1m\n    ##     labels:\n    ##       severity: critical\n    ##     annotations:\n    ##       description: replication for {{ template \"postgresql.fullname\" . }} PostgreSQL is lagging by {{ \"{{ $value }}\" }} hour(s).\n    ##       summary: PostgreSQL replication is lagging by {{ \"{{ $value }}\" }} hour(s).\n    ##\n    rules: []\n"
            ],
            "verify": false,
            "version": "5.7.13",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.kube.kubernetes_namespace.ldap",
            "module.kube.kubernetes_namespace.mariadb",
            "module.kube.kubernetes_namespace.minio",
            "module.kube.kubernetes_namespace.jupyterhub",
            "module.kube.kubernetes_namespace.kafka",
            "module.kube.kubernetes_namespace.keycloak",
            "module.istio_install.helm_release.istio_base",
            "module.istio_install.helm_release.istio_ingress",
            "module.istio_install.helm_release.istio_istiod",
            "module.kube.kubernetes_namespace.argo-events",
            "module.kube.kubernetes_namespace.istio_system",
            "module.kube.kubernetes_namespace.mlflow",
            "module.kube.kubernetes_namespace.spark"
          ]
        }
      ]
    }
  ]
}
